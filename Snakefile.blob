# vim: ft=python

# Rules to make BLOB plots and summary tables.
# These rules are designed to be included in Snakefile.main and will not run standalone.
from hesiod import slurp_file

# Blob plotting is copied from SMRTino but I've now updated the version of NT and also fixed the funny
# Y-axis scale. This change needs to be ported back to SMRTino. Also note I'm running the plots on the
# nolambda files so we'll never see lambda in the blobs. Maybe we should reconsider this??
# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = 10000
BLOB_CHUNKS = 100
BLOB_LEVELS = "phylum order species".split()

# For testing, make blobs of all three passing outputs. Probably we just want "pass" in the
# final version.
BLOB_PARTS  = ["pass", "nolambda", "lambda"]

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 (3?) blob plots and thumbnails for
# each, and arranges the plots into 2 (1?) rows of 3 columns as we wish to
# display them.
# We also depend on the CSV outputs of parse_blob_table, which will be rendered to
# markdown within the make_report script. These are grouped by project not cell, so
# we need a separate rule below.
localrules: per_cell_blob_plots, per_project_blob_tables, fasta_numseqs, parse_blob_table

rule fasta_numseqs:
    output: "{foo}.fasta.numseqs"
    input:  "{foo}.fasta"
    shell:  "grep -o '^>' {input} | wc -l > {output}"

rule per_cell_blob_plots:
    output: "blob/{cell}/plots.yaml"
    input:
        png = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}{thumb}.png",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      taxlevel = BLOB_LEVELS,
                      extn = "cov0 read_cov.cov0".split(),
                      thumb = ['.__thumb', ''] ),
        sample = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta.numseqs",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      ss = [BLOB_SUBSAMPLE] ),
    run:
        # We want to know how big the subsample actually was, as it may be < BLOB_SUBSAMPLE, so check
        # the FASTA, then make a dict of {part: seq_count}
        wc = wildcards
        counts = { part: slurp_file(f)[0]
                   for part, f in zip(BLOB_PARTS, input.sample) }

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here in preserving order but I can make a nested list comprehension.
        # Group by BLOB_PARTS with an appropriate title.
        plots = [ dict(title = 'Taxonomy for {pf} reads ({c} sequences) by {l}'.format(
                                                                            pf = pf,
                                                                            c = counts[pf],
                                                                            l = ', '.join(BLOB_LEVELS) ),

                       files = [ [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}.png".format(
                                                                            cell = wc.cell,
                                                                            run = RUN,
                                                                            ci = parse_cell_name(wc.cell),
                                                                            pf = pf,
                                                                            taxlevel = taxlevel,
                                                                            extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]
                      ) for pf in BLOB_PARTS ]

        dump_yaml(plots, str(output))

# See above. input.sample is as above but now for all the cells.
rule per_project_blob_tables:
    output: "blob/blobstats_by_project.yaml"
    input:
        csv = expand("blob/blobstats.{project}.{pf}.{taxlevel}.csv",
                                          project = CELLS_PER_PROJECT,
                                          pf = BLOB_PARTS,
                                          taxlevel = BLOB_LEVELS ),
        sample = lambda wc: [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta.numseqs".format(
                                          cell = cell,
                                          pf = pf,
                                          run = RUN,
                                          ci = parse_cell_name(cell),
                                          ss = BLOB_SUBSAMPLE )
                                for cell in SC for pf in BLOB_PARTS ],
    run:
        # List ALL the tables of blob stats to show per project
        # The fiddly bit is incorporating the counts dict. See doc/per_project_info.txt
        res = dict()
        for p in CELLS_PER_PROJECT:
            # For each project p make a list of tables, first by PF, then by tax level.
            csv_list = res[p] = list()
            for pf in BLOB_PARTS:
                for tl in BLOB_LEVELS:
                    # For each table we need a dict of counts matching the table lines,
                    # ie. one item per cell.
                    counts_dict = dict()
                    for cell in CELLS_PER_PROJECT[p]:
                        # Recreate the filename from input.sample
                        f = "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta.numseqs".format(
                                                            cell = cell,
                                                            run = RUN,
                                                            ci =  parse_cell_name(cell),
                                                            pf = pf,
                                                            ss = BLOB_SUBSAMPLE )
                        # Did I get that right??
                        assert f in [str(s) for s in input.sample]
                        counts_dict[cell] = slurp_file(f)[0]

                    csv = "blob/blobstats.{project}.{pf}.{taxlevel}.csv".format(
                                    project = p,
                                    pf = pf,
                                    taxlevel = tl )
                    assert csv in [str(s) for s in input.csv]

                    csv_list.append( dict( title = "BLAST hits for {pf} reads by {taxlevel}".format(
                                                            pf = pf,
                                                            taxlevel = tl ),
                                           csv = csv,
                                           counts = counts_dict ) )

        # Finally we have the data structure we need.
        dump_yaml(res, str(output))

rule parse_blob_table:
    output: "blob/blobstats.{project}.{pf}.{taxlevel}.csv"
    input:
        lambda wc: [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.blobplot.stats.txt".format(
                                                            cell = cell,
                                                            run = RUN,
                                                            ci = parse_cell_name(cell),
                                                            **vars(wc) )
                         for cell in CELLS_PER_PROJECT[wc.project] ]
    params:
        pct_limit = 1
    shell:
        "{TOOLBOX} parse_blob_table {output} {params.pct_limit} {input}"

# Convert to FASTA and subsample and munge the headers
# seqtk seq -ACNU == to-fasta, no-comments, no-ambiguous, uppercase
rule fastq_to_subsampled_fasta:
    output: "blob/{foo,.+(_pass|_nolambda)}+sub{n}.fasta"
    input: "{foo}.fastq.gz"
    threads: 2
    shell:
       r"""{PIGZ} -p{threads} -d -c {input} | \
             {TOOLBOX} seqtk seq -ACNU - | \
             {TOOLBOX} seqtk sample - {wildcards.n} | \
             sed 's,/,_,g' > {output}
        """

# Version that works on lambda.bam files
# The sed filter may well be redundant here.
rule bam_to_subsampled_fasta:
    output: "blob/{foo,.+(_lambda)}+sub{n}.fasta"
    input: "{foo}.bam"
    threads: 2
    shell:
       r"""{TOOLBOX} samtools fasta {input} | \
             {TOOLBOX} seqtk sample - {wildcards.n} | \
             sed 's,/,_,g' > {output}
        """

# Makes a .complexity file for our FASTA file
# {foo} will be blob/{cell}/{ci[Run]}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
# I'm filtering out repeated rows to reduce the size of the BLOB DB - there can
# be a lot of repeats so this is worth running on the cluster.
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        'LC_ALL=C ; ( for i in {input} ; do sort -u -k1,2 "$i" ; done ) > {output}'

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} blast_nt -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty,
# hence the final touch. Note this will 'split' a completely empty file if you ask it to.
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.parts} {output.list}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
# If reads_sample is empty this will generate an empty file
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
       r'''if [ ! -s {input.reads_sample} ] ; then touch {output.json} ; exit 0 ; fi
           mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/tmp \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/tmp.blobDB.json {output.json}
        '''

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of png files
# If blobDB.json is empty, make some empty images
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png",          "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        thumbsize = "320x320"
    shadow: 'shallow'
    shell:
       r'''mkdir blob_tmp
           if [ -s {input.json} ] ; then
               export BLOB_COVERAGE_LABEL=Non-Dustiness
               {TOOLBOX} blobtools plot -i {input.json} -o blob_tmp/ --dustplot --sort_first no-hit,other,undef -r {wildcards.taxlevel}
               tree blob_tmp
               mv blob_tmp/tmp.*.stats.txt {output.stats}
               mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.cov0.png {output.plotc[0]}
               mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png {output.plotr[0]}
           else
               echo "No data" > {output.stats}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotc[0]}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotr[0]}
           fi
           {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize} {output.plotc[1]}
           {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize} {output.plotr[1]}
        '''

