# vim: ft=python

# Rules to make BLOB plots and summary tables.
# These rules are designed to be included in Snakefile.main and will not run standalone.
from hesiod import slurp_file

# Blob plotting is originally copied from SMRTino
# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = int(config.get('blob_subsample', 10000))
BLOB_CHUNKS    = int(config.get('blob_chunks', 20))
BLOB_LEVELS    = config.get('blob_levels', "phylum order species".split())
BLAST_SCRIPT   = config.get('blast_script', "blast_nt")

# For testing, make blobs of all three passing outputs. Probably we just want "pass" in the
# final version. See also label_for_part() in the main Snakefile.
#BLOB_PARTS  = "pass nolambda lambda".split()
BLOB_PARTS  = config.get('blob_parts', ["pass"])

# Snakemake won't let us set a congif item to [], so...
if BLOB_PARTS == "none" or BLOB_PARTS == ["none"]:
    BLOB_PARTS = []

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 (3?) blob plots and thumbnails for
# each, and arranges the plots into 2 (1?) rows of 3 columns as we wish to
# display them.
# We also depend on the CSV outputs of parse_blob_table, which will be rendered to
# markdown within the make_report script. These are grouped by project not cell, so
# we need a separate rule below.
localrules: per_cell_blob_plots, per_project_blob_tables, fasta_numseqs, parse_blob_table

# Basic FASTA sequence counter...
# we need a special case for empty files here - these simply have 0 sequences.
rule fasta_numseqs:
    output: "{foo}.fasta.numseqs"
    input:  "{foo}.fasta"
    shell:
         r'''if ! grep -o '^>' {input} | wc -l > {output} ; then
                [ ! -s {input} ] && echo 0 > {output}
             fi
          '''

rule per_cell_blob_plots:
    output: "blob/{base}_{barcode}_plots.yaml"
    input:
        png = lambda wc: expand( "blob/{base}_{bc}_{pf}.{taxlevel}.{extn}{thumb}.png",
                      base = wc.base,
                      bc = wc.barcode,
                      pf = BLOB_PARTS,
                      taxlevel = BLOB_LEVELS,
                      extn = "cov0 read_cov.cov0".split(),
                      thumb = ['.__thumb', ''] ),
        sample = lambda wc: expand( "blob/{base}_{bc}_{pf}+sub{ss}.fasta.numseqs",
                      base = wc.base,
                      bc = wc.barcode,
                      pf = BLOB_PARTS,
                      ss = [BLOB_SUBSAMPLE] ),
    run:
        # We want to know how big the subsample actually was, as it may be < BLOB_SUBSAMPLE, so check
        # the FASTA, then make a dict of {part: seq_count} for this cell.
        wc = wildcards
        if not BLOB_PARTS:
            counts = {}
        else:
            counts = { part: slurp_file(f)[0]
                       for part, f in zip(BLOB_PARTS, input.sample) }

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here in preserving order but I can make a nested list comprehension.
        # Group by BLOB_PARTS with an appropriate title.
        plots = [ dict(title = 'Taxonomy for {pf} reads ({c} sequences) by {l}'.format(
                                                                pf = label_for_part(pf, wc.barcode),
                                                                c = counts[pf],
                                                                l = ', '.join(BLOB_LEVELS) ),

                       has_data = (str(counts[pf]) != '0'),

                       files = [ [ "{basebase}_{bc}_{pf}.{taxlevel}.{extn}.png".format(
                                                                basebase = os.path.basename(wc.base),
                                                                bc = wc.barcode,
                                                                pf = pf,
                                                                taxlevel = taxlevel,
                                                                extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]
                      ) for pf in BLOB_PARTS ]

        dump_yaml(plots, str(output))

# See above. Now we want to make a CSV table per project summarizing the main taxa found
# in the blob database.
# TODO - work out if this should be split by barcode? Prob not.
rule per_project_blob_tables:
    output: "blob/blobstats_by_project.yaml"
    input:
        tsv = expand("blob/blobstats.{project}.{pf}.{taxlevel}.tsv",
                                          project = CELLS_PER_PROJECT,
                                          pf = BLOB_PARTS,
                                          taxlevel = BLOB_LEVELS ),
    run:
        # List ALL the tables of blob stats to show per project
        res = dict()
        for p in CELLS_PER_PROJECT:
            # For each project p make a list of tables, first by {pf}, then by tax level.
            tsv_list = res[p] = list()
            for pf in BLOB_PARTS:
                for tl in BLOB_LEVELS:
                    # Recreate the filename. I should just be able to go through input.tsv
                    # in order but I'm not 100% sure.
                    tsv = f"blobstats.{p}.{pf}.{tl}.tsv"
                    assert "blob/" + tsv in [str(s) for s in input.tsv]

                    # At present we have one table per sample and all the barcodes (if any) are split in rows.
                    tsv_list.append( dict( title = "BLAST hit percentages for {pf} reads by {taxlevel}".format(
                                                            pf = label_for_part(pf),
                                                            taxlevel = tl ),
                                           tsv = tsv ) )

        # And this gives us the data structure we need.
        dump_yaml(res, str(output))

rule parse_blob_table:
    output: "blob/blobstats.{project}.{pf}.{taxlevel}.tsv"
    input:
        lambda wc: [ f"blob/{cellname_to_base(cell)}_{barcode}_{wc.pf}.{wc.taxlevel}.blobplot.stats.txt"
                     for cell in CELLS_PER_PROJECT[wc.project]
                     for barcode in SC[cell] ]
    params:
        pct_limit = 1.0,
        label = 'Cell'
    shell:
        "parse_blob_table.py -l {params.label:q} -t -o {output} -c {params.pct_limit} {input}"

# Convert to FASTA and subsample and munge the headers
# seqtk seq -ACU == to-fasta, no-comments, uppercase
# Also because pigz and/or 'seqtk seq' may well be killed by SIGPIPE we have to ignore pipe fails,
# even though this may mask other problems.
# Note I was thinking to add "sed '/^>/!s/U/T/g'" to fix U's to T's but apparently BLAST is cool
# with the U bases.
rule fastq_to_subsampled_fasta:
    output: "blob/{foo,.+(_pass|_nolambda)}+sub{n}.fasta"
    input: "{foo}.fastq.gz"
    threads: 2
    shell:
       r"""set +o pipefail
           {PIGZ} -p{threads} -d -c {input} | \
             {TOOLBOX} seqtk seq -ACU - | \
             {TOOLBOX} seqtk sample - {wildcards.n} | \
             sed -e 's,/,_,g' > {output}
        """

# Version that works on lambda.bam files
# The sed filter may well be redundant here.
rule bam_to_subsampled_fasta:
    output: "blob/{foo,.+(_lambda)}+sub{n}.fasta"
    input: "{foo}.bam"
    threads: 2
    shell:
       r"""{TOOLBOX} samtools fasta {input} | \
             {TOOLBOX} seqtk sample - {wildcards.n} | \
             sed 's,/,_,g' > {output}
        """

# Makes a .complexity file for our FASTA file
# {foo} will be blob/{cell}/{ci[Experiment]}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
# I'm filtering out repeated rows to reduce the size of the BLOB DB - there can
# be a lot of repeats so this is worth running on the cluster.
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        'LC_ALL=C ; ( for i in {input} ; do sort -u -k1,2 "$i" ; done ) > {output}'

# BLAST a chunk. Note the 'blast_nt' wrapper determines the actual database to search,
# but you can specify an alternate wrapper, which need not be in the TOOLBOX,
# in config['blast_script']
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} {BLAST_SCRIPT} -query {input} -outfmt {params.outfmt:q} \
           -evalue {params.evalue:q} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty,
# hence the final touch. Note this will 'split' a completely empty file if you ask it to.
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.parts} {output.list}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
# If reads_sample is empty this will generate an empty file
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
       r'''if [ ! -s {input.reads_sample} ] ; then touch {output.json} ; exit 0 ; fi
           mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/tmp \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/tmp.blobDB.json {output.json}
        '''

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of png files
# If blobDB.json is empty, make some empty images
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png",          "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        maxsize = "1750x1750",
        thumbsize = "320x320"
    shadow: 'shallow'
    shell:
       r'''mkdir blob_tmp
           if [ -s {input.json} ] ; then
               export BLOB_COVERAGE_LABEL=Non-Dustiness
               {TOOLBOX} blobtools plot -i {input.json} -o blob_tmp/ --dustplot --sort_first no-hit,other,undef -r {wildcards.taxlevel}
               ls blob_tmp
               mv blob_tmp/tmp.*.stats.txt {output.stats}
               {TOOLBOX} convert blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.cov0.png \
                   -resize {params.maxsize}'>' {output.plotc[0]}
               {TOOLBOX} convert blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png \
                   -resize {params.maxsize}'>' {output.plotr[0]}
           else
               echo "No data" > {output.stats}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotc[0]}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotr[0]}
           fi
           {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize}'>' {output.plotc[1]}
           {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize}'>' {output.plotr[1]}
        '''

