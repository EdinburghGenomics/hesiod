# vim: ft=python

# Rules to filter, compress and combine the original files.
# These rules are designed to be included in Snakefile.main and will not run standalone.

# Note that these rules are bypassed if the run directory is missing, allowing us to repeat QC
# without errors relating to missing files.

# Compress the file discovered by the above function and rename it, matching the base of
# the FASTQ and BAM files. Note the original name is preserved in the GZIP header and can
# be revealed by 'gunzip -Nlv {output.gz}'.
rule gzip_sequencing_summary:
    output:
        gz  = "{cell}/{fullid}_sequencing_summary.txt.gz",
        md5 = "md5sums/{cell}/{fullid}_sequencing_summary.txt.gz.md5"
    input:  lambda wc: [find_sequencing_summary(RUNDIR, wc.cell)]
    threads: 2
    shell:
        r"""{PIGZ} -v -p{threads} -Nc {input} >{output.gz}
            ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
         """

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-( ...But I fixed it in upstream now :-)
# Note that the fast5_out() utility function determines outputs for this rule.
# Note2 that barcode=. should work with this even though it's a bit sus.
rule gzip_md5sum_fast5:
    output:
        gz  = "{cell}/fast5_{barcode}_{pf}/{f5fn}.fast5.gz",
        md5 = "md5sums/{cell}/fast5_{barcode}_{pf}/{f5fn}.fast5.gz.md5"
    input:
        RUNDIR + "/{cell}/fast5_{pf}/{barcode}/{f5fn}.fast5"
    threads: 2
    shell:
       r"""{PIGZ} -v -p{threads} -Nc {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

# These two concatenate and zip and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
# TODO - should probably convert this to zip the individual chunks, as per the nolambda files?

# I've broken out the rule that makes _fastq.list files so I can request the list without
# making the actual merged file.
localrules: list_fastq
rule list_fastq:
    priority: 100
    output:
        fofn  = "{cell}/{fullid}_{barcode}_{pf}_fastq.list"
    input:
        fastq = lambda wc: [f"{RUNDIR}/{f}" for f in SC[wc.cell][wc.barcode]['fastq_'+wc.pf]]
    run:
        # Just write all the input files to the output file.
        with open(output.fofn, "w") as fh:
            try:
                for fname in input.fastq: print(fname, file=fh)
            except AttributeError:
                # So there are no files. Make an empty list then.
                pass

rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_{pf}.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_{pf}.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_{pf}.fastq.count"
    input:
        fofn  = "{cell}/{fullid}_{barcode}_{pf}_fastq.list"
    threads: 8
    run:
        # Rely on xargs to deal with way more files than could fit on the command line
        shell(r"xargs -d '\n' cat <{input.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        # Base counter
        shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# We've decided to do the lambda mapping up front. Here's the rule to combine the filtered
# fastq.gz files, by simply concatenating them rather than unzip/rezip.
# Note this is specific to the pass reads. We don't partition the fail reads (should we??)
rule concat_md5sum_nolambda_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_nolambda.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_nolambda.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_nolambda.fastq.count",
        fofn   = temp("fastq_pass_tmp/{cell}/{fullid}_{barcode}_nlfq.list"),
        fofn2  = temp("fastq_pass_tmp/{cell}/{fullid}_{barcode}_nlfq_count.list")
    input:
        fastq  = lambda wc: [f"fastq_pass_tmp/{f}.nlfq.gz"    for f in SC[wc.cell][wc.barcode]['fastq_pass']],
        counts = lambda wc: [f"fastq_pass_tmp/{f}.nlfq.count" for f in SC[wc.cell][wc.barcode]['fastq_pass']]
    run:
        try:
            # Avoid blowing the command line limit by listing files in a file. This could be
            # marked temporary but it's small and handy for debugging.
            with open(output.fofn, "w") as fh:
                for fname in input.fastq: print(fname, file=fh)

            with open(output.fofn2, "w") as fh:
                for fname in input.counts: print(fname, file=fh)

            # Simply concatenate the gzipped files
            shell(r"xargs -d '\n' cat <{output.fofn} > {output.gz}")

            # Combine base counts
            shell(r"xargs -d '\n' cat <{output.fofn2} | fq_base_combiner.awk -r fn=`basename {output.gz} .gz` > {output.counts}")


        except AttributeError:
            # We're merging nothing. But Snakemake must have her output file.
            shell("touch {output.fofn} {output.fofn2}")

            shell("true | {PIGZ} -c > {output.gz}")

            # Generate an empty count.
            shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        # Checksum over the whole file
        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

rule concat_md5sum_lambda_bam:
    output:
        bam  = "{cell}/{fullid}_{barcode}_lambda.bam",
        md5  = "md5sums/{cell}/{fullid}_{barcode}_lambda.bam.md5",
        fofn = temp("{cell}/{fullid}_{barcode}_lambda_bam.list")
    input:
        bam  = lambda wc: [f"fastq_pass_tmp/{f}.lambda.bam" for f in SC[wc.cell][wc.barcode]['fastq_pass']]
    threads: 4
    run:
        try:
            with open(output.fofn, "w") as fh:
                for fname in input.bam: print(fname, file=fh)

            # samtools merge (files in fastq_pass_tmp should be pre-sorted)
            shell("{TOOLBOX} samtools merge -@ {threads} -l9 -b {output.fofn} {output.bam}")

        except AttributeError:
            # We're merging nothing. But Snakemake must have her output file.
            shell("touch {output.bam}")

        shell(r"( cd `dirname {output.bam}` && md5sum `basename {output.bam}` ) > {output.md5}")

# This rule produces the stuff in fastq_pass_tmp, and will normally be applied
# to every individual fastq_pass file prior to combining the results. It uses nested
# implicit FIFOs. I call it "bashception".
# Note this type of construct is vulnerable to the kill-before-flush bug but it seems with
# this arrangement we are OK. See:
#  https://www.pixelbeat.org/docs/coreutils-gotchas.html
# In normal operation the temp dir will be removed by the "onsuccess" handler below.
rule map_lambda:
    output:
        fq     = temp("fastq_pass_tmp/{cell_pf_bc}/{f,[^/]+}.fastq.nlfq.gz"),
        bam    = temp("fastq_pass_tmp/{cell_pf_bc}/{f,[^/]+}.fastq.lambda.bam"),
        counts = temp("fastq_pass_tmp/{cell_pf_bc}/{f,[^/]+}.fastq.nlfq.count"),
    input:
        RUNDIR + "/{cell_pf_bc}/{f}.fastq"
    params:
        ref    = os.environ.get('REFS', '.') + '/phage_lambda.mmi',
        rg     = r'@RG\tID:1\tSM:{cell_pf_bc}\tPL:promethion',
        mmopts = '-t 1 -a --MD --sam-hit-only -y --secondary=no -x map-ont '
    threads: 6
    shell:
       r'''{TOOLBOX} minimap2 {params.mmopts} -R {params.rg:q} {params.ref} {input} | tee >( \
                lambda_splitter.awk \
                    -v paf=/dev/stdin \
                    -v nolambda=>({PIGZ} -c -p{threads} > {output.fq}) \
                    {input} ) | \
           {TOOLBOX} samtools sort - -@ {threads} -o {output.bam}
           {PIGZ} -dc -p{threads} {output.fq} | fq_base_counter.awk -r fn=`basename {output.fq} .gz` > {output.counts}
        '''

# Remove the fastq_pass_tmp directory which should normally be empty of files if
# everything worked.
onsuccess:
    shell("find fastq_pass_tmp/ -type d -delete || true")

