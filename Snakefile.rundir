# vim: ft=python

# Rules to filter, compress and combine the original files.
# These rules are designed to be included in Snakefile.main and will not run standalone.

# Note that these rules are bypassed if the run directory is missing, allowing us to repeat QC
# without errors relating to missing files.

def find_sequencing_summary(wc):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). Either way there should be only one.
    """
    found = glob(format("{RUNDIR}/{wc.cell}/*_sequencing_summary.txt")) + \
            glob(format("{RUNDIR}/{wc.cell}/sequencing_summary/*_sequencing_summary.txt"))

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              " - found {}.".format(len(found)) )

    return found

# Compress the file discovered by the above function and link it to a fixed name.
rule gzip_sequencing_summary:
    output:
        link = "{cell}/sequencing_summary.txt.gz"
    input:
        summary = find_sequencing_summary
    threads: 2
    run:
        # First copy the file, preserving the name. Then link.
        gzfile = "{}/{}.gz".format(wildcards.cell, os.path.basename(str(input.summary)))
        shell(r"{PIGZ} -p{threads} -c <{input.summary} >{gzfile}")

        shell(r"ln -snr {gzfile} {output.link}")

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-( ...But I fixed it in upstream now :-)
rule gzip_md5sum_fast5:
    output:
        gz  = "{foo}.fast5.gz",
        md5 = "md5sums/{foo}.fast5.gz.md5"
    input:
        RUNDIR + "/{foo}.fast5"
    threads: 2
    shell:
       r"""{PIGZ} -v -p{threads} -c {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

# This one concatenates and zips and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
# TODO - should probably convert this to zip the individual chunks, as per the nolambda files?
rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz    = "{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz",
        md5   = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz.md5",
        count = "counts/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.count",
        fofn  = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 8
    run:
        # Here we run the risk of blowing out the command line length limit, so avoid
        # that.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"xargs -d '\n' cat <{output.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        # Base counter
        shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.count}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# We've decided to do the lambda mapping up front. Here's the rule to combine the filtered
# fastq.gz files, by simply concatenating them rather than unzip/rezip.
# Note this is specific to the pass reads. We don't partition the fail reads (should we??)
rule concat_md5sum_nolambda_fastq:
    priority: 100
    output:
        gz    = "{cell}/{all,[^/]+}_nolambda.fastq.gz",
        md5   = "md5sums/{cell}/{all,[^/]+}_nolambda.fastq.gz.md5",
        count = "counts/{cell}/{all,[^/]+}_nolambda.fastq.count",
        fofn  = temp("fastq_pass_tmp/{cell}/{all,[^/]+}_nlfq.list"),
        fofn2 = temp("fastq_pass_tmp/{cell}/{all,[^/]+}_nlfq_count.list")
    input:
        fastq = lambda wc: [format("fastq_pass_tmp/{f}.nlfq.gz") for f in SC[wc.cell]['fastq_pass']],
        count = lambda wc: [format("fastq_pass_tmp/{f}.nlfq.count") for f in SC[wc.cell]['fastq_pass']]
    run:
        # Avoid blowing the command line limit by listing files in a file. This could be
        # marked temporary but it's small and handy for debugging.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        with open(output.fofn2, "w") as fh:
            for fname in input.count: print(fname, file=fh)

        # Simply concatenate the gzipped files
        shell(r"xargs -d '\n' cat <{output.fofn} > {output.gz}")

        # Combine base counts
        shell(r"xargs -d '\n' cat <{output.fofn2} | fq_base_combiner.awk -r fn=`basename {output.gz} .gz` > {output.count}")

        # Checksum over the whole file
        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

rule concat_md5sum_lambda_bam:
    output:
        bam  = "{cell}/{all,[^/]+}_lambda.bam",
        md5  = "md5sums/{cell}/{all,[^/]+}_lambda.bam.md5",
        fofn = "{cell}/{all,[^/]+}_lambda_bam.list"
    input:
        bam  = lambda wc: [format("fastq_pass_tmp/{f}.lambda.bam") for f in SC[wc.cell]['fastq_pass']]
    threads: 4
    run:
        with open(output.fofn, "w") as fh:
            for fname in input.bam: print(fname, file=fh)

        # samtools merge (files in fastq_pass_tmp should be pre-sorted)
        shell("{TOOLBOX} samtools merge -@ {threads} -l9 -b {output.fofn} {output.bam}")

        shell(r"( cd `dirname {output.bam}` && md5sum `basename {output.bam}` ) > {output.md5}")

# This rule produces the stuff in fastq_pass_tmp, and will normally be applied
# to every individual fastq_pass file prior to combining the results. It uses nested
# implicit FIFOs. I call it "bashception".
# Note this type of construct is vulnerable to the kill-before-flush bug but it seems with
# this arrangement we are OK. See:
#  https://www.pixelbeat.org/docs/coreutils-gotchas.html
# I plan to fully clean out the temp dir as a separate op outside of Snakemake,
# deleting it once a cell is done.
rule map_lambda:
    output:
        fq    = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.nlfq.gz"),
        bam   = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.lambda.bam"),
        count = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.nlfq.count"),
    input:
        RUNDIR + "/{cell}/{f}.fastq"
    params:
        ref    = os.environ.get('REFS', '.') + '/phage_lambda.mmi',
        rg     = r'@RG\tID:1\tSM:{cell}\tPL:promethion',
        mmopts = '-t 1 -a --MD --sam-hit-only -y --secondary=no -x map-ont '
    threads: 6
    shell:
       r'''{TOOLBOX} minimap2 {params.mmopts} -R {params.rg:q} {params.ref} {input} | tee >( \
                lambda_splitter.awk \
                    -v paf=/dev/stdin \
                    -v nolambda=>({PIGZ} -c -p{threads} > {output.fq}) \
                    {input} ) | \
           {TOOLBOX} samtools sort - -@ {threads} -o {output.bam}
           {PIGZ} -dc -p{threads} {output.fq} | fq_base_counter.awk -r fn=`basename {output.fq} .gz` > {output.count}
        '''
