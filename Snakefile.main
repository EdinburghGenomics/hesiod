#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_ref)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pprint, pformat

from itertools import product
from functools import partial
from subprocess import CalledProcessError

from hesiod import glob, parse_cell_name, load_final_summary, groupby, dump_yaml, load_yaml

# This is just here to help testing - Snakemake sets it automatically.
logger = snakemake.logging.logger

# $TOOLBOX must be set to something
TOOLBOX = f'env PATH="{os.environ["TOOLBOX"]}:$PATH"'
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')
if not os.path.exists(RUNDIR):
    logger.warning(f"WARNING: {RUNDIR} is not a directory. Suppressing rules that reference RUNDIR.")
    RUNDIR = None

def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return parse_cell_name(RUN, c)['Base']

def scan_cells(rundir, config):
    """ Work out all the cells to process. Normally simple since the list is just passed in
        by driver.sh in config['cellsready'] but I do want to be able to process all by default so
        there is some scanning capability too.
        Then get a list of all the files per cell.
        This function must not fail if files are missing because that blocks the whole Snakefile from
        running even if you want to use an unrelated rule.

        res structure is { cell : barcode : 'fastX_pass' : [ list of files ] }
    """
    if 'cells' in config:
        # Believe what we are told
        cells = config['cells'].split('\t')
    elif rundir:
        # Look for valid cells.
        cells = sorted(set( '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(f"{rundir}/*/*/fastq_????/") ))
    else:
        # Look for cells here in the output (presumably the run already processed and the raw dir was removed)
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(f"*/*/cell_info.yaml") ]

    if 'cellsready' in config:
        # This should include the cells to be processed now AND those already processed.
        cellsready = config['cellsready'].split('\t')

        for c in cellsready:
            assert c in cells, f'Invalid cell (no fastq_pass or not listed in config["cells"]): {c}'
    elif rundir:
        # Look for cells with a final_summary.txt (as made by MinKNOW). Note the new name in MinKNOW 3.6+
        cellsready = [ c for c in cells if (
                       glob(f"{rundir}/{c}/final_summary.txt") or
                       glob(f"{rundir}/{c}/final_summary_*_*.txt") ) ]
    else:
        # If there is no RUNDIR this makes most sense by default.
        cellsready = cells

    if not cellsready:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cellsready }

    if rundir:
        for c, d in res.items():
            for pf, filetype in product(["pass", "fail"], ["fastq", "fastq.gz", "fast5"]):
                category = f"{filetype}_{pf}"
                cat_dir  = f"{filetype.split('.')[0]}_{pf}"
                # Collect un-barcoded files
                non_barcoded_files = [ f[len(rundir) + 1:]
                                       for f in glob(f"{rundir}/{c}/{cat_dir}/*.{filetype}") ]
                barcoded_files = [ f[len(rundir) + 1:]
                                   for f in glob(f"{rundir}/{c}/{cat_dir}/*/*.{filetype}") ]
                if non_barcoded_files:
                    d.setdefault('.', dict())[category] = non_barcoded_files
                for bf in barcoded_files:
                    # Keys in d are to be the barcodes which we extract from the filenames like so:
                    _, barcode, _ = bf[len(c) + 1:].split('/')
                    d.setdefault(barcode, dict()).setdefault(category, list()).append(bf)

        # Add in empty lists for missing items.
        # A quirk of the logic above is that barcodes with "fail" reads but no "pass" reads
        # are listed after those with "pass" reads, and this carries into the reports. If this
        # is a problem, this is the place to fix it by sorting all dicts within res.
        for c, d in res.items():
            for bcdict in d.values():
                for pf, filetype in product(["pass", "fail"], ["fastq", "fastq.gz", "fast5"]):
                    # If the barcode is there we should have all four categories in bcdict
                    bcdict.setdefault(f"{filetype}_{pf}", [])

            # I'm not sure what to do with fast5_skip files, but at this point if I see any I'll just
            # tack them on with fast5_fail. If I see "skipped" files _and_ barcodes well then I really
            # dunno.
            if '.' in d:
                d['.']['fast5_fail'].extend( [ f[len(rundir) + 1:]
                                               for f in glob(f"{rundir}/{c}/fast5_skip/*.fast5") ] )

        # Sanity-check that the file counts match with final_summary.txt
        for c, d in res.items():
            fs = load_final_summary(*glob(f"{rundir}/{c}/final_summary_*_*.txt"))
            for ft in ["fastq", "fast5"]:
                # Add zipped and unzipped FASTQ files...
                ft_sum = sum( len(fileslist[f"{ft}{z}_{pf}"])
                                for fileslist in d.values()
                                for z in (["", ".gz"] if ft == "fastq" else [""])
                                for pf in ["pass", "fail"] )

                ft_expected = fs[f"{ft}_files_in_final_dest"]
                if ft_sum != ft_expected:
                    raise RuntimeError( f"Mismatch between count of {ft.upper()} files for {c}:\n" +
                                        f"{ft_sum} (seen) != {ft_expected} (in final_summary.txt)" )

    # Return the dict of stuff to process, and other counts we've calculated
    return res, dict( cells = len(cells),
                      cellsready = len(cellsready),
                      cellsaborted = 0 )

def sc_counts(sc_dict, width=140):
    """ Make a printable summary of SC
    """
    # Make a dict that just shows the counts
    sc_counts = { c : { bc: { category: f"<{len(filelist)} files>"
                              for category, filelist in d.items() }
                        for bc, d in bc_dict.items() }
                  for c, bc_dict in sc_dict.items() }

    # Ideally I'd use sort_dicts=False but that needs Py3.8
    return pformat(sc_counts, width=width)

def find_representative_fast5(cell, sc, try_glob=True):
    """ Find a fast5 file to scan for metadata. This returns the fast5.gz file as found
        in the output directory but uses the list of files globbed from the input directory.
    """
    fast5_pass = [ plist for bc in sc[cell] for plist in sc[cell][bc]['fast5_pass']  ]
    fast5_fail = [ flist for bc in sc[cell] for flist in sc[cell][bc]['fast5_fail']  ]
    if fast5_pass:
        # Depend on the first file provided by SC[wc.cell] for any barcode
        return fast5_out(fast5_pass[0])
    elif fast5_fail:
        # Sometimes everything fails
        return fast5_out(fast5_fail[0])
    elif try_glob:
        # Look for an existing output file.
        globbed = glob(f"{cell}/fast5_*_????/*.fast5.gz")
        if globbed:
            return globbed[0]
        raise RuntimeError("No representative FAST5 found")
    else:
        # Nope
        raise RuntimeError("No representative FAST5 found and try_glob is False")

def fast5_out(f5_in):
    """ Given a fast5 file, say what the output fast5.gz will be
    """
    f5_split = f5_in.split('/')
    pf = f5_split[2].split('_')[1]

    if len(f5_split) == 5:
        # has barcode
        return f"{f5_split[0]}/{f5_split[1]}/fast5_{f5_split[3]}_{pf}/{f5_split[-1]}.gz"
    else:
        return f"{f5_split[0]}/{f5_split[1]}/fast5_._{pf}/{f5_split[-1]}.gz"


def find_sequencing_summary(rundir, cell):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). From MinKNOW 3.6+ the naming convention changes
       too.
       In any case there should be only one.
    """
    found = glob(f"{rundir}/{cell}/*_sequencing_summary.txt") + \
            glob(f"{rundir}/{cell}/sequencing_summary/*_sequencing_summary.txt") + \
            glob(f"{rundir}/{cell}/sequencing_summary_*_*.txt")

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              f" - found {len(found)}." )

    return found[0]

## End of functions ## Leave this comment in place to help the unit tests.

# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the project number.
# Some outputs are aggregated by {lib}, some by {project} and other are per-cell.
# Note - nothing is aggregated per-lib just yet?!
SC, COUNTS = scan_cells(RUNDIR, config)
RUN = os.path.basename(os.path.realpath('.')).split('.')[0]
CELLS_PER_LIB     = groupby( SC, lambda c: parse_cell_name(RUN, c)['Library'], True)
CELLS_PER_PROJECT = groupby( SC, lambda c: parse_cell_name(RUN, c)['Project'], True)

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            plist.add(load_yaml(y)['Project'])
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

logger.info( f"config =\n{pformat(config)}" )
logger.info( f"RUN {RUN}, SC =\n{sc_counts(SC)}" )

# Global wildcard patterns
wildcard_constraints:
    fullid  = "[^/]+",
    barcode = "[^/_]+",
    pf      = "pass|fail",
    pfs     = "pass|fail|skip",

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.

localrules: main, one_cell, pack_fast5
rule main:
    output:
        plist     = "projects_ready.txt",
        panrep    = f"all_reports/report.{len(SC)}cells.pan",
        rep       = f"all_reports/report.{len(SC)}cells.pan.html",
        replink   = "all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml",
        realnames = "project_realnames.yaml",
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        # shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} --minionqc {input.minionqc} {input.yaml}")
        shell(r'''
            make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} \
                           --blobstats {input.blobstats} --realnames {input.realnames} \
                           {input.yaml}
        ''')

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/easter.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {output.rep} {output.panrep}
        ''')

        shell("ln -snr {output.rep} {output.replink}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this. So to fully process a run you need to call main AND pack_fast5
rule pack_fast5:
    input:
        fast5_gz   = lambda wc: [ fast5_out(f)
                                        for cell, barcodes in SC.items()
                                        for bc in barcodes
                                        for pf in ['fast5_pass', 'fast5_fail']
                                        for f in SC[cell][bc][pf] ],
        fast5_md5  = lambda wc: [ f"md5sums/{fast5_out(f)}.md5"
                                        for cell, barcodes in SC.items()
                                        for bc in barcodes
                                        for pf in ['fast5_pass', 'fast5_fail']
                                        for f in SC[cell][bc][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM)
#  but as requested by Urmi we also retain the combined 'pass' files.
#  All of the files get an md5sum

# See also BLOB_PARTS in Snakefile.blob
FASTQ_PARTS = ["pass", "nolambda", "fail"]

def label_for_part(part, barcode='.'):
    """Was a dict. Now a function.
    """
    if barcode == '.':
        pls = { 'pass'     : "all passed",
                'fail'     : "all failed",
                'nolambda' : "passed and lambda-filtered",
                'lambda'   : "passed and lambda-mapping",
                '_default' : f"{part}" }
    else:
        pls = { 'pass'     : f"{barcode} passed",
                'fail'     : f"{barcode} failed",
                'nolambda' : f"{barcode} lambda-filtered",
                'lambda'   : f"{barcode} lambda-mapping",
                '_default' : f"{barcode} {part}" }

    return pls.get(part, pls['_default'])

# NanoPlot makes many plots, but here are the ones we care about:
NANOPLOT_PLOT_LIST = [ "HistogramReadlength", "LengthvsQualityScatterPlot_dot", "NumberOfReads_Over_Time" ]

def one_cell_inputs(wc):
    """ Provide the inputs to the rule below
    """
    cell = wc.cell
    base = cellname_to_base(cell)
    barcodes = SC[cell].keys()

    xxpand = partial(expand, cell=cell, base=base, bc=barcodes, pf=FASTQ_PARTS)
    return dict(
        fastq_gz     = xxpand("{base}_{bc}_{pf}.fastq.gz"),
        seq_summary  = xxpand("{base}_sequencing_summary.txt.gz"),
        counts       = xxpand("counts/{base}_{bc}_{pf}.fastq.count"),
        lambda_bam   = xxpand("{base}_{bc}_lambda.bam"),
        lambda_stats = xxpand("lambdaqc/{base}_{bc}_lambda.bam.stats"),
        lambda_qmap  = xxpand("lambdaqc/{base}_{bc}_lambda.bam.qmap"),
        blobs        = xxpand("blob/{base}_{bc}_plots.yaml"),
        nanoplot_r   = xxpand("nanoplot/{cell}/{outfile}", outfile = ["NanoStats.yaml",
                                                                      "NanoPlot-report.html" ]),
        nanoplot_p   = xxpand("nanoplot/{cell}/{p}.png", p = NANOPLOT_PLOT_LIST),
        nanoplot_t   = xxpand("nanoplot/{cell}/{p}.__thumb.png", p = NANOPLOT_PLOT_LIST),
        fast5_meta   = f"{cell}/cell_fast5_metadata.yaml",

        # Previously we added:
        #minionqc     = "minionqc/{cell}/summary.yaml",
        )

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input: unpack(one_cell_inputs)
    run:
        ci = parse_cell_name(RUN, wildcards.cell)
        base = cellname_to_base(wildcards.cell)

        # Add pass/fail number of files. If the RUNDIR is not available we'll not attempt
        # to recalculate this. I could still count fast5 files but a better approach would be
        # to serialize SC[wildcards.cell] and just keep hold of this for info.
        cell_content = SC[wildcards.cell]
        for pf in ["pass", "fail"]:
            try:
                # Maybe we can't guarantee the counts in fastq and fast5 match?!
                # Nope - they should always match.
                cell_files = { k: [f for cfb in cell_content.values() for f in cfb[f'{k}_{pf}']]
                               for k in ["fastq", "fast5"] }
                # Add in gzipped file counts
                cell_files[f'fastq'] += [f for cfb in cell_content.values() for f in cfb[f'fastq.gz_{pf}']]
                ci['Files in '+pf] = len(cell_files['fastq'])
                ci['Files in fast5 '+pf] = len(cell_files['fast5'])

            except KeyError as e:
                # FIXME - see comment above.
                logger.warning(repr(e))
                ci['Files in '+pf] = "unknown"

        # Add info from the .count files - add in '_label' and '_barcode' but I'm not sure
        # if we will split them out by barcode in the report or just add them up.
        ci['_counts'] = []
        for pf in FASTQ_PARTS:
            for bc in SC[wildcards.cell]:
                # Reading from input.counts
                cdata = load_yaml(f"counts/{base}_{bc}_{pf}.fastq.count")
                cdata['_label'] = f"{label_for_part(pf).capitalize()} reads"
                cdata['_barcode'] = bc
                ci['_counts'].append(cdata)

        # Link to other reports. I'm now using paths relative to the file rather than the
        # root of the run dir.
        ci['_blobs'] = [ f"../../{b}" for b in input.blobs ]
        ci['_nanoplot'] = f"../../{input.nanoplot_r[0]}"
        #ci['_minionqc'] = str(input.minionqc)

        # Fold in the fast5_metadata
        mdata = load_yaml(str(input.fast5_meta))
        ci.update(mdata)

        dump_yaml(ci, str(output))

# Note this currently gets the metadata from the gzipped fast5 meaning it can be run
# even after the originals are deleted, except that in this case SC will not be populated.
# The weird input definition tries to handle this case by looking for the output file
# directly. In either case we only need a single file, so return the first.
rule fast5_metadata:
    output: "{cell}/cell_fast5_metadata.yaml"
    input:
        fast5 = lambda wc: find_representative_fast5(wc.cell, SC)
    shell:
        "get_fast5_metadata.py -v {input} > {output}"

# Only if the run directory is in place, load the rules that filter, compress and combine the
# original files.
if RUNDIR:
    include: "Snakefile.rundir"

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
# Note this will produce many plots but there are only a few we care to check.
rule nanoplot:
    output:
        stats  = "nanoplot/{cell}/NanoStats.txt",
        rep    = "nanoplot/{cell}/NanoPlot-report.html",
        plots  = [ "nanoplot/{cell}/" + p + ".png" for p in NANOPLOT_PLOT_LIST ],
        thumbs = [ "nanoplot/{cell}/" + p + ".__thumb.png" for p in NANOPLOT_PLOT_LIST ],
    input:
        summary      = lambda wc: f"{cellname_to_base(wc.cell)}_sequencing_summary.txt.gz",
        passlist     = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass_fastq.list"
                                    for bc in SC[wc.cell] ],
        passlist_gz  = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass_fastq.gz.list"
                                    for bc in SC[wc.cell] ],
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(str(input.summary))
        shell(r'cd nanoplot/{wildcards.cell} ; rm -f *.png *.html *.log')

        # We need a special case when there are no passing reads as NanoPlot will choke.
        # Tempting to look at SC[wildcards.cell]['fastq_pass'] but this will not be available
        # if RUNDIR is not set, so inspect input.passlist which should be preserved. Since
        # adding barcode support, there may be multiple passlist files.
        # Since adding .fastq.gz support there are these to consider too.
        if all( os.path.getsize(p) == 0 for p in input.passlist ):
            if all( os.path.getsize(p) <= 32 for p in input.passlist_gz ):
                shell("touch {output}")
                return

        shell(r'cd nanoplot/{wildcards.cell} ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob(f"nanoplot/{wildcards.cell}/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{base}_sequencing_summary.txt.gz", base=[cellname_to_base(c) for c in SC]),
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

# Stats calculated on the lambda.bam files using samtools stat and qualimap
rule samstats:
    output: "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.stats"
    input:  "{cell}/{all}_lambda.bam"
    threads: 4
    run:
        if os.path.getsize(str(input)) == 0:
            shell("echo 'Zero-byte BAM File' > {output}")
        else:
            shell("{TOOLBOX} samtools stats -d -@ {threads} {input} > {output}")

rule qualimap:
    output:
        txt  = "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.qmap",
        html = "lambdaqc/{cell}/{all,[^/]+}_lambda_qmap/qualimapReport.html"
    input:  "{cell}/{all}_lambda.bam"
    params:
        windows = 5000
    threads: 8
    run:
        fail_reason = None
        if os.path.getsize(str(input)) == 0:
            fail_reason = 'Zero-byte BAM File'
        else:
            # Inspect the file to see if it has some reads
            try:
                shell("set +o pipefail ; {TOOLBOX} samtools view {input} | grep -q .")
            except CalledProcessError:
                fail_reason = 'No reads in BAM File'

        if fail_reason:
            # Nothing to QC. Make empty outputs to keep Snakemake happy.
            shell("echo {fail_reason:q} > {output.txt}")
            shell("echo {fail_reason:q} > {output.html}")
            return

        outdir = os.path.dirname(output.html)

        shell("{TOOLBOX} qualimap bamqc -bam {input} -nt {threads} -nw {params.windows} -outdir {outdir}")
        shell("mv {outdir}/genome_results.txt {output.txt}")

# This file normally generated by the driver but it's possible it may be missing.
# In which case we may just create an empty file in order to proceed and avoid the
# missing input error.
# I don't want to have rules in the Snakefile querying the LIMS.
# Obviously this means that use of '-F' will clobber the contents.
rule project_realnames:
    output: "project_realnames.yaml"
    shell:
        "touch {output}"

# Finally add the blob plotting rules.
include: "Snakefile.blob"

