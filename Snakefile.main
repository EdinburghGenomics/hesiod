#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake

import yaml
from snakemake.utils import format
from pprint import pformat
from glob import glob
import subprocess

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        in config['cells'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    all_done = [ '/'.join(fs.split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/final_summary.txt")) ]

    if not all_done:
        logger.error("No complete cells found")

    if 'cells' in config:
        cells = [ s for s in all_done if s in config['cells'].split('\t') ]
    else:
        cells = all_done

    res = { c: dict() for c in cells }

    for c, d in res.items():
        for category in 'fastq_pass fast5_pass fastq_fail fast5_fail'.split():
            filetype = category.split('_')[0]
            d[category] = [ f[len(RUNDIR) + 1:]
                            for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

    return res

# Prevent re-scanning for every cluster job
if '--allowed-rules' in sys.argv:
    SC = dict()
else:
    SC = scan_cells()

if 'logger' in globals():
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in SC.items() }

    logger.info("SC = " + pformat(sc_counts, width=140))

# Aggregate by category for all cells
ALL_FASTQ_PASS = [ f for c, d in SC.items() for category in ['fastq_pass'] for f in d[category] ]
ALL_FAST5_PASS = [ f for c, d in SC.items() for category in ['fast5_pass'] for f in d[category] ]
ALL_FASTQ_FAIL = [ f for c, d in SC.items() for category in ['fastq_fail'] for f in d[category] ]
ALL_FAST5_FAIL = [ f for c, d in SC.items() for category in ['fast5_fail'] for f in d[category] ]
ALL_FILES = [ f for c, d in SC.items() for category in d for f in d[category] ]

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
localrules: main, one_cell_info
rule main:
    input:
        yaml     = [ c + '.info.yml' for c in SC ]

# Per-cell driver rule.
rule one_cell_info:
    output: "{cell}.info.yml"
    input:
        all_md5  = [format("md5sums/{f}.gz.md5") for f in ALL_FILES ],
        all_gz   = [format("{f}.gz") for f in ALL_FILES ],
        blobs    = ["blob/fastq_pass.plots.yml"],
        nanoplot = ["nanoplot/whatever"]
    shell:
        "touch {output}"

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-(
rule gzip_md5sum_fastx:
    output:
        gz  = "{foo}.fast{x,.}.gz",
        md5 = "md5sums/{foo}.fast{x,.}.gz.md5"
    input:  RUNDIR + "/{foo}.fast{x}"
    threads: 2
    shell:
       r"""{PIGZ} -vnT -p{threads} -c -9 -b512 {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

rule blobs:
    output: "blob/fastq_pass.plots.yml"
    shell:
        "touch {output}"

rule nanoplot:
    output: "nanoplot/whatever"
    shell:
        "touch {output}"


