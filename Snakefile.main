#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pformat

from snakemake.utils import format

from hesiod import glob, parse_cell_name, dump_yaml, load_yaml

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        in config['cells'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    if 'cells' in config:
        cells = config['cells'].split('\t')

        for c in cells:
            assert glob(format("{RUNDIR}/{c}/fastq_pass/")), format("Invalid cell (no fastq_pass): {c}")
    else:
        # Look for cells with a final_summary.txt
        cells = [ '/'.join(fs.split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/final_summary.txt")) ]

    if not cells:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cells }

    for c, d in res.items():
        for pf in "pass fail".split():
            for filetype in "fastq fast5".split():
                category = format("{filetype}_{pf}")
                d[category] = [ f[len(RUNDIR) + 1:]
                                for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

    # Sanity-check that the file counts match!
    for c, d in res.items():
        for pf in "pass fail".split():
            assert len(d[format("fastq_{pf}")]) == len(d[format("fast5_{pf}")]), \
                    format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf})")

    return res

# Cells are in the form {lib}/{cell}. Some outputs are aggregated by {lib} and other are per-cell.
SC = scan_cells()
LIBS = sorted(set([ c.split('/')[0] for c in SC ]))
RUN = os.path.basename(os.path.realpath(RUNDIR)).split('.')[0]

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            lib = load_yaml(y)['Library']
            # FIXME - not sure this is the right place for getting the project ID,
            # or the right thing to do when the regex fails.
            mo =  re.match(r"([0-9]{5})[A-Z]{2}", lib)
            if mo:
                plist.add(mo.group(1))
            else:
                plist.add(lib)
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

if 'logger' in globals():
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in SC.items() }

    logger.info( "RUN {}, SC =\n{}".format(RUN, pformat(sc_counts, width=140)) )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - added aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.
localrules: main, one_cell
rule main:
    input:
        yaml     = expand("{cell}/cell_info.yaml", cell=SC),
        minionqc = "minionqc/combinedQC/summary.yaml"
    output:
        plist    = "projects_ready.txt",
        rep      = "all_reports/report.html"
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        panrep = "{}/report.{}cells.pan".format(os.path.dirname(output.rep), len(input.yaml))
        shell("make_report.py -o {panrep} --minionqc {input.minionqc} {input.yaml}")

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {panrep}.html {panrep}
        ''')

        shell("ln -snr {panrep}.html {output.rep}")

# Per-cell driver rule.
rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fast5_gz   = lambda wc: expand("{f}.gz",             f=(SC[wc.cell]['fast5_pass'] + SC[wc.cell]['fast5_fail'])),
        fast5_md5  = lambda wc: expand("md5sums/{f}.gz.md5", f=(SC[wc.cell]['fast5_pass'] + SC[wc.cell]['fast5_fail'])),
        fastq_gz   = lambda wc: expand( "{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.fastq.gz",
                                            cell = [wc.cell],
                                            run = [RUN],
                                            ci = [parse_cell_name(wc.cell)],
                                            pf = "pass fail".split() ),
        fastq_md5  = lambda wc: expand( "md5sums/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.fastq.gz.md5",
                                            cell = [wc.cell],
                                            run = [RUN],
                                            ci = [parse_cell_name(wc.cell)],
                                            pf = "pass fail".split() ),
        blobs      = "blob/{cell}/plots.yml",
        nanoplot   = "nanoplot/{cell}/NanoStats.yaml",
        minionqc   = "minionqc/{cell}/summary.yaml",
    run:
        ci = parse_cell_name(wildcards.cell)
        ci['Run'] = RUN

        # Add pass/fail counts
        cell_files = SC[wildcards.cell]
        for pf in "pass fail".split():
            # We already confirmed the fastq and fast5 counts match
            ci['Files in '+pf] = len(cell_files[format('fastq_{pf}')])

        # Link to other reports
        ci['_blobs'] = str(input.blobs)
        ci['_nanoplot'] = str(input.nanoplot)
        ci['_minionqc'] = str(input.minionqc)

        dump_yaml(ci, str(output))

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-(
rule gzip_md5sum_fast5:
    output:
        gz  = "{foo}.fast5.gz",
        md5 = "md5sums/{foo}.fast5.gz.md5"
    input: RUNDIR + "/{foo}.fast5"
    threads: 2
    shell:
       r"""{PIGZ} -v -p{threads} -c {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

# This one concatenates and zips and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz   = "{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz",
        md5  = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz.md5",
        fofn = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        # Here we run the risk of blowing out the command line lenght limit, so avoid
        # that.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"xargs -d '\n' cat <{output.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# Alternatively I could tar the files. Note that to cat the files in a single stream I can do:
# $ tar -xaOf all_pass_fastq.tar.gz
# But most users won't get that :-(
# TODO - remove this. We won't be doing it.
rule concat_tar_md5sum_fastq:
    priority: 100
    output:
        tar  = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.tar.gz",
        md5  = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.tar.gz.md5",
        fofn = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"tar -cT {output.fofn} --xform='s,.*/\(.*/\),\1,' | {PIGZ} -p{threads} -c  > {output.tar}")

        shell(r"( cd `dirname {output.tar}` && md5sum `basename {output.tar}` ) > {output.md5}")

def find_sequencing_summary(wc):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). Either way there should be only one.
    """
    found = glob(format("{RUNDIR}/{wc.cell}/*_sequencing_summary.txt")) + \
            glob(format("{RUNDIR}/{wc.cell}/sequencing_summary/*_sequencing_summary.txt"))

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              " - found {}.".format(len(found)) )

    return found

rule gzip_sequencing_summary:
    output:
        link = "{cell}/sequencing_summary.txt.gz"
    input:
        summary = find_sequencing_summary
    threads: 2
    run:
        # First copy the file, preserving the name. Then link.
        gzfile = "{}/{}.gz".format(wildcards.cell, os.path.basename(str(input.summary)))

        shell(r"{PIGZ} -p{threads} -c <{input.summary} >{gzfile}")
        shell(r"cd {wildcards.cell} && ln -sn `basename {gzfile}` `basename {output.link}`")

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output: "nanoplot/{cell}/NanoStats.txt"
    input:  "{cell}/sequencing_summary.txt.gz"
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(input[0])
        shell(r'cd "$(dirname {output})" ; rm -f *.png *.html *.log')
        shell(r'cd "$(dirname {output})" ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob("nanoplot/*/*.png") + glob("nanoplot/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{cell}/sequencing_summary.txt.gz", cell=SC)
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")


# Blob plotting is copied from SMRTino but I've now updated the version of NT and also fixed the funny
# Y-axis scale. This change needs to be ported back to SMRTino.
# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = 10000
BLOB_CHUNKS = 100
BLOB_LEVELS = "phylum order species".split()
BLOB_PARTS  = "pass".split()

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 (3?) blob plots and thumbnails for
# each, and arranges the plots into 2 (1?) rows of 3 columns as we wish to
# display them.
localrules: list_blob_plots
rule list_blob_plots:
    output: "blob/{cell}/plots.yml"
    input:
        png = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}{thumb}.png",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      taxlevel = BLOB_LEVELS,
                      extn = "cov0 read_cov.cov0".split(),
                      thumb = ['.__thumb', ''] ),
        sample = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      ss = [BLOB_SUBSAMPLE] ),
    run:
        # We want to know how big the subsample actually was, as it may be <BLOB_SUBSAMPLE, so check
        # the FASTA
        wc = wildcards
        counts = [ l.split()[0] for l in
                   shell("grep -Ho '^>' {input.sample} | uniq -c", iterable=True) ]

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here in preserving order but I can make a nested list comprehension.
        plots = [ dict(title = 'Taxonomy for {pf} reads ({c} sequences) by {l}'.format(
                                                                            pf = s,
                                                                            c = counts[n],
                                                                            l = ', '.join(BLOB_LEVELS) ),
                       files = [ [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}.png".format(
                                                                            cell = wc.cell,
                                                                            ci = parse_cell_name(wc.cell),
                                                                            run = [RUN],
                                                                            pf = s,
                                                                            taxlevel = taxlevel,
                                                                            extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]

                      ) for n, s in enumerate(BLOB_PARTS) ]

        print(dump_yaml(plots, str(output)))

# Convert to FASTA and subsample and munge the headers
rule fastq_to_subsampled_fasta:
    output: "blob/{foo}+sub{n}.fasta"
    input: "{foo}.fastq.gz"
    shell:
       r"""{PIGZ} -d -c {input} | \
             {TOOLBOX} seqtk seq -ACNU - |\
             {TOOLBOX} seqtk sample - {wildcards.n} |\
             sed 's,/,_,g' > {output}
        """

# Makes a .complexity file for our FASTA file
# {foo} will be blob/{cell}/{ci[Run]}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
# I'm filtering out repeated rows to reduce the size of the BLOB DB
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        'LC_ALL=C ; ( for i in {input} ; do sort -u -k1,2 "$i" ; done ) > {output}'

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} blast_nt -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty
# {foo} is blob/{cell}.subreads or blob/{cell}.scraps
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.parts}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
        """mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/tmp \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/tmp.blobDB.json {output.json}
        """

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of png files
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png",          "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        thumbsize = "320x320"
    shadow: 'shallow'
    shell:
        """ mkdir blob_tmp
            export BLOB_COVERAGE_LABEL=Non-Dustiness
            {TOOLBOX} blobtools blobplot -i {input.json} -o blob_tmp/ --dustplot --sort_first no-hit,other,undef -r {wildcards.taxlevel}
            tree blob_tmp
            mv blob_tmp/tmp.*.stats.txt {output.stats}
            mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.cov0.png {output.plotc[0]}
            mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png {output.plotr[0]}
            {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize} {output.plotc[1]}
            {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize} {output.plotr[1]}
        """


## End of BLOB plotter rules ##

