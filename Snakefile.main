#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_ref)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pformat

from snakemake.utils import format

from hesiod import glob, parse_cell_name, groupby, dump_yaml, load_yaml

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')
if not os.path.exists(RUNDIR):
    logger.warning(format("WARNING: {RUNDIR} is not a directory. Suppressing rules that reference RUNDIR."))
    RUNDIR = None

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        by driver.sh in config['cellsready'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    if 'cells' in config:
        # Believe what we are told
        cells = config['cells'].split('\t')
    elif RUNDIR:
        # Look for valid cells
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/fastq_pass/")) ]
    else:
        # Look for cells here in the output (presumably the run already processed and the raw dir was removed)
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("*/*/sequencing_summary.txt.gz")) ]

    if 'cellsready' in config:
        # This should include the cells to be processed now AND those already processed.
        cellsready = config['cellsready'].split('\t')

        for c in cellsready:
            assert c in cells, format("Invalid cell (no fastq_pass or not listed in config[cells]): {c}")
    elif RUNDIR:
        # Look for cells with a final_summary.txt
        cellsready = [ c for c in cells if os.path.exists(format("{RUNDIR}/{c}/final_summary.txt")) ]
    else:
        # If there is no RUNDIR this makes most sense by default.
        cellsready = cells

    if not cellsready:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cellsready }

    if RUNDIR:
        for c, d in res.items():
            for pf in "pass fail".split():
                for filetype in "fastq fast5".split():
                    category = format("{filetype}_{pf}")
                    d[category] = [ f[len(RUNDIR) + 1:]
                                    for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

        # Sanity-check that the file counts match. But we seem to be seeing this in several runs,
        # so maybe it's not an error? Oh - it's because I'm seeing Urmi's already-combined reads, so this
        # is definitely a bad thing.
        for c, d in res.items():
            for pf in "pass fail".split():
                if len(d[format("fastq_{pf}")]) != len(d[format("fast5_{pf}")]):
                    raise RuntimeError( format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf}):\n") +
                                        sc_counts(res) )

    # Return the dict of stuff to process, and other counts we've calculated
    return res, dict( cells = len(cells),
                      cellsready = len(cellsready),
                      cellsaborted = 0 )

def sc_counts(sc_dict, width=140):
    """ Make a printable summary of SC
    """
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in sc_dict.items() }

    return pformat(sc_counts, width=width)


# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the project number.
# Some outputs are aggregated by {lib}, some by {project} and other are per-cell.
# FIXME - nothing is aggregated per-lib just yet?!
SC, COUNTS = scan_cells()
CELLS_PER_LIB     = groupby( SC, lambda c: parse_cell_name(c)['Library'], True)
CELLS_PER_PROJECT = groupby( SC, lambda c: parse_cell_name(c)['Project'], True)
RUN = os.path.basename(os.path.realpath('.')).split('.')[0]

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            lib = load_yaml(y)['Library']
            # FIXME - not sure this is the right place for getting the project ID,
            # or the right thing to do when the regex fails.
            mo =  re.match(r"([0-9]{5})[A-Z]{2}", lib)
            if mo:
                plist.add(mo.group(1))
            else:
                plist.add(lib)
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

if 'logger' in globals():
    logger.info( "RUN {}, SC =\n{}".format(RUN, sc_counts(SC)) )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.
localrules: main, one_cell, pack_fast5
rule main:
    output:
        plist     = "projects_ready.txt",
        panrep    = "all_reports/report.{}cells.pan".format(len(SC)),
        rep       = "all_reports/report.{}cells.pan.html".format(len(SC)),
        replink   = "all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml"
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        # shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} --minionqc {input.minionqc} {input.yaml}")
        shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} {input.yaml}")

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {output.rep} {output.panrep}
        ''')

        shell("ln -snr {output.rep} {output.replink}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this.
rule pack_fast5:
    input:
        fast5_gz   = lambda wc: [ "{}.gz".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],
        fast5_md5  = lambda wc: [ "md5sums/{}.gz.md5".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM)
#  but as requested by Urmi we also retain the combined 'pass' files.
#  All of the files get an md5sum
def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return "{}/{}_{Library}_{CellID}".format(c, RUN, **parse_cell_name(c))

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fastq_gz     = lambda wc: expand( "{base}_{pf}.fastq.gz",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = "pass nolambda fail".split() ),
        counts       = lambda wc: expand( "counts/{base}_{pf}.fastq.count",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = "pass nolambda fail".split() ),
        lambda_bam   = lambda wc: expand( "{base}_lambda.bam",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_stats = lambda wc: expand( "lambdaqc/{base}_lambda.bam.stats",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_qmap  = lambda wc: expand( "lambdaqc/{base}_lambda.bam.qmap",
                                            base = [cellname_to_base(wc.cell)] ),
        blobs        = "blob/{cell}/plots.yaml",
        nanoplot     = [ "nanoplot/{cell}/NanoStats.yaml", "nanoplot/{cell}/NanoPlot-report.html" ],
        #minionqc     = "minionqc/{cell}/summary.yaml",
    run:
        ci = parse_cell_name(wildcards.cell)
        ci['Run'] = RUN

        # Add pass/fail number of files. If the RUNDIR is not available we'll not attempt
        # to recalculate this. I could still count fast5 files but a better approach would be
        # to serialize SC[wildcards.cell] and just keep hold of this for info.
        cell_files = SC[wildcards.cell]
        for pf in "pass fail".split():
            try:
                # Maybe we can't guarantee the counts in fastq and fast5 match?!
                if len(cell_files[format('fastq_{pf}')]) == len(cell_files[format('fast5_{pf}')]):
                    ci['Files in '+pf] = len(cell_files[format('fastq_{pf}')])
                else:
                    ci['Files in '+pf] = "{} ({} in fast5_{pf})".format( len(cell_files[format('fastq_{pf}')]),
                                                                         len(cell_files[format('fast5_{pf}')]),
                                                                         pf = pf )
            except KeyError:
                # FIXME - see comment above.
                ci['Files in '+pf] = "unknown"

        # Add info from the .count files
        ci['_counts'] = []
        for label, cf in zip(["Lambda-filtered passing reads", "All failed reads"], input.counts):
            cdata = load_yaml(cf)
            cdata['_label'] = label
            ci['_counts'].append(cdata)

        # Link to other reports
        ci['_blobs'] = str(input.blobs)
        ci['_nanoplot'] = str(input.nanoplot[0])
        #ci['_minionqc'] = str(input.minionqc)

        dump_yaml(ci, str(output))

def slurp_file(filename):
    """Standard file slurper. Returns a list of lines.
    """
    with open(filename) as fh: res = list(fh)
    if res[-1] == '': res.pop()
    return res

# Only if the run directory is in place, load the rules that filter, compress and combine the
# original files.
if RUNDIR:
    include: "Snakefile.rundir"

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output:
        stats = "nanoplot/{cell}/NanoStats.txt",
        rep   = "nanoplot/{cell}/NanoPlot-report.html"
    input:  "{cell}/sequencing_summary.txt.gz"
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(input[0])
        shell(r'cd "$(dirname {output.stats})" ; rm -f *.png *.html *.log')
        shell(r'cd "$(dirname {output.stats})" ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob("nanoplot/*/*.png") + glob("nanoplot/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{cell}/sequencing_summary.txt.gz", cell=SC)
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

# Stats calculated on the lambda.bam files using samtools stat and qualimap
rule samstats:
    output: "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.stats"
    input:  "{cell}/{all}_lambda.bam"
    threads: 4
    shell:
       r'''{TOOLBOX} samtools stats -d -@ {threads} {input} > {output}'''

rule qualimap:
    output:
        txt  = "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.qmap",
        html = "lambdaqc/{cell}/{all,[^/]+}_lambda_qmap/qualimapReport.html"
    input:  "{cell}/{all}_lambda.bam"
    params:
        windows = 5000
    threads: 8
    shell:
       r'''_outdir=`dirname {output.html}`
           {TOOLBOX} qualimap bamqc -bam {input} -nt {threads} -nw {params.windows} -outdir "$_outdir"
           mv "$_outdir"/genome_results.txt {output.txt}
        '''

# Finally add the blob plotting rules.
include: "Snakefile.blob"

