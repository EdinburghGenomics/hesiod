#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake

import yaml
from snakemake.utils import format
from pprint import pformat
import subprocess

def glob():
    """Regular glob() is useful but we want consistent sort order."""
    from glob import glob
    return lambda p: sorted( (f.rstrip('/') for f in glob(os.path.expanduser(p))) )
glob = glob()

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        in config['cells'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    all_done = [ '/'.join(fs.split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/final_summary.txt")) ]

    if not all_done:
        logger.error("No complete cells found")

    if 'cells' in config:
        cells = [ s for s in all_done if s in config['cells'].split('\t') ]
    else:
        cells = all_done

    res = { c: dict() for c in cells }

    for c, d in res.items():
        for pf in "pass fail".split():
            for filetype in "fastq fast5".split():
                category = format("{filetype}_{pf}")
                d[category] = [ f[len(RUNDIR) + 1:]
                                for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

    # Sanity-check that the file counts match!
    for c, d in res.items():
        for pf in "pass fail".split():
            assert len(d[format("fastq_{pf}")]) == len(d[format("fast5_{pf}")]), \
                    format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf})")

    return res

# Cells are in the form {lib}/{cell}. Some outputs are aggregated by {lib} and other are per-cell.
SC = scan_cells()
LIBS = sorted(set([ c.split('/')[0] for c in SC ]))

def parse_cell_name(cell):
    """Things we get from parsing wildcards.cell"""
    res = dict( run = os.path.basename(os.path.realpath(RUNDIR)).split('.')[0],
                cell = cell )

    # Now shred the filename.
    mo = re.match(r'([0-9A-Z-]+)/(\d{6})_(\d+)_([0-9A-Z-]+)_([0-9a-f]{8})$', cell)
    if mo:
        for n, x in enumerate("library date number slot cellid checksum".split()):
            res[x] = mo.group(n+1)
    else:
        # Not good, but we'll try
        res['library'] = cell.split('/')[0]
        res['cellid'] = cell.split('_')[-2] if '_' in cell else 'UNKNOWN'

    return res

if 'logger' in globals():
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in SC.items() }

    logger.info("SC = " + pformat(sc_counts, width=140))

# Main target is one yml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - added aggregated stats per lib
localrules: main, one_cell
rule main:
    input:
        yaml     = expand("{cell}/cell_info.yml", cell=SC),


# Per-cell driver rule.
rule one_cell:
    output: "{cell}/cell_info.yml"
    input:
        fast5_gz   = lambda wc: expand("{f}.gz",             f=(SC[wc.cell]['fast5_pass'] + SC[wc.cell]['fast5_fail'])),
        fast5_md5  = lambda wc: expand("md5sums/{f}.gz.md5", f=(SC[wc.cell]['fast5_pass'] + SC[wc.cell]['fast5_fail'])),
        fastq_gz   = lambda wc: expand( "{cell}/{ci[run]}_{ci[library]}_{ci[cellid]}_{pf}.fastq.gz",
                                            cell = [wc.cell],
                                            ci = [parse_cell_name(wc.cell)],
                                            pf = "pass fail".split() ),
        fastq_md5  = lambda wc: expand( "md5sums/{cell}/{ci[run]}_{ci[library]}_{ci[cellid]}_{pf}.fastq.gz.md5",
                                            cell = [wc.cell],
                                            ci = [parse_cell_name(wc.cell)],
                                            pf = "pass fail".split() ),
        blobs      = "blob/fastq_pass.plots.yml",
        nanoplot   = "nanoplot/{cell}/NanoStats.yml",
    run:
        cell_files = SC[wildcards.cell]
        ci = parse_cell_name(wildcards.cell)

        with open(str(output)) as ofh:
            print("Run: {}".format(ci['run']), file=ofh)
            print("Cell: {}".format(ci['cell']), file=ofh)

            print("Library: '{}'".format( ci.get('library')), file=ofh)
            print("Date: '{}'".format(    ci.get('date')), file=ofh)
            print("Number: {}".format(    ci.get('number')), file=ofh)
            print("Slot: {}".format(      ci.get('slot')), file=ofh)
            print("CellID: {}".format(    ci.get('cellid')), file=ofh)
            print("Checksum: '{}'".format(ci.get('checksum')), file=ofh)

            for pf in "pass fail".split():
                # We already confirmed the fastq and fast5 counts match
                print("Files in {}: {}".format(pf, cell_files[format('fastq_{pf}')]), file=ofh)

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-(
rule gzip_md5sum_fast5:
    output:
        gz  = "{foo}.fast5.gz",
        md5 = "md5sums/{foo}.fast5.gz.md5"
    input: RUNDIR + "/{foo}.fast5"
    threads: 2
    shell:
       r"""{PIGZ} -v -p{threads} -c {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

# This one concatenates and zips and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
rule concat_gzip_md5sum_fastq:
    output:
        gz   = "{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz",
        md5  = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz.md5",
        fofn = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        # Here we run the risk of blowing out the command line lenght limit, so avoid
        # that.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"xargs -d '\n' cat <{output.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# Alternatively I could tar the files. Note that to cat the files in a single stream I can do:
# $ tar -xaOf all_pass_fastq.tar.gz
# But most users won't got that :-(
rule concat_tar_md5sum_fastq:
    output:
        tar  = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.tar.gz",
        md5  = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.tar.gz.md5",
        fofn = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"tar -cT {output.fofn} --xform='s,.*/\(.*/\),\1,' | {PIGZ} -p{threads} -c  > {output.tar}")

        shell(r"( cd `dirname {output.tar}` && md5sum `basename {output.tar}` ) > {output.md5}")

def find_sequencing_summary(wc):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). Either way there should be only one.
    """
    found = glob(format("{RUNDIR}/{wc.cell}/*_sequencing_summary.txt")) + \
            glob(format("{RUNDIR}/{wc.cell}/sequencing_summary/*_sequencing_summary.txt"))

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              " - found {}.".format(len(found)) )

    return found

rule gzip_sequencing_summary:
    output:
        link = "{cell}/sequencing_summary.txt.gz"
    input:
        summary = find_sequencing_summary
    threads: 2
    run:
        # First copy the file, preserving the name. Then link.
        gzfile = "{}/{}.gz".format(wildcards.cell, os.path.basename(str(input.summary)))

        shell(r"{PIGZ} -p{threads} -c <{input.summary} >{gzfile}")
        shell(r"cd {wildcards.cell} && ln -sn `basename {gzfile}` `basename {output.link}`")

rule blobs:
    output: "blob/fastq_pass.plots.yml"
    shell:
        "touch {output}"

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output: "nanoplot/{cell}/NanoStats.txt"
    input:  "{cell}/sequencing_summary.txt.gz"
    shell:
       r"""ap="$(readlink -f {input})"
           cd "$(dirname {output})"
           rm -f *.png *.html *.log
           NanoPlot --summary "$ap"
        """

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"
