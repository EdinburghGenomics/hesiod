#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_ref)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pformat

from snakemake.utils import format

from hesiod import glob, parse_cell_name, groupby, dump_yaml, load_yaml

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')
if not os.path.exists(RUNDIR):
    logger.warning(format("WARNING: {RUNDIR} is not a directory. Suppressing rules that reference RUNDIR."))
    RUNDIR = None

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        by driver.sh in config['cellsready'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    if 'cells' in config:
        # Believe what we are told
        cells = config['cells'].split('\t')
    elif RUNDIR:
        # Look for valid cells
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/fastq_pass/")) ]
    else:
        # Look for cells here in the output (presumably the run already processed and the raw dir was removed)
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("*/*/sequencing_summary.txt.gz")) ]

    if 'cellsready' in config:
        # This should include the cells to be processed now AND those already processed.
        cellsready = config['cellsready'].split('\t')

        for c in cellsready:
            assert c in cells, format("Invalid cell (no fastq_pass or not listed in config[cells]): {c}")
    elif RUNDIR:
        # Look for cells with a final_summary.txt
        cellsready = [ c for c in cells if os.path.exists(format("{RUNDIR}/{c}/final_summary.txt")) ]
    else:
        # If there is no RUNDIR this makes most sense by default.
        cellsready = cells

    if not cellsready:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cellsready }

    if RUNDIR:
        for c, d in res.items():
            for pf in "pass fail".split():
                for filetype in "fastq fast5".split():
                    category = format("{filetype}_{pf}")
                    d[category] = [ f[len(RUNDIR) + 1:]
                                    for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

        # Sanity-check that the file counts match. But we seem to be seeing this in several runs,
        # so maybe it's not an error? Oh - it's because I'm seeing Urmi's already-combined reads, so this
        # is definitely a bad thing.
        for c, d in res.items():
            for pf in "pass fail".split():
                if len(d[format("fastq_{pf}")]) != len(d[format("fast5_{pf}")]):
                    raise RuntimeError( format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf}):\n") +
                                        sc_counts(res) )

    # Return the dict of stuff to process, and other counts we've calculated
    return res, dict( cells = len(cells),
                      cellsready = len(cellsready),
                      cellsaborted = 0 )

def sc_counts(sc_dict, width=140):
    """ Make a printable summary of SC
    """
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in sc_dict.items() }

    return pformat(sc_counts, width=width)


# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the project number.
# Some outputs are aggregated by {lib}, some by {project} and other are per-cell.
# FIXME - nothing is aggregated per-lib just yet?!
SC, COUNTS = scan_cells()
CELLS_PER_LIB     = groupby( SC, lambda c: parse_cell_name(c)['Library'], True)
CELLS_PER_PROJECT = groupby( SC, lambda c: parse_cell_name(c)['Project'], True)
RUN = os.path.basename(os.path.realpath('.')).split('.')[0]

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            lib = load_yaml(y)['Library']
            # FIXME - not sure this is the right place for getting the project ID,
            # or the right thing to do when the regex fails.
            mo =  re.match(r"([0-9]{5})[A-Z]{2}", lib)
            if mo:
                plist.add(mo.group(1))
            else:
                plist.add(lib)
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

if 'logger' in globals():
    logger.info( "RUN {}, SC =\n{}".format(RUN, sc_counts(SC)) )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.
localrules: main, one_cell, pack_fast5
rule main:
    output:
        plist     = "projects_ready.txt",
        panrep    = "all_reports/report.{}cells.pan".format(len(SC)),
        rep       = "all_reports/report.{}cells.pan.html".format(len(SC)),
        replink   = "all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml"
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        # shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} --minionqc {input.minionqc} {input.yaml}")
        shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} {input.yaml}")

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {output.rep} {output.panrep}
        ''')

        shell("ln -snr {output.rep} {output.replink}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this.
rule pack_fast5:
    input:
        fast5_gz   = lambda wc: [ "{}.gz".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],
        fast5_md5  = lambda wc: [ "md5sums/{}.gz.md5".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM)
#  but as requested by Urmi we also retain the combined 'pass' files.
#  All of the files get an md5sum
def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return "{}/{}_{Library}_{CellID}".format(c, RUN, **parse_cell_name(c))

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fastq_gz     = lambda wc: expand( "{base}_{pf}.fastq.gz",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = "pass nolambda fail".split() ),
        counts       = lambda wc: expand( "counts/{base}_{pf}.fastq.count",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = "pass nolambda fail".split() ),
        lambda_bam   = lambda wc: expand( "{base}_lambda.bam",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_stats = lambda wc: expand( "lambdaqc/{base}_lambda.bam.stats",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_qmap  = lambda wc: expand( "lambdaqc/{base}_lambda.bam.qmap",
                                            base = [cellname_to_base(wc.cell)] ),
        blobs        = "blob/{cell}/plots.yaml",
        nanoplot     = [ "nanoplot/{cell}/NanoStats.yaml", "nanoplot/{cell}/NanoPlot-report.html" ],
        #minionqc     = "minionqc/{cell}/summary.yaml",
    run:
        ci = parse_cell_name(wildcards.cell)
        ci['Run'] = RUN

        # Add pass/fail number of files. If the RUNDIR is not available we'll not attempt
        # to recalculate this. I could still count fast5 files but a better approach would be
        # to serialize SC[wildcards.cell] and just keep hold of this for info.
        cell_files = SC[wildcards.cell]
        for pf in "pass fail".split():
            try:
                # Maybe we can't guarantee the counts in fastq and fast5 match?!
                if len(cell_files[format('fastq_{pf}')]) == len(cell_files[format('fast5_{pf}')]):
                    ci['Files in '+pf] = len(cell_files[format('fastq_{pf}')])
                else:
                    ci['Files in '+pf] = "{} ({} in fast5_{pf})".format( len(cell_files[format('fastq_{pf}')]),
                                                                         len(cell_files[format('fast5_{pf}')]),
                                                                         pf = pf )
            except KeyError:
                # FIXME - see comment above.
                ci['Files in '+pf] = "unknown"

        # Add info from the .count files
        ci['_counts'] = []
        for label, cf in zip(["Lambda-filtered passing reads", "All failed reads"], input.counts):
            cdata = load_yaml(cf)
            cdata['_label'] = label
            ci['_counts'].append(cdata)

        # Link to other reports
        ci['_blobs'] = str(input.blobs)
        ci['_nanoplot'] = str(input.nanoplot[0])
        #ci['_minionqc'] = str(input.minionqc)

        dump_yaml(ci, str(output))

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-( ...But I fixed it in upstream now :-)
if RUNDIR:
    rule gzip_md5sum_fast5:
        output:
            gz  = "{foo}.fast5.gz",
            md5 = "md5sums/{foo}.fast5.gz.md5"
        input:
            RUNDIR + "/{foo}.fast5"
        threads: 2
        shell:
           r"""{PIGZ} -v -p{threads} -c {input} > {output.gz}
               ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
            """

# This one concatenates and zips and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
if RUNDIR:
    rule concat_gzip_md5sum_fastq:
        priority: 100
        output:
            gz    = "{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz",
            md5   = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz.md5",
            count = "counts/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.count",
            fofn  = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
        input:
            fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
        threads: 6
        run:
            # Here we run the risk of blowing out the command line length limit, so avoid
            # that.
            with open(output.fofn, "w") as fh:
                for fname in input.fastq: print(fname, file=fh)

            shell(r"xargs -d '\n' cat <{output.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

            # Base counter
            shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.count}")

            shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# We've decided to do the lambda mapping up front. Here's the rule to combine the filtered
# fastq.gz files, by simply concatenating them rather than unzip/rezip.
# Note this is specific to the pass reads. We don't partition the fail reads (should we??)
if RUNDIR:
    rule concat_md5sum_nolambda_fastq:
        priority: 100
        output:
            gz    = "{cell}/{all,[^/]+}_nolambda.fastq.gz",
            md5   = "md5sums/{cell}/{all,[^/]+}_nolambda.fastq.gz.md5",
            count = "counts/{cell}/{all,[^/]+}_nolambda.fastq.count",
            fofn  = "{cell}/{all,[^/]+}_nolambda_fastq.list",
            fofn2 = temp("fastq_pass_tmp/{cell}/{all,[^/]+}_nlfq_count.list")
        input:
            fastq = lambda wc: [format("fastq_pass_tmp/{f}.nlfq.gz") for f in SC[wc.cell]['fastq_pass']],
            count = lambda wc: [format("fastq_pass_tmp/{f}.nlfq.count") for f in SC[wc.cell]['fastq_pass']]
        run:
            # Avoid blowing the command line limit by listing files in a file. This could be
            # marked temporary but it's small and handy for debugging.
            with open(output.fofn, "w") as fh:
                for fname in input.fastq: print(fname, file=fh)

            with open(output.fofn2, "w") as fh:
                for fname in input.count: print(fname, file=fh)

            # Simply concatenate the gzipped files
            shell(r"xargs -d '\n' cat <{output.fofn} > {output.gz}")

            # Combine base counts
            shell(r"xargs -d '\n' cat <{output.fofn2} | fq_base_combiner.awk -r fn=`basename {output.gz} .gz` > {output.count}")

            # Checksum over the whole file
            shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

if RUNDIR:
    rule concat_md5sum_lambda_bam:
        output:
            bam  = "{cell}/{all,[^/]+}_lambda.bam",
            md5  = "md5sums/{cell}/{all,[^/]+}_lambda.bam.md5",
            fofn = "{cell}/{all,[^/]+}_lambda_bam.list"
        input:
            bam  = lambda wc: [format("fastq_pass_tmp/{f}.lambda.bam") for f in SC[wc.cell]['fastq_pass']]
        threads: 4
        run:
            with open(output.fofn, "w") as fh:
                for fname in input.bam: print(fname, file=fh)

            # samtools merge (files in fastq_pass_tmp should be pre-sorted)
            shell("{TOOLBOX} samtools merge -@ {threads} -l9 -b {output.fofn} {output.bam}")

            shell(r"( cd `dirname {output.bam}` && md5sum `basename {output.bam}` ) > {output.md5}")

# This rule produces the stuff in fastq_pass_tmp, and will normally be applied
# to every individual fastq_pass file prior to combining the results. It uses nested
# implicit FIFOs. I call it "bashception".
# Note this type of construct is vulnerable to the kill-before-flush bug but it seems with
# this arrangement we are OK. See:
#  https://www.pixelbeat.org/docs/coreutils-gotchas.html
# I plan to fully clean out the temp dir as a separate op outside of Snakemake,
# deleting it once a cell is done.
if RUNDIR:
    rule map_lambda:
        output:
            fq    = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.nlfq.gz"),
            bam   = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.lambda.bam"),
            count = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.nlfq.count"),
        input:
            RUNDIR + "/{cell}/{f}.fastq"
        params:
            ref    = os.environ.get('REFS', '.') + '/phage_lambda.mmi',
            rg     = r'@RG\tID:1\tSM:{cell}\tPL:promethion',
            mmopts = '-t 1 -a --MD --sam-hit-only -y --secondary=no -x map-ont '
        threads: 6
        shell:
           r'''{TOOLBOX} minimap2 {params.mmopts} -R {params.rg:q} {params.ref} {input} | tee >( \
                    lambda_splitter.awk \
                        -v paf=/dev/stdin \
                        -v nolambda=>({PIGZ} -c -p{threads} > {output.fq}) \
                        {input} ) | \
               {TOOLBOX} samtools sort - -@ {threads} -o {output.bam}
               {PIGZ} -dc -p{threads} {output.fq} | fq_base_counter.awk -r fn=`basename {output.fq} .gz` > {output.count}
            '''

def find_sequencing_summary(wc):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). Either way there should be only one.
    """
    found = glob(format("{RUNDIR}/{wc.cell}/*_sequencing_summary.txt")) + \
            glob(format("{RUNDIR}/{wc.cell}/sequencing_summary/*_sequencing_summary.txt"))

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              " - found {}.".format(len(found)) )

    return found

def slurp_file(filename):
    """Standard file slurper. Returns a list of lines.
    """
    with open(filename) as fh: res = list(fh)
    if res[-1] == '': res.pop()
    return res

# This rule must be suppressed if the upstream is missing
if RUNDIR:
    rule gzip_sequencing_summary:
        output:
            link = "{cell}/sequencing_summary.txt.gz"
        input:
            summary = find_sequencing_summary
        threads: 2
        run:
            # First copy the file, preserving the name. Then link.
            gzfile = "{}/{}.gz".format(wildcards.cell, os.path.basename(str(input.summary)))
            shell(r"{PIGZ} -p{threads} -c <{input.summary} >{gzfile}")

            shell(r"ln -snr {gzfile} {output.link}")

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output:
        stats = "nanoplot/{cell}/NanoStats.txt",
        rep   = "nanoplot/{cell}/NanoPlot-report.html"
    input:  "{cell}/sequencing_summary.txt.gz"
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(input[0])
        shell(r'cd "$(dirname {output.stats})" ; rm -f *.png *.html *.log')
        shell(r'cd "$(dirname {output.stats})" ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob("nanoplot/*/*.png") + glob("nanoplot/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{cell}/sequencing_summary.txt.gz", cell=SC)
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

# Stats calculated on the lambda.bam files using samtools stat and qualimap
rule samstats:
    output: "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.stats"
    input:  "{cell}/{all}_lambda.bam"
    threads: 4
    shell:
       r'''{TOOLBOX} samtools stats -d -@ {threads} {input} > {output}'''

rule qualimap:
    output:
        txt  = "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.qmap",
        html = "lambdaqc/{cell}/{all,[^/]+}_lambda_qmap/qualimapReport.html"
    input:  "{cell}/{all}_lambda.bam"
    params:
        windows = 5000
    threads: 8
    shell:
       r'''_outdir=`dirname {output.html}`
           {TOOLBOX} qualimap bamqc -bam {input} -nt {threads} -nw {params.windows} -outdir "$_outdir"
           mv "$_outdir"/genome_results.txt {output.txt}
        '''

# Blob plotting is copied from SMRTino but I've now updated the version of NT and also fixed the funny
# Y-axis scale. This change needs to be ported back to SMRTino. Also note I'm running the plots on the
# nolambda files so we'll never see lambda in the blobs. Maybe we should reconsider this??
# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = 10000
BLOB_CHUNKS = 100
BLOB_LEVELS = "phylum order species".split()

# For testing, make blobs of all three passing outputs. Probably we just want "pass" in the
# final version.
BLOB_PARTS  = ["pass", "nolambda", "lambda"]

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 (3?) blob plots and thumbnails for
# each, and arranges the plots into 2 (1?) rows of 3 columns as we wish to
# display them.
# We also depend on the CSV outputs of parse_blob_table, which will be rendered to
# markdown within the make_report script. These are grouped by project not cell, so
# we need a separate rule below.
localrules: per_cell_blob_plots, per_project_blob_tables, fasta_numseqs, parse_blob_table

rule fasta_numseqs:
    output: "{foo}.fasta.numseqs"
    input:  "{foo}.fasta"
    shell:  "grep -o '^>' {input} | wc -l > {output}"

rule per_cell_blob_plots:
    output: "blob/{cell}/plots.yaml"
    input:
        png = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}{thumb}.png",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      taxlevel = BLOB_LEVELS,
                      extn = "cov0 read_cov.cov0".split(),
                      thumb = ['.__thumb', ''] ),
        sample = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta.numseqs",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      ss = [BLOB_SUBSAMPLE] ),
    run:
        # We want to know how big the subsample actually was, as it may be < BLOB_SUBSAMPLE, so check
        # the FASTA, then make a dict of {part: seq_count}
        wc = wildcards
        counts = { part: slurp_file(f)[0]
                   for part, f in zip(BLOB_PARTS, input.sample) }

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here in preserving order but I can make a nested list comprehension.
        # Group by BLOB_PARTS with an appropriate title.
        plots = [ dict(title = 'Taxonomy for {pf} reads ({c} sequences) by {l}'.format(
                                                                            pf = pf,
                                                                            c = counts[pf],
                                                                            l = ', '.join(BLOB_LEVELS) ),

                       files = [ [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}.png".format(
                                                                            cell = wc.cell,
                                                                            run = RUN,
                                                                            ci = parse_cell_name(wc.cell),
                                                                            pf = pf,
                                                                            taxlevel = taxlevel,
                                                                            extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]
                      ) for pf in BLOB_PARTS ]

        dump_yaml(plots, str(output))

# See above. input.sample is as above but now for all the cells.
rule per_project_blob_tables:
    output: "blob/blobstats_by_project.yaml"
    input:
        csv = expand("blob/blobstats.{project}.{pf}.{taxlevel}.csv",
                                          project = CELLS_PER_PROJECT,
                                          pf = BLOB_PARTS,
                                          taxlevel = BLOB_LEVELS ),
        sample = lambda wc: [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta.numseqs".format(
                                          cell = cell,
                                          pf = pf,
                                          run = [RUN],
                                          ci = parse_cell_name(cell),
                                          ss = [BLOB_SUBSAMPLE] )
                                for cell in SC for pf in BLOB_PARTS ],
    run:
        # List ALL the tables of blob stats to show per project
        # The fiddly bit is incorporating the counts dict. See doc/per_project_info.txt
        res = dict()
        for p in CELLS_PER_PROJECT:
            # For each project p make a list of tables, first by PF, then by tax level.
            csv_list = res[p] = list()
            for pf in BLOB_PARTS:
                for tl in BLOB_LEVELS:
                    # For each table we need a dict of counts matching the table lines,
                    # ie. one item per cell.
                    counts_dict = dict()
                    for cell in CELLS_PER_PROJECT[p]:
                        # Recreate the filename from input.sample
                        f = "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta.numseqs".format(
                                                            cell = cell,
                                                            run = [RUN],
                                                            ci =  parse_cell_name(cell),
                                                            pf = pf,
                                                            ss = [BLOB_SUBSAMPLE] )
                        # Did I get that right??
                        assert f in [str(s) for s in input.sample]
                        counts_dict[cell] = slurp_file(f)[0]

                    csv = "blob/blobstats.{project}.{pf}.{taxlevel}.csv".format(
                                    project = p,
                                    pf = pf,
                                    taxlevel = tl )
                    assert csv in [str(s) for s in input.csv]

                    csv_list.append( dict( title = "BLAST hits for {pf} reads by {taxlevel}".format(
                                                            pf = pf,
                                                            taxlevel = tl ),
                                           csv = csv,
                                           counts = counts_dict ) )

        # Finally we have the data structure we need.
        dump_yaml(res, str(output))

rule parse_blob_table:
    output: "blob/blobstats.{project}.{pf}.{taxlevel}.csv"
    input:
        lambda wc: [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.blobplot.stats.txt".format(
                                                            cell = cell,
                                                            run = RUN,
                                                            ci = parse_cell_name(cell),
                                                            **vars(wc) )
                         for cell in CELLS_PER_PROJECT[wc.project] ]
    params:
        pct_limit = 1
    shell:
        "{TOOLBOX} parse_blob_table {output} {params.pct_limit} {input}"

# Convert to FASTA and subsample and munge the headers
# seqtk seq -ACNU == to-fasta, no-comments, no-ambiguous, uppercase
rule fastq_to_subsampled_fasta:
    output: "blob/{foo,.+(_pass|_nolambda)}+sub{n}.fasta"
    input: "{foo}.fastq.gz"
    shell:
       r"""{PIGZ} -d -c {input} | \
             {TOOLBOX} seqtk seq -ACNU - | \
             {TOOLBOX} seqtk sample - {wildcards.n} | \
             sed 's,/,_,g' > {output}
        """

# Version that works on lambda.bam files
# The sed filter may well be redundant here.
rule bam_to_subsampled_fasta:
    output: "blob/{foo,.+(_lambda)}+sub{n}.fasta"
    input: "{foo}.bam"
    shell:
       r"""{TOOLBOX} samtools fasta {input} | \
             {TOOLBOX} seqtk sample - {wildcards.n} | \
             sed 's,/,_,g' > {output}
        """

# Makes a .complexity file for our FASTA file
# {foo} will be blob/{cell}/{ci[Run]}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
# I'm filtering out repeated rows to reduce the size of the BLOB DB - there can
# be a lot of repeats so this is worth running on the cluster.
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        'LC_ALL=C ; ( for i in {input} ; do sort -u -k1,2 "$i" ; done ) > {output}'

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} blast_nt -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty,
# hence the final touch. Note this will 'split' a completely empty file if you ask it to.
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.parts} {output.list}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
# If reads_sample is empty this will generate an empty file
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
       r'''if [ ! -s {input.reads_sample} ] ; then touch {output.json} ; exit 0 ; fi
           mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/tmp \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/tmp.blobDB.json {output.json}
        '''

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of png files
# If blobDB.json is empty, make some empty images
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png",          "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        thumbsize = "320x320"
    shadow: 'shallow'
    shell:
       r'''mkdir blob_tmp
           if [ -s {input.json} ] ; then
               export BLOB_COVERAGE_LABEL=Non-Dustiness
               {TOOLBOX} blobtools plot -i {input.json} -o blob_tmp/ --dustplot --sort_first no-hit,other,undef -r {wildcards.taxlevel}
               tree blob_tmp
               mv blob_tmp/tmp.*.stats.txt {output.stats}
               mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.cov0.png {output.plotc[0]}
               mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png {output.plotr[0]}
           else
               echo "No data" > {output.stats}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotc[0]}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotr[0]}
           fi
           {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize} {output.plotc[1]}
           {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize} {output.plotr[1]}
        '''


## End of BLOB plotter rules ##

