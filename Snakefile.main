#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_ref)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pformat

from itertools import product
from subprocess import CalledProcessError

from hesiod import glob, parse_cell_name, groupby, dump_yaml, load_yaml

logger = snakemake.logging.logger

TOOLBOX = f'env PATH="{os.environ["TOOLBOX"]}:$PATH"'
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')
if not os.path.exists(RUNDIR):
    logger.warning(f"WARNING: {RUNDIR} is not a directory. Suppressing rules that reference RUNDIR.")
    RUNDIR = None

def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return parse_cell_name(RUN, c)['Base']

def scan_cells(rundir, config):
    """ Work out all the cells to process. Normally simple since the list is just passed in
        by driver.sh in config['cellsready'] but I do want to be able to process all by default so
        there is some scanning capability too.
        Then get a list of all the files per cell.
        This function must not fail if files are missing because that blocks the whole Snakefile from
        running even if you want to use an unrelated rule.

        res structure is { cell : barcode : 'fastX_pass' : [ list of files ] }
    """
    if 'cells' in config:
        # Believe what we are told
        cells = config['cells'].split('\t')
    elif rundir:
        # Look for valid cells.
        cells = sorted(set( '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(f"{rundir}/*/*/fastq_????/") ))
    else:
        # Look for cells here in the output (presumably the run already processed and the raw dir was removed)
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(f"*/*/cell_info.yaml") ]

    if 'cellsready' in config:
        # This should include the cells to be processed now AND those already processed.
        cellsready = config['cellsready'].split('\t')

        for c in cellsready:
            # FIXME - test this condition
            assert c in cells, f'Invalid cell (no fastq_pass or not listed in config["cells"]): {c}'
    elif rundir:
        # Look for cells with a final_summary.txt (as made by MinKNOW). Note the new name in MinKNOW 3.6+
        cellsready = [ c for c in cells if (
                       glob(f"{rundir}/{c}/final_summary.txt") or
                       glob(f"{rundir}/{c}/final_summary_*_*.txt") ) ]
    else:
        # If there is no RUNDIR this makes most sense by default.
        cellsready = cells

    if not cellsready:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cellsready }

    if rundir:
        for c, d in res.items():
            for pf, filetype in product(["pass", "fail"], ["fastq", "fast5"]):
                category = f"{filetype}_{pf}"
                # Collect un-barcoded files
                non_barcoded_files = [ f[len(rundir) + 1:]
                                       for f in glob(f"{rundir}/{c}/{category}/*.{filetype}") ]
                barcoded_files = [ f[len(rundir) + 1:]
                                   for f in glob(f"{rundir}/{c}/{category}/*/*.{filetype}") ]
                if non_barcoded_files:
                    d.setdefault('.', dict())[category] = non_barcoded_files
                for bf in barcoded_files:
                    _, barcode, _ = bf[len(c) + 1:].split('/')
                    d.setdefault(barcode, dict()).setdefault(category, list()).append(bf)

        # Add in empty lists for missing items.
        for c, d in res.items():
            for bcdict in d.values():
                for pf, filetype in product(["pass", "fail"], ["fastq", "fast5"]):
                    # If the barcode is there we should have all four categories
                    bcdict.setdefault(f"{filetype}_{pf}", [])

        # Sanity-check that the file counts match. But we seem to be seeing this in several runs,
        # so maybe it's not an error? Oh - it's because I'm seeing Urmi's already-combined reads, so this
        # is definitely a bad thing.
        for c, d in res.items():
            for pf in ["pass", "fail"]:
                fastq_sum = sum( len(fileslist[f"fastq_{pf}"]) for fileslist in d.values() )
                fast5_sum = sum( len(fileslist[f"fast5_{pf}"]) for fileslist in d.values() )
                if fastq_sum != fast5_sum:
                    raise RuntimeError( f"Mismatch between count of FASTQ and FAST5 files for {c} ({pf}):\n" +
                                        sc_counts(res) )

    # Return the dict of stuff to process, and other counts we've calculated
    return res, dict( cells = len(cells),
                      cellsready = len(cellsready),
                      cellsaborted = 0 )

def sc_counts(sc_dict, width=140):
    """ Make a printable summary of SC
    """
    # Make a dict that just shows the counts
    sc_counts = { c : { bc: { category: f"<{len(filelist)} files>"
                              for category, filelist in d.items() } }
                  for c, bc_dict in sc_dict.items()
                  for bc, d in bc_dict.items() }

    # Ideally I'd use sort_dicts=False but that needs Py3.8
    return pformat(sc_counts, width=width)

## End of functions ## Leave this comment in place to help the unit tests.

# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the project number.
# Some outputs are aggregated by {lib}, some by {project} and other are per-cell.
# FIXME - nothing is aggregated per-lib just yet?!
SC, COUNTS = scan_cells(RUNDIR, config)
RUN = os.path.basename(os.path.realpath('.')).split('.')[0]
CELLS_PER_LIB     = groupby( SC, lambda c: parse_cell_name(RUN, c)['Library'], True)
CELLS_PER_PROJECT = groupby( SC, lambda c: parse_cell_name(RUN, c)['Project'], True)

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            plist.add(load_yaml(y)['Project'])
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

if 'logger' in globals():
    logger.info( f"RUN {RUN}, SC =\n{sc_counts(SC)}" )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.

localrules: main, one_cell, pack_fast5
rule main:
    output:
        plist     = "projects_ready.txt",
        panrep    = f"all_reports/report.{len(SC)}cells.pan",
        rep       = f"all_reports/report.{len(SC)}cells.pan.html",
        replink   = "all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml",
        realnames = "project_realnames.yaml",
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        # shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} --minionqc {input.minionqc} {input.yaml}")
        shell(r'''make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} \
                                    --blobstats {input.blobstats} --realnames {input.realnames} \
                                    {input.yaml}
               ''')

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/easter.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {output.rep} {output.panrep}
        ''')

        shell("ln -snr {output.rep} {output.replink}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this. So to fully process a run you need to call main AND pack_fast5
rule pack_fast5:
    input:
        fast5_gz   = lambda wc: [ f"{f}.gz" for cell in SC
                                            for pf in ['fast5_pass', 'fast5_fail']
                                            for f in SC[cell][pf] ],
        fast5_md5  = lambda wc: [ f"md5sums/{f}.gz.md5" for cell in SC
                                                    for pf in ['fast5_pass', 'fast5_fail']
                                                    for f in SC[cell][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM)
#  but as requested by Urmi we also retain the combined 'pass' files.
#  All of the files get an md5sum

# See also BLOB_PARTS in Snakefile.blob
FASTQ_PARTS = ["pass", "nolambda", "fail"]
PART_LABELS = { 'pass'     : "all passed",
                'fail'     : "all failed",
                'nolambda' : "lambda-filtered passed",
                'lambda'   : "lambda-mapping passed" }

# NanoPlot makes many plots, but here are the ones we care about:
NANOPLOT_PLOT_LIST = [ "HistogramReadlength", "LengthvsQualityScatterPlot_dot", "NumberOfReads_Over_Time" ]

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fastq_gz     = lambda wc: expand( "{base}_{pf}.fastq.gz",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = FASTQ_PARTS ),
        seq_summary  = lambda wc: expand( "{base}_sequencing_summary.txt.gz",
                                            base = [cellname_to_base(wc.cell)] ),
        counts       = lambda wc: expand( "counts/{base}_{pf}.fastq.count",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = FASTQ_PARTS ),
        lambda_bam   = lambda wc: expand( "{base}_lambda.bam",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_stats = lambda wc: expand( "lambdaqc/{base}_lambda.bam.stats",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_qmap  = lambda wc: expand( "lambdaqc/{base}_lambda.bam.qmap",
                                            base = [cellname_to_base(wc.cell)] ),
        blobs        = "blob/{cell}/plots.yaml",
        nanoplot_r   = [ "nanoplot/{cell}/NanoStats.yaml", "nanoplot/{cell}/NanoPlot-report.html" ],
        nanoplot_p   = [ "nanoplot/{cell}/" + p + ".png" for p in NANOPLOT_PLOT_LIST ],
        nanoplot_t   = [ "nanoplot/{cell}/" + p + ".__thumb.png" for p in NANOPLOT_PLOT_LIST ],
        fast5_meta   = "{cell}/cell_fast5_metadata.yaml",
        #minionqc     = "minionqc/{cell}/summary.yaml",
    run:
        ci = parse_cell_name(RUN, wildcards.cell)

        # Add pass/fail number of files. If the RUNDIR is not available we'll not attempt
        # to recalculate this. I could still count fast5 files but a better approach would be
        # to serialize SC[wildcards.cell] and just keep hold of this for info.
        cell_files = SC[wildcards.cell]
        for pf in "pass fail".split():
            try:
                # Maybe we can't guarantee the counts in fastq and fast5 match?!
                # Nope - they should always match.
                if len(cell_files[f'fastq_{pf}']) == len(cell_files[f'fast5_{pf}')]):
                    ci['Files in '+pf] = len(cell_files[f'fastq_{pf}'])
                else:
                    ci['Files in '+pf] = "{} ({} in fast5_{pf})".format( len(cell_files[f'fastq_{pf}']),
                                                                         len(cell_files[f'fast5_{pf}']),
                                                                         pf = pf )
            except KeyError:
                # FIXME - see comment above.
                ci['Files in '+pf] = "unknown"

        # Add info from the .count files
        ci['_counts'] = []
        for label, cf in zip(FASTQ_PARTS, input.counts):
            cdata = load_yaml(cf)
            cdata['_label'] = f"{PART_LABELS.get(label, label).capitalize()} reads"
            ci['_counts'].append(cdata)

        # Link to other reports. I'm now using paths relative to the file rather than the
        # root of the run dir.
        ci['_blobs'] = f"../../{input.blobs}"
        ci['_nanoplot'] = f"../../{input.nanoplot_r[0]}"
        #ci['_minionqc'] = str(input.minionqc)

        # Fold in the fast5_metadata
        mdata = load_yaml(str(input.fast5_meta))
        ci.update(mdata)

        dump_yaml(ci, str(output))

# Note this currently gets the metadata from the gzipped fast5 meaning it can be run
# even after the originals are deleted, except that in this case SC will not be populated.
# The weird input definition tries to handle this case by looking for the output file
# directly. In either case we only need a single file, so return the first.
def i_fast5_metadata(wc):
    if SC[wc.cell]['fast5_pass']:
        # Depend on the first file provided by SC[wc.cell]
        return [ f"{f}.gz" for f in SC[wc.cell]['fast5_pass'] ][:1]
    elif SC[wc.cell]['fast5_fail']:
        # Somethimes everything fails
        return [ f"{f}.gz" for f in SC[wc.cell]['fast5_fail'] ][:1]
    else:
        # Look for an existing output file.
        return glob("{cell}/fast5_????/*.fast5.gz".format(**vars(wc)))[:1]

rule fast5_metadata:
    output: "{cell}/cell_fast5_metadata.yaml"
    input:
        fast5 = i_fast5_metadata
    shell:
        "get_fast5_metadata.py -v {input} > {output}"

# Only if the run directory is in place, load the rules that filter, compress and combine the
# original files.
if RUNDIR:
    include: "Snakefile.rundir"

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
# Note this will produce many plots but there are only a few we care to check.
rule nanoplot:
    output:
        stats  = "nanoplot/{cell}/NanoStats.txt",
        rep    = "nanoplot/{cell}/NanoPlot-report.html",
        plots  = [ "nanoplot/{cell}/" + p + ".png" for p in NANOPLOT_PLOT_LIST ],
        thumbs = [ "nanoplot/{cell}/" + p + ".__thumb.png" for p in NANOPLOT_PLOT_LIST ],
    input:
        summary  = lambda wc: f"{cellname_to_base(wc.cell)}_sequencing_summary.txt.gz",
        passlist = lambda wc: f"{cellname_to_base(wc.cell)}_pass_fastq.list"
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(str(input.summary))
        shell(r'cd "$(dirname {output.stats})" ; rm -f *.png *.html *.log')

        # We need a special case when there are no passing reads as NanoPlot will choke.
        # Tempting to look at SC[wildcards.cell]['fastq_pass'] but this will not be available
        # if RUNDIR is not set, so inspect input.passlist which should be preserved.
        if os.path.getsize(str(input.passlist)) == 0:
           shell("touch {output}")
           return

        shell(r'cd "$(dirname {output.stats})" ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob("nanoplot/*/*.png") + glob("nanoplot/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{base}_sequencing_summary.txt.gz", base=[cellname_to_base(c) for c in SC]),
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

# Stats calculated on the lambda.bam files using samtools stat and qualimap
rule samstats:
    output: "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.stats"
    input:  "{cell}/{all}_lambda.bam"
    threads: 4
    shell:
       r'''{TOOLBOX} samtools stats -d -@ {threads} {input} > {output}'''

rule qualimap:
    output:
        txt  = "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.qmap",
        html = "lambdaqc/{cell}/{all,[^/]+}_lambda_qmap/qualimapReport.html"
    input:  "{cell}/{all}_lambda.bam"
    params:
        windows = 5000
    threads: 8
    run:
        fail_reason = None
        if os.path.getsize(str(input)) == 0:
            fail_reason = 'Zero-byte BAM File'
        else:
            # Inspect the file to see if it has some reads
            try:
                shell("set +o pipefail ; {TOOLBOX} samtools view {input} | grep -q .")
            except CalledProcessError:
                fail_reason = 'No reads in BAM File'

        if fail_reason:
            # Nothing to QC. Make empty outputs to keep Snakemake happy.
            shell("echo {fail_reason:q} > {output.txt}")
            shell("echo {fail_reason:q} > {output.html}")
            return

        outdir = os.path.dirname(output.html)

        shell("{TOOLBOX} qualimap bamqc -bam {input} -nt {threads} -nw {params.windows} -outdir {outdir}")
        shell("mv {outdir}/genome_results.txt {output.txt}")

# This file normally generated by the driver but it's possible it may be missing.
# In which case we may just create an empty file in order to proceed and avoid the
# missing input error.
# I don't want to have rules in the Snakefile querying the LIMS.
# Obviously this means that use of '-F' will clobber the contents.
rule project_realnames:
    output: "project_realnames.yaml"
    shell:
        "touch {output}"

# Finally add the blob plotting rules.
include: "Snakefile.blob"

