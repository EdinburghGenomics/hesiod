#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_ref)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pformat

from snakemake.utils import format

from hesiod import glob, parse_cell_name, dump_yaml, load_yaml

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        by driver.sh in config['cellsready'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    if 'cells' in config:
        # Believe what we are told
        cells = config['cells'].split('\t')
    else:
        # Look for valid cells
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/fastq_pass/")) ]

    if 'cellsready' in config:
        # This should include the cells to be processed now AND those already processed.
        cellsready = config['cellsready'].split('\t')

        for c in cellsready:
            assert c in cells, format("Invalid cell (no fastq_pass or not listed in config[cells]): {c}")
    else:
        # Look for cells with a final_summary.txt
        cellsready = [ c for c in cells if os.path.exists(format("{RUNDIR}/{c}/final_summary.txt")) ]

    if not cellsready:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cellsready }

    for c, d in res.items():
        for pf in "pass fail".split():
            for filetype in "fastq fast5".split():
                category = format("{filetype}_{pf}")
                d[category] = [ f[len(RUNDIR) + 1:]
                                for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

    # Sanity-check that the file counts match. But we seem to be seeing this in several runs,
    # so maybe it's not an error? Oh - it's because I'm seeing Urmi's already-combined reads, so this
    # is definitely a bad thing.
    for c, d in res.items():
        for pf in "pass fail".split():
            if len(d[format("fastq_{pf}")]) != len(d[format("fast5_{pf}")]):
                raise RuntimeError( format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf}):\n") +
                                    sc_counts(res) )

    # Return the dict of stuff to process, and other counts we've calculated
    return res, dict( cells = len(cells),
                      cellsready = len(cellsready),
                      cellsaborted = 0 )

def sc_counts(sc_dict, width=140):
    """ Make a printable summary of SC
    """
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in sc_dict.items() }

    return pformat(sc_counts, width=width)


# Cells are in the form {lib}/{cell}. Some outputs are aggregated by {lib} and other are per-cell.
SC, COUNTS = scan_cells()
LIBS = sorted(set([ c.split('/')[0] for c in SC ]))
RUN = os.path.basename(os.path.realpath(RUNDIR)).split('.')[0]

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            lib = load_yaml(y)['Library']
            # FIXME - not sure this is the right place for getting the project ID,
            # or the right thing to do when the regex fails.
            mo =  re.match(r"([0-9]{5})[A-Z]{2}", lib)
            if mo:
                plist.add(mo.group(1))
            else:
                plist.add(lib)
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

if 'logger' in globals():
    logger.info( "RUN {}, SC =\n{}".format(RUN, sc_counts(SC)) )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - added aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.
localrules: main, one_cell, pack_fast5
rule main:
    input:
        yaml     = expand("{cell}/cell_info.yaml", cell=SC),
        minionqc = "minionqc/combinedQC/summary.yaml"
    output:
        plist    = "projects_ready.txt",
        rep      = "all_reports/report.html"
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        panrep = "{}/report.{}cells.pan".format(os.path.dirname(output.rep), len(input.yaml))
        shell("make_report.py -o {panrep} --totalcells {COUNTS[cells]} --minionqc {input.minionqc} {input.yaml}")

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {panrep}.html {panrep}
        ''')

        shell("ln -snr {panrep}.html {output.rep}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this.
rule pack_fast5:
    input:
        fast5_gz   = lambda wc: [ "{}.gz".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],
        fast5_md5  = lambda wc: [ "md5sums/{}.gz.md5".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM)
#  All of the files get an md5sum
def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return "{}/{}_{Library}_{CellID}".format(c, RUN, **parse_cell_name(c))

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fastq_gz   = lambda wc: expand( "{base}_{pf}.fastq.gz",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = "nolambda fail".split() ),
        counts     = lambda wc: expand( "counts/{base}_{pf}.fastq.count",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = "nolambda fail".split() ),
        lambda_bam = lambda wc: expand( "{base}_lambda.bam",
                                            base = [cellname_to_base(wc.cell)] ),
        blobs      = "blob/{cell}/plots.yaml",
        nanoplot   = "nanoplot/{cell}/NanoStats.yaml",
        minionqc   = "minionqc/{cell}/summary.yaml",
    run:
        ci = parse_cell_name(wildcards.cell)
        ci['Run'] = RUN

        # Add pass/fail number of files
        cell_files = SC[wildcards.cell]
        for pf in "pass fail".split():
            # Maybe we can't guarantee the counts in fastq and fast5 match?!
            if len(cell_files[format('fastq_{pf}')]) == len(cell_files[format('fast5_{pf}')]):
                ci['Files in '+pf] = len(cell_files[format('fastq_{pf}')])
            else:
                ci['Files in '+pf] = "{} ({} in fast5_{pf})".format( len(cell_files[format('fastq_{pf}')]),
                                                                     len(cell_files[format('fast5_{pf}')]),
                                                                     pf = pf )

        # Add info from the .count files
        ci['_counts'] = []
        for label, cf in zip(["Lambda-filtered passing reads", "All failed reads"], input.counts):
            cdata = load_yaml(cf)
            cdata['_label'] = label
            ci['_counts'].append(cdata)

        # Link to other reports
        ci['_blobs'] = str(input.blobs)
        ci['_nanoplot'] = str(input.nanoplot)
        ci['_minionqc'] = str(input.minionqc)

        dump_yaml(ci, str(output))

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-(
rule gzip_md5sum_fast5:
    output:
        gz  = "{foo}.fast5.gz",
        md5 = "md5sums/{foo}.fast5.gz.md5"
    input:
        RUNDIR + "/{foo}.fast5"
    threads: 2
    shell:
       r"""{PIGZ} -v -p{threads} -c {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

# This one concatenates and zips and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz    = "{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz",
        md5   = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz.md5",
        count = "counts/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.count",
        fofn  = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        # Here we run the risk of blowing out the command line lenght limit, so avoid
        # that.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"xargs -d '\n' cat <{output.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        # Base counter
        shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.count}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# We've decided to do the lambda mapping up front. Here's the rule to combine the filtered
# fastq.gz files, by simply concatenating them rather than unzip/rezip.
# Note this is specific to the pass reads. We don't partition the fail reads (should we??)
rule concat_md5sum_nolambda_fastq:
    priority: 100
    output:
        gz    = "{cell}/{all,[^/]+}_nolambda.fastq.gz",
        md5   = "md5sums/{cell}/{all,[^/]+}_nolambda.fastq.gz.md5",
        count = "counts/{cell}/{all,[^/]+}_nolambda.fastq.count",
        fofn  = "{cell}/{all,[^/]+}_nolambda_fastq.list",
        fofn2 = temp("fastq_pass_tmp/{cell}/{all,[^/]+}_nlfq_count.list")
    input:
        fastq = lambda wc: [format("fastq_pass_tmp/{f}.nlfq.gz") for f in SC[wc.cell]['fastq_pass']],
        count = lambda wc: [format("fastq_pass_tmp/{f}.nlfq.count") for f in SC[wc.cell]['fastq_pass']]
    run:
        # Avoid blowing the command line limit by listing files in a file. This could be
        # marked temporary but it's small and handy for debugging.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        with open(output.fofn2, "w") as fh:
            for fname in input.count: print(fname, file=fh)

        # Simply concatenate the gzipped files
        shell(r"xargs -d '\n' cat <{output.fofn} > {output.gz}")

        # Combine base counts
        shell(r"xargs -d '\n' cat <{output.fofn2} | fq_base_combiner.awk -r fn=`basename {output.gz} .gz` > {output.count}")

        # Checksum over the whole file
        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

rule concat_md5sum_lambda_bam:
    output:
        bam  = "{cell}/{all,[^/]+}_lambda.bam",
        md5  = "md5sums/{cell}/{all,[^/]+}_lambda.bam.md5",
        fofn = "{cell}/{all,[^/]+}_lambda_bam.list"
    input:
        bam  = lambda wc: [format("fastq_pass_tmp/{f}.lambda.bam") for f in SC[wc.cell]['fastq_pass']]
    threads: 4
    run:
        with open(output.fofn, "w") as fh:
            for fname in input.bam: print(fname, file=fh)

        # samtools merge (files in fastq_pass_tmp should be pre-sorted)
        shell("{TOOLBOX} samtools merge -@ {threads} -l9 -b {output.fofn} {output.bam}")

        shell(r"( cd `dirname {output.bam}` && md5sum `basename {output.bam}` ) > {output.md5}")

# This rule produces the stuff in fastq_pass_tmp, and will normally be applied
# to every individual fastq_pass file. It uses nested implicit FIFOs. I call it "bashception".
# Note this type of construct is vulnerable to the kill-before-flush bug but it seems with
# this arrangement we are OK.
# I plan to fully clean out the temp dir as a separate op outside of Snakemake,
# deleting it once a cell is done.
rule map_lambda:
    output:
        fq    = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.nlfq.gz"),
        bam   = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.lambda.bam"),
        count = temp("fastq_pass_tmp/{cell}/{f,[^/]+}.fastq.nlfq.count"),
    input:
        RUNDIR + "/{cell}/{f}.fastq"
    params:
        ref    = os.environ.get('REFS', '.') + '/phage_lambda.mmi',
        rg     = r'@RG\tID:1\tSM:{cell}\tPL:promethion',
        mmopts = '-t 1 -a --MD --sam-hit-only -y --secondary=no -x map-ont '
    threads: 6
    shell:
       r'''{TOOLBOX} minimap2 {params.mmopts} -R {params.rg:q} {params.ref} {input} | tee >( \
                lambda_splitter.awk \
                    -v paf=/dev/stdin \
                    -v nolambda=>({PIGZ} -c -p{threads} > {output.fq}) \
                    {input} ) | \
           {TOOLBOX} samtools sort - -@ {threads} -o {output.bam}
           {PIGZ} -dc -p{threads} {output.fq} | fq_base_counter.awk -r fn=`basename {output.fq} .gz` > {output.count}
        '''

def find_sequencing_summary(wc):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). Either way there should be only one.
    """
    found = glob(format("{RUNDIR}/{wc.cell}/*_sequencing_summary.txt")) + \
            glob(format("{RUNDIR}/{wc.cell}/sequencing_summary/*_sequencing_summary.txt"))

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              " - found {}.".format(len(found)) )

    return found

rule gzip_sequencing_summary:
    output:
        link = "{cell}/sequencing_summary.txt.gz"
    input:
        summary = find_sequencing_summary
    threads: 2
    run:
        # First copy the file, preserving the name. Then link.
        gzfile = "{}/{}.gz".format(wildcards.cell, os.path.basename(str(input.summary)))

        shell(r"{PIGZ} -p{threads} -c <{input.summary} >{gzfile}")
        shell(r"cd {wildcards.cell} && ln -sn `basename {gzfile}` `basename {output.link}`")

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output: "nanoplot/{cell}/NanoStats.txt"
    input:  "{cell}/sequencing_summary.txt.gz"
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(input[0])
        shell(r'cd "$(dirname {output})" ; rm -f *.png *.html *.log')
        shell(r'cd "$(dirname {output})" ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob("nanoplot/*/*.png") + glob("nanoplot/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{cell}/sequencing_summary.txt.gz", cell=SC)
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")


# Blob plotting is copied from SMRTino but I've now updated the version of NT and also fixed the funny
# Y-axis scale. This change needs to be ported back to SMRTino.
# BLAST S sequences in C chunks
BLOB_SUBSAMPLE = 10000
BLOB_CHUNKS = 100
BLOB_LEVELS = "phylum order species".split()
BLOB_PARTS  = ["nolambda"]

# This is how I want to pass my plots into compile_cell_info.py
# Serves as the driver by depending on the 6 (3?) blob plots and thumbnails for
# each, and arranges the plots into 2 (1?) rows of 3 columns as we wish to
# display them.
localrules: list_blob_plots
rule list_blob_plots:
    output: "blob/{cell}/plots.yaml"
    input:
        png = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}{thumb}.png",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      taxlevel = BLOB_LEVELS,
                      extn = "cov0 read_cov.cov0".split(),
                      thumb = ['.__thumb', ''] ),
        sample = lambda wc: expand( "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta",
                      cell = wc.cell,
                      pf = BLOB_PARTS,
                      run = [RUN],
                      ci = [parse_cell_name(wc.cell)],
                      ss = [BLOB_SUBSAMPLE] ),
    run:
        # We want to know how big the subsample actually was, as it may be <BLOB_SUBSAMPLE, so check
        # the FASTA
        wc = wildcards
        counts = [ next(shell("grep -o '^>' {f} | wc -l", iterable=True)).strip()
                   for f in input.sample ]

        # I need to emit the plots in order in pairs. Unfortunately expand() won't quite
        # cut it here in preserving order but I can make a nested list comprehension.
        plots = [ dict(title = 'Taxonomy for {pf} reads ({c} sequences) by {l}'.format(
                                                                            pf = s,
                                                                            c = counts[n],
                                                                            l = ', '.join(BLOB_LEVELS) ),
                       files = [ [ "blob/{cell}/{run}_{ci[Library]}_{ci[CellID]}_{pf}.{taxlevel}.{extn}.png".format(
                                                                            cell = wc.cell,
                                                                            ci = parse_cell_name(wc.cell),
                                                                            run = RUN,
                                                                            pf = s,
                                                                            taxlevel = taxlevel,
                                                                            extn = extn )
                                    for taxlevel in BLOB_LEVELS ]
                                 for extn in "read_cov.cov0 cov0".split() ]

                      ) for n, s in enumerate(BLOB_PARTS) ]

        dump_yaml(plots, str(output))

# Convert to FASTA and subsample and munge the headers
rule fastq_to_subsampled_fasta:
    output: "blob/{foo}+sub{n}.fasta"
    input: "{foo}.fastq.gz"
    shell:
       r"""{PIGZ} -d -c {input} | \
             {TOOLBOX} seqtk seq -ACNU - |\
             {TOOLBOX} seqtk sample - {wildcards.n} |\
             sed 's,/,_,g' > {output}
        """

# Makes a .complexity file for our FASTA file
# {foo} will be blob/{cell}/{ci[Run]}_{ci[Library]}_{ci[CellID]}_{pf}+sub{ss}.fasta
rule fasta_to_complexity:
    output: "{foo}.complexity"
    input: "{foo}.fasta"
    params:
        level = 10
    shell:
        "{TOOLBOX} dustmasker -level {params.level} -in {input} -outfmt fasta 2>/dev/null | count_dust.py > {output}"

# Combine all the 100 blast reports into one
# I'm filtering out repeated rows to reduce the size of the BLOB DB - there can
# be a lot of repeats so this is worth running on the cluster.
rule merge_blast_reports:
    output: "{foo}.blast"
    input: [ "{{foo}}.blast_part_{:04d}".format(n) for n in range(BLOB_CHUNKS) ]
    shell:
        'LC_ALL=C ; ( for i in {input} ; do sort -u -k1,2 "$i" ; done ) > {output}'

# BLAST a chunk. Note the 'blast_nt' wrapper determines the database to search.
rule blast_chunk:
    output: temp("{foo}.blast_part_{chunk}")
    input: "{foo}.fasta_part_{chunk}"
    threads: 4
    params:
        evalue = '1e-50',
        outfmt = '6 qseqid staxid bitscore'
    shell:
        """{TOOLBOX} blast_nt -query {input} -outfmt '{params.outfmt}' \
           -evalue {params.evalue} -max_target_seqs 1 -out {output}.tmp -num_threads {threads}
           mv {output}.tmp {output}
        """

# Split the FASTA in a fixed number of chunks. All files must be made, even if empty,
# hence the final touch. Note this will 'split' a completely empty file if you ask it to.
rule split_fasta_in_chunks:
    output:
        parts = [ temp("{{foo}}.fasta_part_{:04d}".format(n)) for n in range(BLOB_CHUNKS) ],
        list = "{foo}.fasta_parts"
    input: "{foo}.fasta"
    params:
        chunksize = BLOB_SUBSAMPLE // BLOB_CHUNKS
    shell:
        """awk 'BEGIN {{n_seq=0;n_file=0;}} \
                  /^>/ {{if(n_seq%{params.chunksize}==0){{ \
                         file=sprintf("{wildcards.foo}.fasta_part_%04d", n_file); n_file++; \
                         print file >> "{output.list}"; \
                       }} \
                       print >> file; n_seq++; next; \
                  }} \
                  {{ print >> file; }}' {input}
           touch {output.parts} {output.list}
        """

# Makes a blob db per FASTA using the complexity file as a COV file.
# {foo} is {cell}.subreads or {cell}.scraps
# If reads_sample is empty this will generate an empty file
rule blob_db:
    output:
        json = "blob/{foo}.blobDB.json",
    input:
        blast_results = "blob/{{foo}}+sub{}.blast".format(BLOB_SUBSAMPLE),
        reads_sample  = "blob/{{foo}}+sub{}.fasta".format(BLOB_SUBSAMPLE),
        cov           = "blob/{{foo}}+sub{}.complexity".format(BLOB_SUBSAMPLE)
    shadow: 'shallow'
    shell:
       r'''if [ ! -s {input.reads_sample} ] ; then touch {output.json} ; exit 0 ; fi
           mkdir blob_tmp
           {TOOLBOX} blobtools create -i {input.reads_sample} -o blob_tmp/tmp \
               -t {input.blast_results} -c {input.cov}
           ls -l blob_tmp
           mv blob_tmp/tmp.blobDB.json {output.json}
        '''

# Run the blob plotting command once per set per tax level. Produce a single
# stats file and a pair of png files
# If blobDB.json is empty, make some empty images
rule blob_plot_png:
    output:
        plotc = ["blob/{foo}.{taxlevel}.cov0.png",          "blob/{foo}.{taxlevel}.cov0.__thumb.png"],
        plotr = ["blob/{foo}.{taxlevel}.read_cov.cov0.png", "blob/{foo}.{taxlevel}.read_cov.cov0.__thumb.png"],
        stats = "blob/{foo}.{taxlevel}.blobplot.stats.txt"
    input:
        json = "blob/{foo}.blobDB.json"
    params:
        thumbsize = "320x320"
    shadow: 'shallow'
    shell:
       r'''mkdir blob_tmp
           if [ -s {input.json} ] ; then
               export BLOB_COVERAGE_LABEL=Non-Dustiness
               {TOOLBOX} blobtools blobplot -i {input.json} -o blob_tmp/ --dustplot --sort_first no-hit,other,undef -r {wildcards.taxlevel}
               tree blob_tmp
               mv blob_tmp/tmp.*.stats.txt {output.stats}
               mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.cov0.png {output.plotc[0]}
               mv blob_tmp/tmp.*.{wildcards.taxlevel}.*.blobplot.read_cov.cov0.png {output.plotr[0]}
           else
               echo "No data" > {output.stats}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotc[0]}
               {TOOLBOX} gm_label.sh {params.thumbsize} "No data to plot" {output.plotr[0]}
           fi
           {TOOLBOX} convert {output.plotc[0]} -resize {params.thumbsize} {output.plotc[1]}
           {TOOLBOX} convert {output.plotr[0]} -resize {params.thumbsize} {output.plotr[1]}
        '''


## End of BLOB plotter rules ##

