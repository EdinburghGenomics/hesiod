#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata
# You must run `scan_cells.py > sc_data.yaml` before starting the workflow.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_refs)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pprint, pformat

from functools import partial
from subprocess import CalledProcessError

from hesiod import ( glob, parse_cell_name, load_final_summary,
                     find_sequencing_summary, find_summary,
                     dump_yaml, load_yaml, empty_sc_data )

# This is just here to help testing - Snakemake sets it automatically for workflows
logger = snakemake.logging.logger

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Global vars and setup
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# $TOOLBOX must be set to something
TOOLBOX = f'env PATH="{os.environ["TOOLBOX"]}:$PATH"'
PIGZ    = 'pigz -nT -9 -b512'

# Convert name of CWD to EXPERIMENT
EXPERIMENT = os.path.basename(os.path.realpath('.')).split('.')[0]

POD5_BATCH_SIZE = config.get("pod5_batch_size", 100)

logger.info( f"EXPERIMENT = {EXPERIMENT}" )
logger.info( f"config = {pformat(config)}" )

# Check the PATH includes the directory containing this Snakefile
location_of_snakefile = os.path.dirname(os.path.abspath(workflow.snakefile))
if not any([ os.path.abspath(p) == location_of_snakefile
             for p in os.environ['PATH'].split(':') ]):
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If no explicit input dir supplied, just assume the usual symlink will work...
EXPDIR = config.get('rundata', 'rundata')
if not os.path.exists(EXPDIR):
    logger.info(f"WARNING: {EXPDIR} is not a directory. Suppressing rules that reference EXPDIR.")
    EXPDIR = None

# Load the data structure generated by scan_cells.py
# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the
# project number.
# Some outputs are aggregated {project} and other are per-cell.
try:
    SC_DATA = load_yaml(config.get("sc_data", "sc_data.yaml"), as_tuple="sc_data")
except Exception:
    SC_DATA = empty_sc_data()

SC = SC_DATA.scanned_cells
CELL_COUNT = SC_DATA.counts['cellsready']

logger.info( f"SC =\n{SC_DATA.printable_counts}" )

# NanoPlot makes many plots, but here are the ones we care about.
# Note that newer NanoPlot makes more plots and uses different names.
NANOPLOT_PLOT_LIST = [ "HistogramReadlength",
                       "LengthvsQualityScatterPlot_dot",
                       "NumberOfReads_Over_Time",
                       "ActivePores_Over_Time" ]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Utility functions
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return parse_cell_name(EXPERIMENT, c)['Base']

def save_out_plist(yaml_files, out_file):
    """Save contents of projects_ready.txt
    """
    plist = set()
    for y in yaml_files:
        plist.add(load_yaml(y)['Project'])
    with open(out_file, "w") as ofh:
        print( *sorted(plist), file=ofh, sep='\n' )

def label_for_part(part, barcode='.'):
    """Was a dict. Now a function.
    """
    if barcode == '.':
        pls = { 'pass'     : "all passed",
                'fail'     : "all failed",
                '_default' : f"{part}" }
    else:
        pls = { 'pass'     : f"{barcode} passed",
                'fail'     : f"{barcode} failed",
                '_default' : f"{barcode} {part}" }

    return pls.get(part, pls['_default'])

def get_cell_info( experiment, cell, cell_content, counts, fin_summary,
                   sample_names = None,
                   sample_cutoff = 0.01,
                   blobs = None,
                   nanoplot = None,
                   pod5_meta = None,
                   duplex = None,
                   minknow_report = None ):
    """Compiles the content of cell_info.yaml from various bits and pieces.
       Only really for use by the 'one_cell' rule. I guess I could make this a
       separate script.
    """
    ci = parse_cell_name(experiment, cell)

    # Add pass/fail number of files. If the relevant cell_content is not available we'll not attempt
    # to recalculate this, but in general it should all be loaded from sc_data.
    for pf in ["pass", "fail"]:
        try:
            # The counts in fastq and pod5 may well not match. Record all counts.
            # The dict comprehension aggregates over all barcodes.
            cell_files = { k: [f for cfb in cell_content.values() for f in cfb[f'{k}_{pf}']]
                           for k in ["fastq", "pod5"] }
            # Add in gzipped file counts
            cell_files[f'fastq'] += [f for cfb in cell_content.values() for f in cfb.get(f'fastq.gz_{pf}',[])]
            ci['Files in '+pf] = len(cell_files['fastq'])
            ci['Files in pod5 '+pf] = len(cell_files['pod5'])

        except KeyError as e:
            # see comment above.
            logger.warning(repr(e))
            ci['Files in '+pf] = "unknown"

    # Add info from the .count files - add in '_label' and '_barcode' too but I'm not sure
    # if we will split them out by barcode in the report or just add them up.
    ci['_counts'] = []

    for pf in ["pass", "fail"]:
        for bc in cell_content:
            # The items in counts come from input.counts. Assume each YAML
            # file yields a dict
            cdata = counts[(bc, pf)]
            cdata['_part'] = pf
            cdata['_label'] = f"{label_for_part(pf).capitalize()} reads"
            cdata['_barcode'] = bc
            ci['_counts'].append(cdata)

    # The fin_summary also contains the original experiment name, which I could get from
    # rundata/pipeline/upstream but this seems more robust.
    if fin_summary.get('protocol_group_id'):
        ci['UpstreamExpt'] = fin_summary['protocol_group_id']

    # And in fact just save the entire fin_summary into the info.yaml, so that make_report.py
    # can get at it.
    ci['_final_summary'] = fin_summary

    # Do we have an estimated count of the duplex reads?
    if duplex is not None:
        total_passing = sum( v['total_reads'] for k, v in counts.items() if k[1] == 'pass' )
        # Avoid div by zero if there are no passing reads
        perc_duplex = 0
        if duplex:
            perc_duplex = (100 * 2 * duplex) / total_passing
        ci['_duplex'] = [ [ 'Duplex pairs',             duplex ],
                          [ 'from total passing reads', total_passing ],
                          [ '% of passing reads',       f"{perc_duplex:.2f}%" ] ]
    else:
        ci['_duplex'] = [ [ 'Duplex pairs',             "No duplex estimate available" ] ]

    # Link to other reports. I'm now using paths relative to the file rather than the
    # root of the run dir.
    if blobs:
        ci['_blobs'] = [ f"../../{b}" for b in blobs ]
    if nanoplot:
        ci['_nanoplot'] = f"../../{nanoplot}"
    if minknow_report:
        ci['_minknow_report'] = f"../../{minknow_report}"

    # Fold in the pod5_metadata
    if pod5_meta:
        ci.update(pod5_meta)

    # Add in the barcode-based filter. Here we can choose how to deal with internal
    # and external names, but remember that external names in particular must be
    # robustly quoted.
    ci['_filter_type'] = 'none'
    if sample_names:
        ci['_filter_yaml'] = sample_names
    if sample_names and sample_names.get('barcodes'):
        # At least one barcode is listed
        ci['_filter_type'] = 'yaml'
        ci['_filter'] = { bc['bc']:
                            (bc['int_name'] + ' ' + bc.get('ext_name', '')).strip()
                          for bc in sample_names['barcodes'] }
    else:
        # Either there is no filter or it has no barcodes. Resort to
        # comparing counts to sample_cutoff
        # TODO - check no filter is applied for non-barcoded cells
        barcodes_counts = count_up_passing( ci['_counts'],
                                            cutoff = sample_cutoff,
                                            include_unclassified = False)
        if barcodes_counts and list(barcodes_counts) != ['.']:
            ci['_filter_type'] = f'cutoff {sample_cutoff}'
            ci['_filter'] = { bc: 'Unbarcoded' if bc == '.' else bc
                              for bc in barcodes_counts }
        # if this fails for some reason, _filter_type will stay as none

    return ci

def count_up_passing(counts, cutoff=None, include_unclassified=True):
    """Takes a _counts list-of-dicts and returns a dict of
       barcode -> pass_count dict
       Optionally supply a cutoff as a percentage.
       Optionally discard 'unclassified', even if higher than cutoff.
    """
    if cutoff:
        cell_total_reads = sum([ c.get('total_reads', 0)
                                 for c in counts
                                 if c['_part'] in ['pass', 'fail'] ])
        abs_cutoff = int(cell_total_reads * (cutoff / 100))
    else:
        abs_cutoff = 0


    res = { c['_barcode']: c['total_reads']
             for c in counts
             if c['_part'] == 'pass'
             and c['total_reads'] > abs_cutoff }

    if not include_unclassified and 'unclassified' in res:
        del res['unclassified']

    return res

## End of functions ## Leave this comment in place to help the unit tests.

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The workflow
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Global wildcard patterns
wildcard_constraints:
    fullid     = "[^/]+",
    barcode    = "[^/_]+",
    pf         = "pass|fail",
    pfs        = "pass|fail|skip",
    batch      = "[0-9]{8}",
    batch_size = "[0-9]+",

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.

localrules: main, one_cell, copy_pod5, sample_names
rule main:
    output:
        plist     = f"projects_ready.txt",
        panrep    = f"all_reports/report.{CELL_COUNT}cells.pan",
        rep       = f"all_reports/report.{CELL_COUNT}cells.pan.html",
        allpanrep = f"all_reports/report.{CELL_COUNT}cells.all.pan",
        allrep    = f"all_reports/report.{CELL_COUNT}cells.all.pan.html",
        replink   = f"all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml",
        projnames = "project_realnames.yaml",
    params:
        templates = os.environ.get('TEMPLATES', '.'),
        totalcells = SC_DATA.counts['cells']
    run:
        # Un-silence sys.stderr in sub-jobs:
        logger.quiet.discard('all')

        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        # At this point I could detect if the only barcode in the whole
        # experiment is '.' then I don't need allrep but for now lets always do both.
        for panrep, rep, filt in [ (output.panrep,    output.rep,    'on'),
                                   (output.allpanrep, output.allrep, 'all') ]:

            shell(r'''
                make_report.py -o {panrep} --totalcells {params.totalcells} \
                               --blobstats {input.blobstats} --projnames {input.projnames} \
                               --filter {filt} {input.yaml}
            ''')

            shell(r'''
                {TOOLBOX} pandoc -f markdown \
                                    --template={params.templates}/template.html \
                                    --include-in-header={params.templates}/javascript.js.html \
                                    --include-in-header={params.templates}/easter.js.html \
                                    --include-in-header={params.templates}/local.css.html \
                                    --toc --toc-depth=4 \
                                    -o {rep} {panrep}
            ''')

        # Link just the main one
        shell("ln -snr {output.rep} {output.replink}")

# I've split out the pod5 batch-merging and md5summing in order to allow me to re-run everything
# else without doing this part. So to fully process a run you need to call targets 'main' AND
# 'copy_pod5'
rule copy_pod5:
    input:
        all_md5 = [ f"md5sums/{cell}/pod5_{bc}_{pf}/all_pod5.md5"
                        for cell, barcodes in SC.items()
                        for bc in barcodes
                        for pf in ['pass', 'fail'] ]
    run:
        # Un-silence sys.stderr in sub-jobs:
        logger.quiet.discard('all')

        ldcache = {}

        # Exlicitly verify that all the files are actually present (but don't run 'md5sum -c'
        # that would be silly).
        for ifile in input.all_md5:
            print(f"Checking md5 file {ifile}")
            with open(str(ifile)) as ifh:
                for md5line in ifh:
                    target_file = md5line.rstrip('\n').split(maxsplit=1)[1]
                    cell = os.path.dirname(os.path.dirname(ifile))[len("md5sums/"):]
                    # I could use os.stat() to assert the file exists, but stat is
                    # expensive on Lustre, so use listdir instead:
                    dname, fname = os.path.split(f"{cell}/{target_file}")

                    if dname not in ldcache:
                        ldcache[dname] = set(os.listdir(dname))
                    assert fname in ldcache[dname], f"{cell}/{target_file} is missing"

# Per-cell driver rule. In short:
#  All pod5 files get merged into batches of 80
#  All _fail.fastq files get concatenated and compressed
#  All of the files get an md5sum

def one_cell_inputs(wc):
    """ Provide the inputs to the rule below
    """
    cell = wc.cell
    base = cellname_to_base(cell)
    barcodes = SC[cell].keys()

    # Load the final summary to see if this is RNA or DNA. If the summary
    # YAML is already in the output this bypasses needing EXPDIR.
    fs = load_final_summary(f"{EXPDIR}/{cell}/",
                            yamlfile = f"{cell}/cell_final_summary.yaml")

    xxpand = partial(expand, cell = cell,
                             base = base,
                             bc = barcodes,
                             pf = ['pass','fail'])

    # The MinKNOW report used to be a PDF but is now HTML. We need to peek at which is there,
    # but as usual remember the input may not be available.
    minknow_report = f"{base}_report.pdf"
    if find_summary("report.pdf", EXPDIR, cell, allow_missing=True):
        logger.debug("PDF report in the input")
    elif os.path.exists(minknow_report) and os.path.getsize(minknow_report):
        logger.debug("PDF report already in the output")
    else:
        logger.debug("Defaulting to looking for HTML report")
        minknow_report = f"{base}_report.html"

    res = dict(
        fastq_gz     = xxpand("{base}_{bc}_{pf}.fastq.gz"),
        seq_summary  = xxpand("{base}_sequencing_summary.txt.gz"),
        minknow_rept = minknow_report,
        counts       = xxpand("counts/{base}_{bc}_{pf}.fastq.count"),
        blobs        = xxpand("blob/{base}_{bc}_plots.yaml"),
        nanoplot_r   = xxpand("nanoplot/{cell}/{outfile}", outfile = ["NanoStats.yaml",
                                                                      "NanoPlot-report.html" ]),
        nanoplot_p   = xxpand("nanoplot/{cell}/{p}.png", p = NANOPLOT_PLOT_LIST),
        nanoplot_t   = xxpand("nanoplot/{cell}/{p}.__thumb.png", p = NANOPLOT_PLOT_LIST),

        duplex_list  = f"duplex_scan/{cell}/pair_ids_filtered.txt",

        pod5_meta    = f"{cell}/cell_pod5_metadata.yaml",
        fin_summary  = f"{cell}/cell_final_summary.yaml",

        sample_names = f"{cell}/sample_names.yaml",

        # Previously we added:
        #minionqc     = "minionqc/{cell}/summary.yaml",
        )

    # If there are any BAM files, we also need the merged BAM files
    if any([ SC[cell][bc].get(f'bam_{pf}')
             for bc in barcodes
             for pf in ["pass", "fail"] ]):
        res['merged_bam'] = xxpand("{base}_{bc}_{pf}.bam")

    return res

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input: unpack(one_cell_inputs)
    params:
        base = lambda wc: cellname_to_base(wc.cell)
    run:
        # Cos sys.stderr gets silenced in sub-jobs:
        logger.quiet.discard('all')

        # The final summary data has been parsed into YAML
        fin_summary = load_final_summary(None, yamlfile = str(input.fin_summary))
        # The counts files have sequence counts (not file counts) per barcode and pass/fail
        cdata = dict()
        for c in input.counts:
            # Extract the bc and pf from the filename, then save into the counts dict
            bc, pf = c[len(f"counts/{params.base}_"):-len(".fastq.count")].split("_")
            cdata[(bc, pf)] = load_yaml(c)
        # The HTML report from MinKnow may be an empty file, in which case ignore it
        minknow_report = str(input.minknow_rept)
        if not os.path.getsize(minknow_report):
            minknow_report = None

        # For duplex reads we just need the count of lines in input.duplex_list
        try:
            with open(str(input.duplex_list)) as dlfh:
                duplex_count = sum(1 for __ in dlfh)
        except AttributeError:
            # No duplex today
            duplex_count = None

        ci = get_cell_info( experiment = EXPERIMENT,
                            cell = wildcards.cell,
                            cell_content = SC[wildcards.cell],
                            counts = cdata,
                            duplex = duplex_count,
                            fin_summary = fin_summary,
                            blobs = input.blobs,
                            pod5_meta = load_yaml(input.pod5_meta),
                            nanoplot = input.nanoplot_r[0],
                            minknow_report = minknow_report,
                            sample_names = load_yaml(input.sample_names) )

        dump_yaml(ci, str(output))

# Local rule that fetches the barcode->sample mappings for one cell
# The Python script will see $SAMPLE_NAMES_DIR
rule sample_names:
    output: "{cell}/sample_names.yaml"
    shell:
        "sample_names_fetch.py {wildcards.cell}"

def find_representative_pod5(wildcards):
    """Looks in SC_DATA for a representative POD5 for this cell. If there is none,
       raises an exception.
       Since Snakemake now tracks only batches of POD5 files we need to convert this
       representative file into a batch directory.
    """
    rep_pod5 = SC_DATA.representative_pod5[wildcards.cell]
    if not rep_pod5:
        raise RuntimeError("No representative POD5 in SC_DATA")

    # FIXME - 00000000 probably no longer works as len is variable
    batch_file = f"{os.path.dirname(rep_pod5)}/batch{POD5_BATCH_SIZE}_00000000.pod5"

    return batch_file

# This should get the same metadata from the batched or un-batched POD5 meaning it can be run
# even after the originals are deleted.
# The weird input definition tries to handle this. Whichever we use we only need a single file,
# so return the first.
rule pod5_metadata:
    output: "{cell}/cell_pod5_metadata.yaml"
    input:
        pod5 = find_representative_pod5
    shell:
        "get_pod5_metadata.py -v {input.pod5} > {output}"

# Only if the run directory is in place, load the rules that filter, compress and combine the
# original files.
if EXPDIR:
    include: "Snakefile.rundata"

# https://github.com/nanoporetech/duplex-tools
# FIXME - this is using a LOT of memory. Can we do better?
rule pairs_from_summary:
    output:
        out_dir    = directory("duplex_scan/{cell}"),
        pair_ids   = "duplex_scan/{cell}/pair_ids.txt",
        pair_stats = "duplex_scan/{cell}/pair_stats.txt",
    input:
        summary    = lambda wc: f"{cellname_to_base(wc.cell)}_sequencing_summary.txt.gz"
    params:
        min_qscore = "9.0"
    resources:
        mem_mb = 0,
        extra_slurm_flags = '--exclusive',
    shell:
        "duplex_tools pairs_from_summary --min_qscore {params.min_qscore} {input.summary} {output.out_dir}"

# Slightly tricky:
# - Symlink input.passfastq files here to be explicit about which to scan
# - Check for success in the log
rule filter_pairs:
    output:
        pair_ids  = "duplex_scan/{cell}/pair_ids_filtered.txt",
        scores    = "duplex_scan/{cell}/pair_ids_scored.csv",
        segments  = "duplex_scan/{cell}/read_segments.pkl",
    input:
        pair_ids  = "duplex_scan/{cell}/pair_ids.txt",
        pf_fastq  = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_{pf}.fastq.gz"
                                 for bc in SC[wc.cell]
                                 for pf in ["pass", "fail"] ],
    log: "duplex_scan/{cell}/filter_pairs.log"
    params:
        fastq_dir = "duplex_scan/{cell}/"
    resources:
        mem_mb = 0,
        extra_slurm_flags = '--exclusive',
    shell:
       r"""ln -sfrvt {params.fastq_dir} {input.pf_fastq}
           if [ -s {input.pair_ids} ] ; then
             duplex_tools filter_pairs {input.pair_ids} {params.fastq_dir} 2>&1 | tee {log}
             grep -Fq "Found 100.0% of required reads" {log}
           else
             touch {output}
             echo 'Skipping filter_pairs as {input.pair_ids} is empty' > {log}
           fi
        """

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
# Note this will produce many plots but there are only a few we care to check.
rule nanoplot:
    output:
        stats  = "nanoplot/{cell}/NanoStats.txt",
        rep    = "nanoplot/{cell}/NanoPlot-report.html",
        plots  = [ "nanoplot/{cell}/" + p + ".png" for p in NANOPLOT_PLOT_LIST ],
        thumbs = [ "nanoplot/{cell}/" + p + ".__thumb.png" for p in NANOPLOT_PLOT_LIST ],
    input:
        summary      = lambda wc: f"{cellname_to_base(wc.cell)}_sequencing_summary.txt.gz",
        passlist     = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass_fastq.list"
                                    for bc in SC[wc.cell] ],
        passlist_gz  = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass_fastq.gz.list"
                                    for bc in SC[wc.cell] ],
    params:
        thumbsize = "320x320"
    resources:
        mem_mb = 52000,
        n_cpus = 9,
    threads: 9
    run:
        summary_ap = os.path.realpath(str(input.summary))
        shell(r'cd nanoplot/{wildcards.cell} ; rm -f *.png *.html *.log')

        # We need a special case when there are no passing reads as NanoPlot will choke.
        # Tempting to look at SC[wildcards.cell]['fastq_pass'] but this list could be empty
        # if EXPDIR is not set, so inspect input.passlist which should be preserved. Since
        # adding barcode support, there may be multiple passlist files.
        # Since adding .fastq.gz support there are these to consider too.
        if all( os.path.getsize(p) == 0 for p in input.passlist ):
            if all( os.path.getsize(p) == 0 for p in input.passlist_gz ):
                shell("touch {output}")
                return

        shell(r'cd nanoplot/{wildcards.cell} ; NanoPlot -t {threads} --summary {summary_ap}')

        # Finally, make thumbnails for everything
        for apng in glob(f"nanoplot/{wildcards.cell}/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# This file normally generated by the driver but it's possible it may be missing.
# In which case we may just create an empty file in order to proceed and avoid the
# missing input error.
# I don't want to have rules in the Snakefile querying the LIMS.
# Obviously this means that use of '-F' will clobber the contents.
rule project_realnames:
    output: "project_realnames.yaml"
    shell:
        "touch {output}"

# Finally add the blob plotting rules.
include: "Snakefile.blob"

