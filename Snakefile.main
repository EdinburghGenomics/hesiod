#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_ref)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pformat

from snakemake.utils import format
from subprocess import CalledProcessError

from hesiod import glob, parse_cell_name, groupby, dump_yaml, load_yaml

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')
if not os.path.exists(RUNDIR):
    logger.warning(format("WARNING: {RUNDIR} is not a directory. Suppressing rules that reference RUNDIR."))
    RUNDIR = None

def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return parse_cell_name(RUN, c)['Base']

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        by driver.sh in config['cellsready'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    if 'cells' in config:
        # Believe what we are told
        cells = config['cells'].split('\t')
    elif RUNDIR:
        # Look for valid cells.
        cells = sorted(set( '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/fastq_????/")) ))
    else:
        # Look for cells here in the output (presumably the run already processed and the raw dir was removed)
        cells = [ '/'.join(fs.strip('/').split('/')[-3:-1]) for fs in glob(format("*/*/cell_info.yaml")) ]

    if 'cellsready' in config:
        # This should include the cells to be processed now AND those already processed.
        cellsready = config['cellsready'].split('\t')

        for c in cellsready:
            assert c in cells, format("Invalid cell (no fastq_pass or not listed in config[cells]): {c}")
    elif RUNDIR:
        # Look for cells with a final_summary.txt (as made by MinKNOW). Note the new name in MinKNOW 3.6+
        cellsready = [ c for c in cells if (
                       glob(format("{RUNDIR}/{c}/final_summary.txt")) or
                       glob(format("{RUNDIR}/{c}/final_summary_*_*.txt")) ) ]
    else:
        # If there is no RUNDIR this makes most sense by default.
        cellsready = cells

    if not cellsready:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cellsready }

    if RUNDIR:
        for c, d in res.items():
            for pf in "pass fail".split():
                for filetype in "fastq fast5".split():
                    category = format("{filetype}_{pf}")
                    d[category] = [ f[len(RUNDIR) + 1:]
                                    for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

        # Sanity-check that the file counts match. But we seem to be seeing this in several runs,
        # so maybe it's not an error? Oh - it's because I'm seeing Urmi's already-combined reads, so this
        # is definitely a bad thing.
        for c, d in res.items():
            for pf in "pass fail".split():
                if len(d[format("fastq_{pf}")]) != len(d[format("fast5_{pf}")]):
                    raise RuntimeError( format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf}):\n") +
                                        sc_counts(res) )

    # Return the dict of stuff to process, and other counts we've calculated
    return res, dict( cells = len(cells),
                      cellsready = len(cellsready),
                      cellsaborted = 0 )

def sc_counts(sc_dict, width=140):
    """ Make a printable summary of SC
    """
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in sc_dict.items() }

    return pformat(sc_counts, width=width)


# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the project number.
# Some outputs are aggregated by {lib}, some by {project} and other are per-cell.
# FIXME - nothing is aggregated per-lib just yet?!
SC, COUNTS = scan_cells()
RUN = os.path.basename(os.path.realpath('.')).split('.')[0]
CELLS_PER_LIB     = groupby( SC, lambda c: parse_cell_name(RUN, c)['Library'], True)
CELLS_PER_PROJECT = groupby( SC, lambda c: parse_cell_name(RUN, c)['Project'], True)

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            plist.add(load_yaml(y)['Project'])
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

if 'logger' in globals():
    logger.info( "RUN {}, SC =\n{}".format(RUN, sc_counts(SC)) )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.

localrules: main, one_cell, pack_fast5
rule main:
    output:
        plist     = "projects_ready.txt",
        panrep    = "all_reports/report.{}cells.pan".format(len(SC)),
        rep       = "all_reports/report.{}cells.pan.html".format(len(SC)),
        replink   = "all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml",
        realnames = "project_realnames.yaml",
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        # shell("make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} --minionqc {input.minionqc} {input.yaml}")
        shell(r'''make_report.py -o {output.panrep} --totalcells {COUNTS[cells]} \
                                    --blobstats {input.blobstats} --realnames {input.realnames} \
                                    {input.yaml}
               ''')

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/easter.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {output.rep} {output.panrep}
        ''')

        shell("ln -snr {output.rep} {output.replink}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this. So to fully process a run you need to call main AND pack_fast5
rule pack_fast5:
    input:
        fast5_gz   = lambda wc: [ "{}.gz".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],
        fast5_md5  = lambda wc: [ "md5sums/{}.gz.md5".format(f) for cell in SC
                                                    for pf in 'fast5_pass fast5_fail'.split()
                                                    for f in SC[cell][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM)
#  but as requested by Urmi we also retain the combined 'pass' files.
#  All of the files get an md5sum

# See also BLOB_PARTS in Snakefile.blob
FASTQ_PARTS = ["pass", "nolambda", "fail"]
PART_LABELS = { 'pass'     : "all passed",
                'fail'     : "all failed",
                'nolambda' : "lambda-filtered passed",
                'lambda'   : "lambda-mapping passed" }


rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fastq_gz     = lambda wc: expand( "{base}_{pf}.fastq.gz",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = FASTQ_PARTS ),
        seq_summary  = lambda wc: expand( "{base}_sequencing_summary.txt.gz",
                                            base = [cellname_to_base(wc.cell)] ),
        counts       = lambda wc: expand( "counts/{base}_{pf}.fastq.count",
                                            base = [cellname_to_base(wc.cell)],
                                            pf = FASTQ_PARTS ),
        lambda_bam   = lambda wc: expand( "{base}_lambda.bam",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_stats = lambda wc: expand( "lambdaqc/{base}_lambda.bam.stats",
                                            base = [cellname_to_base(wc.cell)] ),
        lambda_qmap  = lambda wc: expand( "lambdaqc/{base}_lambda.bam.qmap",
                                            base = [cellname_to_base(wc.cell)] ),
        blobs        = "blob/{cell}/plots.yaml",
        nanoplot     = [ "nanoplot/{cell}/NanoStats.yaml", "nanoplot/{cell}/NanoPlot-report.html" ],
        fast5_meta   = "{cell}/cell_fast5_metadata.yaml",
        #minionqc     = "minionqc/{cell}/summary.yaml",
    run:
        ci = parse_cell_name(RUN, wildcards.cell)

        # Add pass/fail number of files. If the RUNDIR is not available we'll not attempt
        # to recalculate this. I could still count fast5 files but a better approach would be
        # to serialize SC[wildcards.cell] and just keep hold of this for info.
        cell_files = SC[wildcards.cell]
        for pf in "pass fail".split():
            try:
                # Maybe we can't guarantee the counts in fastq and fast5 match?!
                # Nope - they should always match.
                if len(cell_files[format('fastq_{pf}')]) == len(cell_files[format('fast5_{pf}')]):
                    ci['Files in '+pf] = len(cell_files[format('fastq_{pf}')])
                else:
                    ci['Files in '+pf] = "{} ({} in fast5_{pf})".format( len(cell_files[format('fastq_{pf}')]),
                                                                         len(cell_files[format('fast5_{pf}')]),
                                                                         pf = pf )
            except KeyError:
                # FIXME - see comment above.
                ci['Files in '+pf] = "unknown"

        # Add info from the .count files
        ci['_counts'] = []
        for label, cf in zip(FASTQ_PARTS, input.counts):
            cdata = load_yaml(cf)
            cdata['_label'] = "{} reads".format(PART_LABELS.get(label, label).capitalize())
            ci['_counts'].append(cdata)

        # Link to other reports. I'm now using paths relative to the file rather than the
        # root of the run dir.
        ci['_blobs'] = "../../{}".format(input.blobs)
        ci['_nanoplot'] = "../../{}".format(input.nanoplot[0])
        #ci['_minionqc'] = str(input.minionqc)

        # Fold in the fast5_metadata
        mdata = load_yaml(str(input.fast5_meta))
        ci.update(mdata)

        dump_yaml(ci, str(output))

# Note this currently gets the metadata from the gzipped fast5 meaning it can be run
# even after the originals are deleted, except that in this case SC will not be populated.
# The weird input definition tries to handle this case by looking for the output file
# directly. In either case we only need a single file, so return the first.
def i_fast5_metadata(wc):
    if SC[wc.cell]['fast5_pass']:
        # Depend on the first file provided by SC[wc.cell]
        return [ "{}.gz".format(f) for f in SC[wc.cell]['fast5_pass'] ][:1]
    elif SC[wc.cell]['fast5_fail']:
        # Somethimes everything fails
        return [ "{}.gz".format(f) for f in SC[wc.cell]['fast5_fail'] ][:1]
    else:
        # Look for an existing output file.
        return glob("{cell}/fast5_????/*.fast5.gz".format(**vars(wc)))[:1]

rule fast5_metadata:
    output: "{cell}/cell_fast5_metadata.yaml"
    input:
        fast5 = i_fast5_metadata
    shell:
        "get_fast5_metadata.py -v {input} > {output}"

# Only if the run directory is in place, load the rules that filter, compress and combine the
# original files.
if RUNDIR:
    include: "Snakefile.rundir"

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output:
        stats = "nanoplot/{cell}/NanoStats.txt",
        rep   = "nanoplot/{cell}/NanoPlot-report.html"
    input:
        summary  = lambda wc: "{base}_sequencing_summary.txt.gz".format(base=cellname_to_base(wc.cell)),
        passlist = lambda wc: "{base}_pass_fastq.list".format(base=cellname_to_base(wc.cell))
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(str(input.summary))
        shell(r'cd "$(dirname {output.stats})" ; rm -f *.png *.html *.log')

        # We need a special case when there are no passing reads as NanoPlot will choke.
        # Tempting to look at SC[wildcards.cell]['fastq_pass'] but this will not be available
        # if RUNDIR is not set, so inspect input.passlist which should be preserved.
        if os.path.getsize(str(input.passlist)) == 0:
           shell("touch {output}")
           return

        shell(r'cd "$(dirname {output.stats})" ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob("nanoplot/*/*.png") + glob("nanoplot/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{base}_sequencing_summary.txt.gz", base=[cellname_to_base(c) for c in SC]),
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

# Stats calculated on the lambda.bam files using samtools stat and qualimap
rule samstats:
    output: "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.stats"
    input:  "{cell}/{all}_lambda.bam"
    threads: 4
    shell:
       r'''{TOOLBOX} samtools stats -d -@ {threads} {input} > {output}'''

rule qualimap:
    output:
        txt  = "lambdaqc/{cell}/{all,[^/]+}_lambda.bam.qmap",
        html = "lambdaqc/{cell}/{all,[^/]+}_lambda_qmap/qualimapReport.html"
    input:  "{cell}/{all}_lambda.bam"
    params:
        windows = 5000
    threads: 8
    run:
        fail_reason = None
        if os.path.getsize(str(input)) == 0:
            fail_reason = 'Zero-byte BAM File'
        else:
            # Inspect the file to see if it has some reads
            try:
                shell("set +o pipefail ; {TOOLBOX} samtools view {input} | grep -q .")
            except CalledProcessError:
                fail_reason = 'No reads in BAM File'

        if fail_reason:
            # Nothing to QC. Make empty outputs to keep Snakemake happy.
            shell("echo {fail_reason:q} > {output.txt}")
            shell("echo {fail_reason:q} > {output.html}")
            return

        outdir = os.path.dirname(output.html)

        shell("{TOOLBOX} qualimap bamqc -bam {input} -nt {threads} -nw {params.windows} -outdir {outdir}")
        shell("mv {outdir}/genome_results.txt {output.txt}")

# This file normally generated by the driver but it's possible it may be missing.
# In which case we may just create an empty file in order to proceed and avoid the
# missing input error.
# I don't want to have rules in the Snakefile querying the LIMS.
# Obviously this means that use of '-F' will clobber the contents.
rule project_realnames:
    output: "project_realnames.yaml"
    shell:
        "touch {output}"

# Finally add the blob plotting rules.
include: "Snakefile.blob"

