#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake

from collections import OrderedDict
from pprint import pformat
import shutil

import yaml, yamlloader
from snakemake.utils import format


def glob():
    """Regular glob() is useful but we want consistent sort order."""
    from glob import glob
    return lambda p: sorted( (f.rstrip('/') for f in glob(os.path.expanduser(p))) )
glob = glob()

logger = snakemake.logging.logger

TOOLBOX = format('env PATH="{os.environ[TOOLBOX]}:$PATH"')
PIGZ    = 'pigz -nT -9 -b512'

for p in os.environ['PATH'].split(':'):
    if os.path.abspath(p) == os.path.dirname(os.path.abspath(workflow.snakefile)):
        break
else:
    # The directory containing this file should be in the PATH
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If not supplied, just assume the usual symlink will work...
RUNDIR = config.get('rundata', 'rundata')

def scan_cells():
    """ Work out all the cells to process. Should be simple since the list is passed
        in config['cells'] but I do want to be able to process all by default.
        Then get a list of all the files per cell.
        TODO - can we avoid doing this if --allowed-rules is specified?
    """
    if 'cells' in config:
        cells = config['cells'].split('\t')

        for c in cells:
            assert glob(format("{RUNDIR}/{c}/fastq_pass/")), format("Invalid cell (no fastq_pass): {c}")
    else:
        # Look for cells with a final_summary.txt
        cells = [ '/'.join(fs.split('/')[-3:-1]) for fs in glob(format("{RUNDIR}/*/*/final_summary.txt")) ]

    if not cells:
        # Not a fatal error if some specific rule is being invoked.
        logger.error("List of cells to process is empty")

    res = { c: dict() for c in cells }

    for c, d in res.items():
        for pf in "pass fail".split():
            for filetype in "fastq fast5".split():
                category = format("{filetype}_{pf}")
                d[category] = [ f[len(RUNDIR) + 1:]
                                for f in glob(format("{RUNDIR}/{c}/{category}/*.{filetype}")) ]

    # Sanity-check that the file counts match!
    for c, d in res.items():
        for pf in "pass fail".split():
            assert len(d[format("fastq_{pf}")]) == len(d[format("fast5_{pf}")]), \
                    format("Mismatch between count of FASTQ and FAST5 files for {c} ({pf})")

    return res

# Cells are in the form {lib}/{cell}. Some outputs are aggregated by {lib} and other are per-cell.
SC = scan_cells()
LIBS = sorted(set([ c.split('/')[0] for c in SC ]))
RUN = os.path.basename(os.path.realpath(RUNDIR)).split('.')[0]

def parse_cell_name(cell):
    """Things we get from parsing wildcards.cell"""
    res = OrderedDict()
    res['Run'] = RUN
    res['Cell'] = cell

    # Now shred the filename.
    mo = re.match(r'([0-9A-Z-]+)/(\d{8})_(\d+)_([0-9A-Z-]+)_([0-9A-Z]+)_([0-9a-f]{8})$', cell)
    if mo:
        for n, x in enumerate("Library Date Number Slot CellID Checksum".split()):
            res[x] = mo.group(n+1)
    else:
        # Not good, but we'll try
        res['Library'] = cell.split('/')[0]
        res['CellID'] = cell.split('_')[-2] if '_' in cell else 'UNKNOWN'

    return res

def save_out_plist(yaml_files, out_file):
        plist = set()
        for y in yaml_files:
            with open(y) as yfh:
                lib = yaml.safe_load(yfh)['Library']
                # FIXME - not sure this is the right place for getting the project ID,
                # or the right thing to do when the regex fails.
                mo =  re.match(r"([0-9]{5})[A-Z]{2}", lib)
                if mo:
                    plist.add(mo.group(1))
                else:
                    plist.add(lib)
        with open(out_file, "w") as ofh:
            print( *sorted(plist), file=ofh, sep='\n' )

def save_out_report(base_path, cells):
    """Assemble the reports - for now just a directory of reports from NanoPlot but this
       will change soon.
    """
    # FIXME - this should be part of the rule deps not a glob+copy. Also the file name choice
    # is arbitrary.
    # TODO - and of course this code should be in a separate Python script
    # TODO - and it should be made from a template
    for cell in cells:
        ci = parse_cell_name(cell)
        src_rep = format("nanoplot/{cell}/NanoPlot-report.html")
        dest_rep = format("NanoPlot_{ci[Library]}_{ci[CellID]}-report.html")
        shutil.copy(src_rep, os.path.join(base_path, dest_rep))

    # Add a report.html
    from datetime import datetime

    pan = "{}/report.{}cells.pan".format(base_path, len(cells))
    with open(pan, 'w') as repfh:
        P = lambda *a: print(*a, file=repfh)

        P("% Promethion run {}".format(RUN))
        P("% Hesiod version {}".format("devel"))
        P("% {}".format(datetime.now().strftime("%A, %d %b %Y %H:%M")))

        P()
        P("# About this run (experiment? project?)")
        P()
        P('<dl class="dl-horizontal">')
        P('<dt>RunID</dt> <dd>{}</dd>'.format(RUN))
        P('<dt>Instrument</dt> <dd>Promethion 1</dd>')
        P('<dt>CellCount</dt> <dd>{}</dd>'.format(len(SC)))
        P('<dt>LibraryCount</dt> <dd>{}</dd>'.format(len(LIBS)))
        P('<dt>StartTime</dt> <dd>something</dd>')
        P('<dt>LastCellTime</dt> <dd>something</dd>')
        P('</dl>')

        for cell in cells:
            ci = parse_cell_name(cell)
            P()
            P("## Cell {}".format(cell))
            P()
            P(":::::: {.bs-callout}")
            P('<dl class="dl-horizontal">')
            for k, v in ci.items():
                P('<dt>{}</dt> <dd>{}</dd>'.format(k,v))
            P('</dl>')
            P()
            P(format("[NanoPlot Report](NanoPlot_{ci[Library]}_{ci[CellID]}-report.html)"))
            P("::::::")

        P()
        P("*~~~*")

    return pan

if 'logger' in globals():
    # Make a dict that just shows the counts
    sc_counts = { c : { category: "<{} files>".format(len(filelist))
                        for category, filelist in d.items() }
                  for c, d in SC.items() }

    logger.info( "RUN {}, SC =\n{}".format(RUN, pformat(sc_counts, width=140)) )

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - added aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.
localrules: main, one_cell
rule main:
    input:
        yaml     = expand("{cell}/cell_info.yaml", cell=SC),
        minionqc = "minionqc/combinedQC/summary.yaml"
    output:
        plist    = "projects_ready.txt",
        rep      = "all_reports/report.html"
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))

        panrep = save_out_report(os.path.dirname(output.rep), SC)
        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {panrep}.html {panrep}
        ''')

        shell("ln -snr {panrep}.html {output.rep}")

# Per-cell driver rule.
rule one_cell:
    output: "{cell}/cell_info.yaml"
    input:
        fast5_gz   = lambda wc: expand("{f}.gz",             f=(SC[wc.cell]['fast5_pass'] + SC[wc.cell]['fast5_fail'])),
        fast5_md5  = lambda wc: expand("md5sums/{f}.gz.md5", f=(SC[wc.cell]['fast5_pass'] + SC[wc.cell]['fast5_fail'])),
        fastq_gz   = lambda wc: expand( "{cell}/{ci[Run]}_{ci[Library]}_{ci[CellID]}_{pf}.fastq.gz",
                                            cell = [wc.cell],
                                            ci = [parse_cell_name(wc.cell)],
                                            pf = "pass fail".split() ),
        fastq_md5  = lambda wc: expand( "md5sums/{cell}/{ci[Run]}_{ci[Library]}_{ci[CellID]}_{pf}.fastq.gz.md5",
                                            cell = [wc.cell],
                                            ci = [parse_cell_name(wc.cell)],
                                            pf = "pass fail".split() ),
        blobs      = "blob/fastq_pass.plots.yaml",
        nanoplot   = "nanoplot/{cell}/NanoStats.yaml",
        minionqc   = "minionqc/{cell}/summary.yaml",
    run:
        cell_files = SC[wildcards.cell]
        ci = parse_cell_name(wildcards.cell)

        # Add pass/fail counts
        for pf in "pass fail".split():
            # We already confirmed the fastq and fast5 counts match
            ci['Files in '+pf] = len(cell_files[format('fastq_{pf}')])

        # yamlloader is basically the same as my yaml_ordered hack. It will go away with Py3.7.
        with open(str(output), "w") as ofh:
            yaml.dump(ci, ofh, Dumper=yamlloader.ordereddict.CSafeDumper)

# gzipper that uses pigz and compresses from RUNDIR to CWD
# md5summer that keeps the file path out of the .md5 file
# I made these a single rule to reduce the number of submitted jobs, with
# the assuption we'll always be doing both, and "group:" in Snakemake is currently
# broken with DRMAA :-(
rule gzip_md5sum_fast5:
    output:
        gz  = "{foo}.fast5.gz",
        md5 = "md5sums/{foo}.fast5.gz.md5"
    input: RUNDIR + "/{foo}.fast5"
    threads: 2
    shell:
       r"""{PIGZ} -v -p{threads} -c {input} > {output.gz}
           ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
        """

# This one concatenates and zips and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz   = "{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz",
        md5  = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}.fastq.gz.md5",
        fofn = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        # Here we run the risk of blowing out the command line lenght limit, so avoid
        # that.
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"xargs -d '\n' cat <{output.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# Alternatively I could tar the files. Note that to cat the files in a single stream I can do:
# $ tar -xaOf all_pass_fastq.tar.gz
# But most users won't get that :-(
# TODO - remove this. We won't be doing it.
rule concat_tar_md5sum_fastq:
    priority: 100
    output:
        tar  = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.tar.gz",
        md5  = "md5sums/{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.tar.gz.md5",
        fofn = "{cell}/{all,[^/]+}_{pf,pass|fail}_fastq.list"
    input:
        fastq = lambda wc: [format("{RUNDIR}/{f}") for f in SC[wc.cell]['fastq_'+wc.pf]]
    threads: 6
    run:
        with open(output.fofn, "w") as fh:
            for fname in input.fastq: print(fname, file=fh)

        shell(r"tar -cT {output.fofn} --xform='s,.*/\(.*/\),\1,' | {PIGZ} -p{threads} -c  > {output.tar}")

        shell(r"( cd `dirname {output.tar}` && md5sum `basename {output.tar}` ) > {output.md5}")

def find_sequencing_summary(wc):
    """For a given cell, the sequencing summary may be in the top level dir (new style) or in a
       sequencing_summary subdirectory (old style). Either way there should be only one.
    """
    found = glob(format("{RUNDIR}/{wc.cell}/*_sequencing_summary.txt")) + \
            glob(format("{RUNDIR}/{wc.cell}/sequencing_summary/*_sequencing_summary.txt"))

    assert len(found) == 1, ( "There should be exactly one sequencing_summary.txt per cell"
                              " - found {}.".format(len(found)) )

    return found

rule gzip_sequencing_summary:
    output:
        link = "{cell}/sequencing_summary.txt.gz"
    input:
        summary = find_sequencing_summary
    threads: 2
    run:
        # First copy the file, preserving the name. Then link.
        gzfile = "{}/{}.gz".format(wildcards.cell, os.path.basename(str(input.summary)))

        shell(r"{PIGZ} -p{threads} -c <{input.summary} >{gzfile}")
        shell(r"cd {wildcards.cell} && ln -sn `basename {gzfile}` `basename {output.link}`")

rule blobs:
    output: "blob/fastq_pass.plots.yaml"
    shell:
        "touch {output}"

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
rule nanoplot:
    output: "nanoplot/{cell}/NanoStats.txt"
    input:  "{cell}/sequencing_summary.txt.gz"
    shell:
       r"""ap="$(readlink -f {input})"
           cd "$(dirname {output})"
           rm -f *.png *.html *.log
           NanoPlot --summary "$ap"
        """

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.
localrules: minionqc # For now.
rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{cell}/sequencing_summary.txt.gz", cell=SC)
    threads: 2
    run:
        # Remove old
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for d in output.per_cell:
            shell('rm -rf "$(dirname {d})"')

        # Gather files in a directory
        shell("mkdir -p minionqc/_links")
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads}')

