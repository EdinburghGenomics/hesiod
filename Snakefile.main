#!/bin/bash
# vim: ft=python

# Contents >>>
#   + Embedded BASH script to bootstrap the workflow
#   + Initialisation and configuration
#   + Helper functions
#   + The rules specific to this workflow
#   + More generic rules

# This workflow expects to be run in the output directory and see input files at ./rundata
# You must run `scan_cells.py > sc_data.yaml` before starting the workflow.

"""true" ### Begin shell script part
set -u

source "`dirname $0`"/shell_helper_functions.sh

# Sanity-check that the virtual env is active
if ! which NanoPlot >/dev/null ; then
    echo "***"
    echo "*** NanoPlot not in PATH. You probably need to activate the Virtual Env"
    echo "***"
    echo
fi

# The TOOLBOX setting gets passed down to jobs that run on SLURM. The PATH setting
# does not, as SLURM resets that env var.
export TOOLBOX="$(find_toolbox)"
export TEMPLATES="$(find_templates)"
export REFS="$(find_refs)"
export PATH="${PATH}:$(dirname "$0")"

snakerun_drmaa "$0" "$@"

"exit""" ### End of shell script part
#!/usr/bin/env snakemake
from pprint import pprint, pformat

from functools import partial
from subprocess import CalledProcessError

from hesiod import ( glob, parse_cell_name, load_final_summary,
                     find_sequencing_summary, find_summary,
                     fast5_out, dump_yaml, load_yaml, empty_sc_data )

# This is just here to help testing - Snakemake sets it automatically for workflows
logger = snakemake.logging.logger

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Global vars and setup
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# $TOOLBOX must be set to something
TOOLBOX = f'env PATH="{os.environ["TOOLBOX"]}:$PATH"'
PIGZ    = 'pigz -nT -9 -b512'

# Convert name of CWD to EXPERIMENT
EXPERIMENT = os.path.basename(os.path.realpath('.')).split('.')[0]

logger.info( f"EXPERIMENT = {EXPERIMENT}" )
logger.info( f"config = {pformat(config)}" )

# Reference for when we come to remove lambda reads from the FASTQ.
# There is a different calibration reference for RNA. See doc/rna_support.txt
def get_calibration_refs():
    # $REFS points to directory with the calibration reference files
    if os.environ["REFS"] in [ '', '.', './' ]:
        refs = ''
    else:
        refs = os.path.abspath(os.environ["REFS"]) + '/'
    return { k: f'{refs}{v}.fasta' for k, v in
                                   { "lambda":  "lambda_3.6kb",
                                     "eno"   :  "YHR174W",
                                   }.items() }
calibration_refs = get_calibration_refs()

# Check the PATH includes the directory containing this Snakefile
location_of_snakefile = os.path.dirname(os.path.abspath(workflow.snakefile))
if not any([ os.path.abspath(p) == location_of_snakefile
             for p in os.environ['PATH'].split(':') ]):
    os.environ['PATH'] += ':' + os.path.dirname(workflow.snakefile)

# If no explicit input dir supplied, just assume the usual symlink will work...
EXPDIR = config.get('rundata', 'rundata')
if not os.path.exists(EXPDIR):
    logger.warning(f"WARNING: {EXPDIR} is not a directory. Suppressing rules that reference EXPDIR.")
    EXPDIR = None

# Load the data structure generated by scan_cells.py
# Cells are in the form {lib}/{cell}. The first 5 chars of the lib name are taken to be the project number.
# Some outputs are aggregated {project} and other are per-cell.
try:
    SC_DATA = load_yaml(config.get("sc_data", "sc_data.yaml"), as_tuple="sc_data")
except:
    SC_DATA = empty_sc_data()

SC = SC_DATA.scanned_cells
CELL_COUNT = SC_DATA.counts['cellsready']

logger.info( f"SC =\n{SC_DATA.printable_counts}" )

# NanoPlot makes many plots, but here are the ones we care about.
# Note that newer NanoPlot makes more plots and uses different names.
NANOPLOT_PLOT_LIST = [ "HistogramReadlength",
                       "LengthvsQualityScatterPlot_dot",
                       "NumberOfReads_Over_Time",
                       "ActivePores_Over_Time" ]

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Utility functions
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

def cellname_to_base(c):
    """Given a cell name, what base name do we choose for the output files?
    """
    return parse_cell_name(EXPERIMENT, c)['Base']

def save_out_plist(yaml_files, out_file):
    """Save contents of projects_ready.txt
    """
    plist = set()
    for y in yaml_files:
        plist.add(load_yaml(y)['Project'])
    with open(out_file, "w") as ofh:
        print( *sorted(plist), file=ofh, sep='\n' )

# See also BLOB_PARTS in Snakefile.blob
def get_fastq_parts(is_rna=False):
    """This replaces the constant FASTQ_PARTS
    """
    if is_rna:
        return ["pass", "noeno", "fail"]
    else:
        return ["pass", "nolambda", "fail"]

def label_for_part(part, barcode='.'):
    """Was a dict. Now a function.
    """
    if barcode == '.':
        pls = { 'pass'     : "all passed",
                'fail'     : "all failed",
                'nolambda' : "passed and lambda-filtered",
                'noeno'    : "passed and control-filtered",
                'lambda'   : "passed and lambda-mapping",
                'eno'      : "passed and control-mapping",
                '_default' : f"{part}" }
    else:
        pls = { 'pass'     : f"{barcode} passed",
                'fail'     : f"{barcode} failed",
                'nolambda' : f"{barcode} lambda-filtered",
                'noeno'    : f"{barcode} control-filtered",
                'lambda'   : f"{barcode} lambda-mapping",
                'eno'      : f"{barcode} control-mapping",
                '_default' : f"{barcode} {part}" }

    return pls.get(part, pls['_default'])

def get_cell_info( experiment, cell, cell_content, counts, fin_summary,
                   blobs = None,
                   nanoplot = None,
                   fast5_meta = None,
                   duplex = None,
                   minknow_report = None ):
    """Compiles the content of cell_info.yaml from various bits and pieces.
       Only really for use by the 'one_cell' rule. I guess I could make this a
       separate script.
    """
    ci = parse_cell_name(experiment, cell)

    # Add pass/fail number of files. If the relevant cell_content is not available we'll not attempt
    # to recalculate this, but in general it should all be loaded from sc_data.
    for pf in ["pass", "fail"]:
        try:
            # The counts in fastq and fast5 may well not match. Record all counts.
            # The dict comprehension aggregates over all barcodes.
            cell_files = { k: [f for cfb in cell_content.values() for f in cfb[f'{k}_{pf}']]
                           for k in ["fastq", "fast5"] }
            # Add in gzipped file counts
            cell_files[f'fastq'] += [f for cfb in cell_content.values() for f in cfb.get(f'fastq.gz_{pf}',[])]
            ci['Files in '+pf] = len(cell_files['fastq'])
            ci['Files in fast5 '+pf] = len(cell_files['fast5'])

        except KeyError as e:
            # FIXME - see comment above.
            logger.warning(repr(e))
            ci['Files in '+pf] = "unknown"

    # Add info from the .count files - add in '_label' and '_barcode' too but I'm not sure
    # if we will split them out by barcode in the report or just add them up.
    ci['_counts'] = []

    for pf in get_fastq_parts(is_rna=fin_summary['is_rna']):
        for bc in cell_content:
            # The items in counts come from input.counts. Assume each YAML
            # file yields a dict
            cdata = counts[(bc, pf)]
            cdata['_part'] = pf
            cdata['_label'] = f"{label_for_part(pf).capitalize()} reads"
            cdata['_barcode'] = bc
            ci['_counts'].append(cdata)

    # The fin_summary also contains the original experiment name, which I could get from
    # rundata/pipeline/upstream but this seems more robust.
    if fin_summary.get('protocol_group_id'):
        ci['UpstreamExpt'] = fin_summary['protocol_group_id']

    # And in fact just save the entire fin_summary into the info.yaml, so that make_report.py
    # can get at it.
    ci['_final_summary'] = fin_summary

    # Do we have an estimated count of the duplex reads?
    if duplex is not None:
        # total_passing = sum(c['total_reads'] for c in ci['_counts'] if c['_part'] == 'pass')
        total_passing = sum( v['total_reads'] for k, v in counts.items() if k[1] == 'pass' )
        # Avoid div by zero if there are no passing reads
        perc_duplex = 0
        if duplex:
            perc_duplex = (100 * 2 * duplex) / total_passing
        ci['_duplex'] = [ [ 'Duplex pairs',             duplex ],
                          [ 'from total passing reads', total_passing ],
                          [ '% of passing reads',       f"{perc_duplex:.2f}%" ] ]

    # Link to other reports. I'm now using paths relative to the file rather than the
    # root of the run dir.
    if blobs:
        ci['_blobs'] = [ f"../../{b}" for b in blobs ]
    if nanoplot:
        ci['_nanoplot'] = f"../../{nanoplot}"
    if minknow_report:
        ci['_minknow_report'] = f"../../{minknow_report}"

    # Fold in the fast5_metadata
    if fast5_meta:
        ci.update(fast5_meta)

    return ci

## End of functions ## Leave this comment in place to help the unit tests.

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# The workflow
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Global wildcard patterns
wildcard_constraints:
    fullid  = "[^/]+",
    barcode = "[^/_]+",
    pf      = "pass|fail",
    pfs     = "pass|fail|skip",
    calref  = "|".join(calibration_refs),

# Main target is one yaml file (of metadata) per cell. A little bit like statfrombam.yml in the
# project QC pipelines.
# TODO - add aggregated stats per lib
# NB - I think we need to run this with -f to ensure all the .yaml files are always created
# and the report is always refreshed.

localrules: main, one_cell, pack_fast5
rule main:
    output:
        plist     = f"projects_ready.txt",
        panrep    = f"all_reports/report.{CELL_COUNT}cells.pan",
        rep       = f"all_reports/report.{CELL_COUNT}cells.pan.html",
        replink   = f"all_reports/report.html",
    input:
        yaml      = expand("{cell}/cell_info.yaml", cell=SC),
        blobstats = "blob/blobstats_by_project.yaml",
        # minionqc = "minionqc/combinedQC/summary.yaml",
        realnames = "project_realnames.yaml",
    params:
        templates = os.environ.get('TEMPLATES', '.')
    run:
        # After generating all YAML, all projects are ready.
        save_out_plist(input.yaml, str(output.plist))
        counts = SC_DATA.counts

        # shell("make_report.py -o {output.panrep} --totalcells {counts[cells]} --minionqc {input.minionqc} {input.yaml}")
        shell(r'''
            make_report.py -o {output.panrep} --totalcells {counts[cells]} \
                           --blobstats {input.blobstats} --realnames {input.realnames} \
                           {input.yaml}
        ''')

        shell(r'''
            {TOOLBOX} pandoc -f markdown \
                                --template={params.templates}/template.html \
                                --include-in-header={params.templates}/javascript.js.html \
                                --include-in-header={params.templates}/easter.js.html \
                                --include-in-header={params.templates}/local.css.html \
                                --toc --toc-depth=4 \
                                -o {output.rep} {output.panrep}
        ''')

        shell("ln -snr {output.rep} {output.replink}")

# I've split out the fast5 packing in order to allow me to re-run everything else without
# doing this. So to fully process a run you need to call main AND pack_fast5
# Note that as of v0.12, FAST5 file compression is no longer attempted
rule pack_fast5:
    input:
        fast5_link = lambda wc: [ fast5_out(f)
                                        for cell, barcodes in SC.items()
                                        for bc in barcodes
                                        for pf in ['fast5_pass', 'fast5_fail']
                                        for f in SC[cell][bc][pf] ],
        fast5_md5  = lambda wc: [ f"md5sums/{fast5_out(f)}.md5"
                                        for cell, barcodes in SC.items()
                                        for bc in barcodes
                                        for pf in ['fast5_pass', 'fast5_fail']
                                        for f in SC[cell][bc][pf] ],

# Per-cell driver rule. In short:
#  All fast5 files get individually compressed
#  All _fail.fastq files get concatenated and compressed
#  All _pass.fastq files get split into nolambda (concat and compress) and lambda (merged BAM),
#   or for RNA it's noeno, and a merged BAM of eno reads.
#  but as requested by Urmi we also retain the combined 'pass' files.
#  All of the files get an md5sum

def one_cell_inputs(wc):
    """ Provide the inputs to the rule below
    """
    cell = wc.cell
    base = cellname_to_base(cell)
    barcodes = SC[cell].keys()

    # Load the final summary to see if this is RNA or DNA. If the summary
    # YAML is already in the output this bypasses needing EXPDIR.
    fs = load_final_summary(f"{EXPDIR}/{cell}/",
                            yamlfile = f"{cell}/cell_final_summary.yaml")
    fastq_parts = get_fastq_parts(is_rna=fs['is_rna'])
    calref = "eno" if fs['is_rna'] else "lambda"

    xxpand = partial(expand, cell=cell, base=base, bc=barcodes, pf=fastq_parts, calref=calref)

    # The MinKNOW report used to be a PDF but is now HTML. We need to peek at which is there,
    # but as usual remember the input may not be available.
    minknow_report = f"{base}_report.pdf"
    if find_summary("report.pdf", EXPDIR, cell, allow_missing=True):
        logger.debug("PDF report in the input")
    elif os.path.exists(minknow_report) and os.path.getsize(minknow_report):
        logger.debug("PDF report already in the output")
    else:
        logger.debug("Defaulting to looking for HTML report")
        minknow_report = f"{base}_report.html"

    return dict(
        fastq_gz     = xxpand("{base}_{bc}_{pf}.fastq.gz"),
        seq_summary  = xxpand("{base}_sequencing_summary.txt.gz"),
        minknow_rept = minknow_report,
        counts       = xxpand("counts/{base}_{bc}_{pf}.fastq.count"),
        calref_bam   = xxpand("{base}_{bc}_{calref}.bam"),
        calref_stats = xxpand("{calref}qc/{base}_{bc}_{calref}.bam.stats"),
        calref_qmap  = xxpand("{calref}qc/{base}_{bc}_{calref}.bam.qmap"),
        blobs        = xxpand("blob/{base}_{bc}_plots.yaml"),
        nanoplot_r   = xxpand("nanoplot/{cell}/{outfile}", outfile = ["NanoStats.yaml",
                                                                      "NanoPlot-report.html" ]),
        nanoplot_p   = xxpand("nanoplot/{cell}/{p}.png", p = NANOPLOT_PLOT_LIST),
        nanoplot_t   = xxpand("nanoplot/{cell}/{p}.__thumb.png", p = NANOPLOT_PLOT_LIST),

        duplex_list  = f"duplex_scan/{cell}/pair_ids_filtered.txt",

        fast5_meta   = f"{cell}/cell_fast5_metadata.yaml",
        fin_summary  = f"{cell}/cell_final_summary.yaml",

        # Previously we added:
        #minionqc     = "minionqc/{cell}/summary.yaml",
        )

rule one_cell:
    output: "{cell}/cell_info.yaml"
    input: unpack(one_cell_inputs)
    params:
        base = lambda wc: cellname_to_base(wc.cell)
    run:
        # The final summary data has been parsed into YAML
        fin_summary = load_final_summary(None, yamlfile = str(input.fin_summary))
        # Ditto the FAST5 metadata
        mdata = load_yaml(str(input.fast5_meta))
        # The counts files have sequence counts (not file counts) per barcode and pass/fail
        cdata = dict()
        for c in input.counts:
            # Extract the bc and pf from the filename, then save into the counts dict
            bc, pf = c[len(f"counts/{params.base}_"):-len(".fastq.count")].split("_")
            cdata[(bc, pf)] = load_yaml(c)
        # The HTML report from MinKnow may be an empty file, in which case ignore it
        minknow_report = str(input.minknow_rept)
        if not os.path.getsize(minknow_report):
            minknow_report = None
        # For duplex reads we just need the count of lines in input.duplex_list
        with open(str(input.duplex_list)) as dlfh:
            duplex_count = sum(1 for _ in dlfh)

        ci = get_cell_info( experiment = EXPERIMENT,
                            cell = wildcards.cell,
                            cell_content = SC[wildcards.cell],
                            counts = cdata,
                            duplex = duplex_count,
                            fin_summary = fin_summary,
                            blobs = input.blobs,
                            fast5_meta = mdata,
                            nanoplot = input.nanoplot_r[0],
                            minknow_report = minknow_report )

        dump_yaml(ci, str(output))

def find_representative_fast5(wildcards):
    """Looks in SC_DATA for a representative FAST5 for this cell. If there is none,
       raises an exception.
    """
    rep_fast5 = SC_DATA.representative_fast5[wildcards.cell]
    if not rep_fast5:
        raise RuntimeError("No representative FAST5 in SC_DATA")
    return rep_fast5

# Note this currently gets the metadata from the gzipped fast5 meaning it can be run
# even after the originals are deleted, except that in this case SC will not be populated.
# The weird input definition tries to handle this case by looking for the output file
# directly. In either case we only need a single file, so return the first.
rule fast5_metadata:
    output: "{cell}/cell_fast5_metadata.yaml"
    input:
        fast5 = find_representative_fast5
    shell:
        "get_fast5_metadata.py -v {input} > {output}"

# Only if the run directory is in place, load the rules that filter, compress and combine the
# original files.
if EXPDIR:
    include: "Snakefile.rundata"

# https://github.com/nanoporetech/duplex-tools
rule pairs_from_summary:
    output:
        out_dir    = directory("duplex_scan/{cell}"),
        pair_ids   = "duplex_scan/{cell}/pair_ids.txt",
        pair_stats = "duplex_scan/{cell}/pair_stats.txt",
    input:
        summary    = lambda wc: f"{cellname_to_base(wc.cell)}_sequencing_summary.txt.gz"
    shell:
        "duplex_tools pairs_from_summary {input.summary} {output.out_dir}"

# Slightly tricky:
# - Symlink input.passfastq files here to be explicit about which to scan
# - Check for success in the log
rule filter_pairs:
    output:
        pair_ids  = "duplex_scan/{cell}/pair_ids_filtered.txt",
        scores    = "duplex_scan/{cell}/pair_ids_scored.csv",
        segments  = "duplex_scan/{cell}/read_segments.pkl",
    input:
        pair_ids  = "duplex_scan/{cell}/pair_ids.txt",
        passfastq = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass.fastq.gz"
                                 for bc in SC[wc.cell] ],
    log: "duplex_scan/{cell}/filter_pairs.log"
    params:
        fastq_dir = "duplex_scan/{cell}/"
    shell:
       r"""ln -sfrvt {params.fastq_dir} {input.passfastq}
           duplex_tools filter_pairs {input.pair_ids} {params.fastq_dir} 2>&1 | tee {log}
           grep -Fq "Found 100.0% of required reads" {log}
        """

# Make a nanoplot report form the sequencing summary and also do a quick conversion on
# the stats, which come out as unstructured text.
# Note this will produce many plots but there are only a few we care to check.
rule nanoplot:
    output:
        stats  = "nanoplot/{cell}/NanoStats.txt",
        rep    = "nanoplot/{cell}/NanoPlot-report.html",
        plots  = [ "nanoplot/{cell}/" + p + ".png" for p in NANOPLOT_PLOT_LIST ],
        thumbs = [ "nanoplot/{cell}/" + p + ".__thumb.png" for p in NANOPLOT_PLOT_LIST ],
    input:
        summary      = lambda wc: f"{cellname_to_base(wc.cell)}_sequencing_summary.txt.gz",
        passlist     = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass_fastq.list"
                                    for bc in SC[wc.cell] ],
        passlist_gz  = lambda wc: [ f"{cellname_to_base(wc.cell)}_{bc}_pass_fastq.gz.list"
                                    for bc in SC[wc.cell] ],
    params:
        thumbsize = "320x320"
    run:
        ap = os.path.realpath(str(input.summary))
        shell(r'cd nanoplot/{wildcards.cell} ; rm -f *.png *.html *.log')

        # We need a special case when there are no passing reads as NanoPlot will choke.
        # Tempting to look at SC[wildcards.cell]['fastq_pass'] but this list could be empty
        # if EXPDIR is not set, so inspect input.passlist which should be preserved. Since
        # adding barcode support, there may be multiple passlist files.
        # Since adding .fastq.gz support there are these to consider too.
        if all( os.path.getsize(p) == 0 for p in input.passlist ):
            if all( os.path.getsize(p) == 0 for p in input.passlist_gz ):
                shell("touch {output}")
                return

        shell(r'cd nanoplot/{wildcards.cell} ; NanoPlot --summary {ap}')

        # Finally, make thumbnails for everything
        for apng in glob(f"nanoplot/{wildcards.cell}/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

localrules: nanostats
rule nanostats:
    output: "nanoplot/{cell}/NanoStats.yaml"
    input:  "nanoplot/{cell}/NanoStats.txt"
    shell: r"parse_nanostats.py <{input} >{output}"

# MinIONQC (which, despite, the name, is also good for Promethion) makes all the outputs at once,
# so here's a funny rule.

# This was breaking when submitted to the cluster. It turns out I had it using /tmp and that was full,
# but the error was being masked. Annoying.

rule minionqc:
    output:
        combined = "minionqc/combinedQC/summary.yaml",
        per_cell = expand("minionqc/{cell}/summary.yaml", cell=SC),
    input:
        expand("{base}_sequencing_summary.txt.gz", base=[cellname_to_base(c) for c in SC]),
    params:
        thumbsize = "320x320"
    threads: 6
    run:
        # Remove old files
        shell("rm -rf minionqc/combinedQC minionqc/_links")
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('rm -rf minionqc/{libdir}/{celldir}')
            shell('rm -rf minionqc/{celldir}')

        # Gather files in a directory
        for f in input:
            shell('mkdir -p minionqc/_links/"$(dirname {f})"')
            shell('ln -snr {f} minionqc/_links/{f}')

        # Run it
        shell('{TOOLBOX} minionqc -o minionqc -i minionqc/_links -p {threads} >&2')

        # Due to the way minionqc decides on output dir names, we have to make
        # a correction like so:
        for pc in output.per_cell:
            libdir, celldir = str(pc).split('/')[-3:-1]
            shell('mv -vt minionqc/{libdir} minionqc/{celldir}')

        # Finally, make thumbnails for everything
        for apng in glob("minionqc/*/*.png") + glob("minionqc/*/*/*.png"):
            if not apng.endswith('.__thumb.png'):
                athumb = apng[:-4] + '.__thumb.png'
                shell("{TOOLBOX} convert {apng} -resize {params.thumbsize} {athumb}")

# Stats calculated on the {calref}.bam files using samtools stat and qualimap
rule samstats:
    output: "{calref}qc/{cell}/{all,[^/]+}_{calref}.bam.stats"
    input:  "{cell}/{all}_{calref}.bam"
    threads: 4
    run:
        if os.path.getsize(str(input)) == 0:
            shell("echo 'Zero-byte BAM File' > {output}")
        else:
            shell("{TOOLBOX} samtools stats -d -@ {threads} {input} > {output}")

rule qualimap:
    output:
        txt  = "{calref}qc/{cell}/{all,[^/]+}_{calref}.bam.qmap",
        html = "{calref}qc/{cell}/{all,[^/]+}_{calref}_qmap/qualimapReport.html"
    input:  "{cell}/{all}_{calref}.bam"
    params:
        windows = 5000
    threads: 8
    run:
        fail_reason = None
        if os.path.getsize(str(input)) == 0:
            fail_reason = 'Zero-byte BAM File'
        else:
            # Inspect the file to see if it has some reads
            try:
                shell("set +o pipefail ; {TOOLBOX} samtools view {input} | grep -q .")
            except CalledProcessError:
                fail_reason = 'No reads in BAM File'

        if fail_reason:
            # Nothing to QC. Make empty outputs to keep Snakemake happy.
            shell("echo {fail_reason:q} > {output.txt}")
            shell("echo {fail_reason:q} > {output.html}")
            return

        outdir = os.path.dirname(output.html)

        shell("{TOOLBOX} qualimap bamqc -bam {input} -nt {threads} -nw {params.windows} -outdir {outdir}")
        shell("mv {outdir}/genome_results.txt {output.txt}")

# This file normally generated by the driver but it's possible it may be missing.
# In which case we may just create an empty file in order to proceed and avoid the
# missing input error.
# I don't want to have rules in the Snakefile querying the LIMS.
# Obviously this means that use of '-F' will clobber the contents.
rule project_realnames:
    output: "project_realnames.yaml"
    shell:
        "touch {output}"

# Finally add the blob plotting rules.
include: "Snakefile.blob"

