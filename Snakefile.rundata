# vim: ft=python

from hesiod import ( load_final_summary, find_sequencing_summary, find_summary,
                     dump_yaml, load_yaml )

# Rules to filter, compress and combine the original files.
# These rules are designed to be included in Snakefile.main and will not run standalone.

# Note that these rules are bypassed if the rundata directory is missing, allowing us to repeat QC
# without errors relating to missing files.

# Compress the file discovered by the above function and rename it, matching the base of
# the FASTQ and BAM files. Note the original name is preserved in the GZIP header and can
# be revealed by 'gunzip -Nlv {output.gz}'.
rule gzip_sequencing_summary:
    output:
        gz  = "{cell}/{fullid}_sequencing_summary.txt.gz",
        md5 = "md5sums/{cell}/{fullid}_sequencing_summary.txt.gz.md5"
    input:  lambda wc: [find_sequencing_summary(EXPDIR, wc.cell)]
    threads: 2
    shell:
        r"""{PIGZ} -v -p{threads} -Nc {input} >{output.gz}
            ( cd "$(dirname {output.gz:q})" && md5sum -- "$(basename {output.gz:q})" ) > {output.md5:q}
         """

localrules: convert_final_summary, copy_report
rule convert_final_summary:
    output:
        yaml = "{cell}/cell_final_summary.yaml",
    input: lambda wc: find_summary('final_summary.txt', EXPDIR, wc.cell)
    run:
        dump_yaml(load_final_summary(str(input)), str(output.yaml))

# This needs to work for the HTML or the PDF reports
rule copy_report:
    output:
        pdf = "{cell}/{fullid}_report.{extn,pdf|html}"
    input: lambda wc: find_summary(f"report.{wc.extn}", EXPDIR, wc.cell, allow_missing=True) or []
    run:
        if input:
            shell("cp -T {input} {output.pdf}")
        else:
            shell("touch {output}")

# Link (or copy) and checksum a batch of fast5 files:
# * see doc/job_grouping.txt for why I'm doing manual batching not using Snakemake groups
# * Some validation is provided by h5stat. Not perfect but it should at least spot truncations.
# * md5sum is invoked such that keeps the full path out of the .md5 file, as usual
# * barcode=. should work with this even though it's a bit sus.
# * making the input 'ancient' is an attempt to speed up the DAG build but I'm not sure it helps
# * due to hard-link logic the 'chgrp' bit will change the ownership on the input file, which
#   would not normally happen even with g+s in effect. We need to avoid this for test runs because
#   these often use real data for input and I would end up with production files owned by tbooth2,
#   but the same-owner check is good for this, I think.
def i_copy_md5sum_fast5_batch(wildcards):
    all_in_dir = SC[wildcards.cell][wildcards.barcode][f"fast5_{wildcards.pfs}"]

    batch_size = int(wildcards.bsize)
    batch_num = int(wildcards.batch)

    return [ ancient(f"{EXPDIR}/{f}") for f in
             all_in_dir[batch_num*batch_size:(batch_num+1)*batch_size] ]

rule copy_md5sum_fast5_batch:
    output:
        dir = directory("{cell}/fast5_{barcode}_{pfs}/batch{bsize}_{batch}"),
        md5 = "md5sums/{cell}/fast5_{barcode}_{pfs}/batch{bsize}_{batch}.md5"
    input:
        i_copy_md5sum_fast5_batch
    params:
        batchdir = "batch{bsize}_{batch}"
    shell:
       r"""true > {output.md5}.tmp
           for ifile in {input} ; do
               ofile="{output.dir}/$(basename "$ifile")"
               if [ "$(stat -c %u/%D "$ifile")" = "$(id -u)/$(stat -c %D $(dirname "$ofile"))"  ] ; then
                  ln -T "$ifile" "$ofile"
                  chgrp --reference="$(dirname "$ofile")" "$ofile"
               else
                  cp -T "$ifile" "$ofile"
               fi
               {TOOLBOX} h5stat "$ofile" > /dev/null
               ( cd "$(dirname {output.dir})" && md5sum -- "{params.batchdir}/$(basename "$ofile")" ) >> {output.md5}.tmp
           done
           mv {output.md5}.tmp {output.md5}
        """

# These two concatenate and zip and sum the fastq. The fastq are smaller so one final file is OK.
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
# TODO - should possibly convert this to zip the individual chunks, as per the no{calref} files?

# I've broken out the rule that makes _fastq.list files so I can request the list without
# making the actual merged file. The basic FASTQ merge step doesn't do any batching.
localrules: list_fastq_or_bam
rule list_fastq_or_bam:
    priority: 100
    output:
        fofn  = r"{cell}/{fullid}_{barcode}_{pf}_{extn,fastq|fastq\.gz|bam}.list"
    input:
        infiles = lambda wc: [f"{EXPDIR}/{f}" for f in SC[wc.cell][wc.barcode][f"{wc.extn}_{wc.pf}"]]
    run:
        # Just write all the input filenames to the output file.
        with open(output.fofn, "w") as fh:
            try:
                for fname in input.infiles: print(fname, file=fh)
            except AttributeError:
                # So there are no files. Make an empty list then.
                pass

rule concat_gzip_md5sum_bam:
    priority: 100
    output:
        bam    = "{cell}/{fullid}_{barcode}_{pf}.bam",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_{pf}.bam.md5",
    input:
        fofn    = "{cell}/{fullid}_{barcode}_{pf}_bam.list",
    threads: 8
    resources:
        mem_mb = 24000,
        n_cpus = 8,
    run:
        # Rely on xargs to deal with way more files than could fit on the command line
        # and zip them all into one.
        shell(r"""{TOOLBOX} samtools merge \
                    -@ {threads} -n -l9 \
                    -b {input.fofn} \
                    {output.bam}""")

        # md5sum
        shell(r"( cd $(dirname {output.bam}) && md5sum $(basename {output.bam}) ) > {output.md5}")


rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_{pf}.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_{pf}.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_{pf}.fastq.count",
    input:
        fofn    = "{cell}/{fullid}_{barcode}_{pf}_fastq.list",
        fofn_gz = "{cell}/{fullid}_{barcode}_{pf}_fastq.gz.list",
    threads: 8
    resources:
        mem_mb = 24000,
        n_cpus = 8,
    run:
        # Rely on xargs to deal with way more files than could fit on the command line
        # and zip them all into one.
        shell(r"xargs -rd '\n' cat <{input.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        # Add already gzipped files. Normally there will only be one or the other, but we do support both
        # Note that we're just concatenating the zipped files, not decompressing and recompressing,
        # but we do want to verify integrity, and this version should do so in a cache-friendly way.
        shell(r"""xargs -n 1 -rd '\n' sh -c '{PIGZ} -t "$@" >&2 || exit 255 ; cat "$@"' - <{input.fofn_gz} >> {output.gz}""")

        # Base counter
        shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        shell(r"( cd $(dirname {output.gz}) && md5sum $(basename {output.gz}) ) > {output.md5}")

# We've decided to do the calref mapping up front. Here's two rules to combine the filtered
# fastq.gz files (by simply concatenating them rather than unzip/rezip) and the BAM files
# aligned to the calibration reference.
# Note this is specific to the pass reads. We don't partition the fail reads (should we??)
def i_concat_md5sum_nocalref(wildcards):
    """Available files within a single directory are divided into
       batches of, say, 100
       This is very similar to i_copy_fast5() and we'll use the same FAST5_BATCH_SIZE
    """
    pf = "pass"
    # Dict mapping each directory to the number of files it contains.
    # .fastq and .fastq.gz are concatenated together.
    dirs = { f"{cell}/fastq_{bc}_{pf}" : sum([ len(cats[f'{ftype}_{pf}'])
                                               for ftype in ['fastq', 'fastq.gz'] ])
                    for cell, barcodes in SC.items()
                    for bc, cats in barcodes.items()
                    for pf in [pf] }

    # But we only want the batches for one cell+barcode per job
    batch_dir = f"{wildcards.cell}/fastq_{wildcards.barcode}_{pf}"

    # List of .list files we thus need to generate.
    res = []
    total = dirs[batch_dir]
    batches_for_dir = ((total - 1) // FAST5_BATCH_SIZE) + 1
    for b in range(batches_for_dir):
        listfile = f"fastq_no{wildcards.calref}_tmp/{batch_dir}/batch{FAST5_BATCH_SIZE}_{b:08d}.list"
        res.append(listfile)
    return res

# This rule combines all the FASTQ, with calref removed, listed in all the
# listfiles for a given barcode on a given cell. It also numerically combines
# the counts and md5sums everything.
rule concat_md5sum_nocalref_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_no{calref}.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_no{calref}.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_no{calref}.fastq.count",
    input:
        listfiles = i_concat_md5sum_nocalref
    run:
        try:
            # each file in input.listfiles is created by one map_calref job and
            # lists out the basenames of the output files that we need to collect up.
            idir = os.path.dirname(input.listfiles[0])
            shell(r"""sed -e 's/^/{idir}\//' \
                          -e 's/$/.no{wildcards.calref}.fastq.gz/' {input.listfiles} | \
                         xargs -rd '\n' cat \
                      > {output.gz}""")
            shell(r"""sed -e 's/^/{idir}\//' \
                          -e 's/$/.no{wildcards.calref}.fastq.count/' {input.listfiles} | \
                         xargs -rd '\n' cat | \
                         fq_base_combiner.awk -r fn=`basename {output.gz} .gz` \
                      > {output.counts}""")
        except AttributeError:
            # No input then
            shell(r"touch {output.gz}") # or should this be "true | gzip -c > {output.gz}"?
            shell(r"true | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        # Checksum over the whole file
        shell(r"( cd $(dirname {output.gz}) && md5sum $(basename {output.gz}) ) > {output.md5}")

rule concat_md5sum_calref_bam:
    output:
        bam  = "{cell}/{fullid}_{barcode}_{calref}.bam",
        md5  = "md5sums/{cell}/{fullid}_{barcode}_{calref}.bam.md5",
    input:
        listfiles = i_concat_md5sum_nocalref
    threads: 4
    resources:
        mem_mb = 12000,
        n_cpus = 4,
    run:
        try:
            idir = os.path.dirname(input.listfiles[0])
            shell(r"""{TOOLBOX} samtools merge -@ {threads} -l9 -b \
                         <(sed -e 's/^/{idir}\//' \
                               -e 's/$/.{wildcards.calref}.bam/' {input.listfiles}) \
                         {output.bam}""")
        except AttributeError:
            # We're merging nothing. But Snakemake must have her output file.
            shell("touch {output.bam}")

        shell(r"( cd $(dirname {output.bam}) && md5sum $(basename {output.bam}) ) > {output.md5}")

# This rule produces the stuff in fastq_pass_tmp, by scanning the original .fastq files
# for calibration reference, and will process batches of
# fastq_pass files prior to combining the results in the two rules above. It uses nested
# implicit FIFOs for the actual mapping. I call it "bashception".
# Note this type of construct is vulnerable to the kill-before-flush bug but it seems with
# this arrangement we are OK. See:
#  https://www.pixelbeat.org/docs/coreutils-gotchas.html
# In normal operation the fastq_pass_tmp dir will be removed by the "onsuccess" handler below.
def i_map_calref_batch(wildcards):
    """As with like i_copy_md5sum_fast5_batch() this returns the list of input files
       (both .fastq and .fastq.gz) which are the members of a given batch of files,
       which map_calref_batch will process in a loop.
    """
    all_in_dir = [ f for ftype in ["fastq", "fastq.gz"]
                   for fs in SC[wildcards.cell][wildcards.barcode][f"{ftype}_{wildcards.pf}"]
                   for f in fs ]

    batch_size = int(wildcards.bsize)
    batch_num = int(wildcards.batch)

    return [ ancient(f"{EXPDIR}/{f}") for f in
             all_in_dir[batch_num*batch_size:(batch_num+1)*batch_size] ]


rule map_calref_batch:
    output:
        listfile = "fastq_no{calref}_tmp/{cell}/fastq_{barcode}_{pf}/batch{bsize}_{batch}.list"
    input:
        fq     = i_map_calref_batch,
        ref    = lambda wc: ancient(calibration_refs[wc.calref]),
    params:
        rg     = r'@RG\tID:1\tSM:{cell_pf_bc}\tPL:promethion',
        mmopts = '-t 1 -a --MD --sam-hit-only -y --secondary=no -x map-ont '
    wildcard_constraints:
        f      = "[^/]+",
        gz     = "gz|",
    threads: 6
    shell:
       r"""true > {output.listfile}.tmp
           for ifile in {input.fq} ; do
               obase="$(basename "${{ifile%%.*}}")"
               odir="$(dirname {output.listfile})"
               ofastq="$odir/$obase.no{wildcards.calref}.fastq.gz"
               ocount="$odir/$obase.no{wildcards.calref}.fastq.count"
               obam="$odir/$obase.{wildcards.calref}.bam"
               {TOOLBOX} minimap2 {params.mmopts} -R {params.rg:q} {input.ref} "$ifile" | tee >( \
                    lambda_splitter.awk \
                        -v paf=/dev/stdin \
                        -v nolambda=>({PIGZ} -c -p{threads} > "$ofastq") \
                        <({PIGZ} -fdc "$ifile") ) | \
                    {TOOLBOX} samtools sort - -@ {threads} -o "$obam"
               {PIGZ} -dc -p{threads} "$ofastq" | fq_base_counter.awk -r fn="$obase.no{wildcards.calref}.fastq" > "$ocount"
               echo "$obase" >> {output.listfile}.tmp
           done
           mv {output.listfile}.tmp {output.listfile}
        """

# Remove the fastq_pass_tmp directory which should normally be empty of files if
# everything worked.
# FIXME - this now has to be 'rm -r'
onsuccess:
    shell("find fastq_pass_tmp/ -type d -delete || true")

