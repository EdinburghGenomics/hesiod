# vim: ft=python

from hesiod import ( load_final_summary, find_sequencing_summary, find_summary,
                     dump_yaml, load_yaml )

# Rules to filter, compress and combine the original files.
# These rules are designed to be included in Snakefile.main and will not run standalone.

# Note that these rules are bypassed if the rundata directory is missing, allowing us to repeat QC
# without errors relating to missing files.

# Compress the file discovered by the above function and rename it, matching the base of
# the FASTQ and BAM files. Note the original name is preserved in the GZIP header and can
# be revealed by 'gunzip -Nlv {output.gz}'.
rule gzip_sequencing_summary:
    output:
        gz  = "{cell}/{fullid}_sequencing_summary.txt.gz",
        md5 = "md5sums/{cell}/{fullid}_sequencing_summary.txt.gz.md5"
    input:  lambda wc: [find_sequencing_summary(EXPDIR, wc.cell)]
    threads: 2
    shell:
        r"""{PIGZ} -v -p{threads} -Nc {input} >{output.gz}
            ( cd "$(dirname {output.gz:q})" && md5sum -- "$(basename {output.gz:q})" ) > {output.md5:q}
         """

localrules: convert_final_summary, copy_report
rule convert_final_summary:
    output:
        yaml = "{cell}/cell_final_summary.yaml",
    input: lambda wc: find_summary('final_summary.txt', EXPDIR, wc.cell)
    run:
        dump_yaml(load_final_summary(str(input)), str(output.yaml))

# This needs to work for the HTML or the PDF reports
rule copy_report:
    output:
        pdf = "{cell}/{fullid}_report.{extn}"
    input: lambda wc: find_summary(f"report.{wc.extn}", EXPDIR, wc.cell, allow_missing=True) or []
    wildcard_constraints:
        extn = r"pdf|html",
    run:
        if input:
            shell("cp -T {input} {output.pdf}")
        else:
            shell("touch {output}")

# Link (or copy) and checksum a batch of fast5 files:
# * see doc/job_grouping.txt for why I'm doing manual batching not using Snakemake groups
# * Some validation is provided by h5stat. Not perfect but it should at least spot truncations.
# * md5sum is invoked such that keeps the full path out of the .md5 file, as usual
# * barcode=. should work with this even though it's a bit sus.
# * making the input 'ancient' is an attempt to speed up the DAG build but I'm not sure it helps
# * due to hard-link logic the 'chgrp' bit will change the ownership on the input file, which
#   would not normally happen even with g+s in effect. We need to avoid this for test runs because
#   these often use real data for input and I would end up with production files owned by tbooth2,
#   but the same-owner check is good for this, I think.
def i_copy_md5sum_fast5_batch(wildcards):
    all_in_dir = SC[wildcards.cell][wildcards.barcode][f"fast5_{wildcards.pfs}"]

    batch_size = int(wildcards.bsize)
    batch_num = int(wildcards.batch)

    return [ ancient(f"{EXPDIR}/{f}") for f in
             all_in_dir[batch_num*batch_size:(batch_num+1)*batch_size] ]

rule copy_md5sum_fast5_batch:
    output:
        dir = directory("{cell}/fast5_{barcode}_{pfs}/batch{bsize}_{batch}"),
        md5 = "md5sums/{cell}/fast5_{barcode}_{pfs}/batch{bsize}_{batch}.md5"
    input:
        i_copy_md5sum_fast5_batch
    params:
        batchdir = "fast5_{barcode}_{pfs}/batch{bsize}_{batch}"
    shell:
       r"""true > {output.md5}.tmp
           mkdir -v {output.dir}
           for ifile in {input} ; do
               ofile="{output.dir}/$(basename "$ifile")"
               if [ "$(stat -c %u/%D "$ifile")" = "$(id -u)/$(stat -c %D $(dirname "$ofile"))"  ] ; then
                  ln -T "$ifile" "$ofile"
                  chgrp --reference="$(dirname "$ofile")" "$ofile"
               else
                  cp -T "$ifile" "$ofile"
               fi
               {TOOLBOX} h5stat "$ofile" > /dev/null
               ( cd {wildcards.cell} && md5sum -- "{params.batchdir}/$(basename "$ofile")" ) >> {output.md5}.tmp
           done
           mv {output.md5}.tmp {output.md5}
        """

localrules: merge_fast5_md5_batches

# The copy_fast5 rule in Snakefile.main drives execution of the rule below which runs per barcode
# (each time for pass+fail) and drives the rule above which actually copies/sums the batches of files.

def i_merge_fast5_md5_batches(wildcards):
    """Available files within a single directory are divided into
       batches of, say, 100
    """
    fast5_dir = f"{wildcards.cell}/fast5_{wildcards.bc}_{wildcards.pf}"

    # How many file to be batched up for this barcode? And thus how many batches?
    total_files = len(SC[wildcards.cell][wildcards.bc][f'fast5_{wildcards.pf}'])
    batches_needed = ((total_files - 1) // FAST5_BATCH_SIZE) + 1

    # List of md5 files we thus need to generate.
    res = []
    for b in range(batches_needed):
        res.append(f"md5sums/{fast5_dir}/batch{FAST5_BATCH_SIZE}_{b:08d}.md5")
    return res

rule merge_fast5_md5_batches:
    output: "md5sums/{cell}/fast5_{bc}_{pf}/all_fast5.md5"
    input:  i_merge_fast5_md5_batches
    run:
        # Cos sys.stderr gets silenced in sub-jobs:
        logger.quiet.discard('all')

        # Re-calculate the number of fast5 files for this barcode, as we can't easily extract
        # it from the input function. We'll use this for a sanity check at the end.
        total_files = len(SC[wildcards.cell][wildcards.bc][f'fast5_{wildcards.pf}'])

        # Compile the per-batch md5 files into one per barcode.
        lines_written = 0
        with open(str(output), 'x') as ofh:
            out_dir = os.path.dirname(str(output))
            print(f"Writing fast5 md5 for {out_dir}")
            # Could do this with shell("cat ...")
            for c in input:
                with open(str(c)) as ifh:
                    for md5line in ifh:
                        ofh.write(md5line)
                        lines_written += 1
        # We should now be able to check that the count of md5 lines matches
        # the expected number of lines in SC, which is useful as we no longer
        # have Snakemake tracking every individual file.
        assert lines_written == total_files, "lines_written == total_files"

# These two concatenate and zip and sum the fastq. The fastq are smaller so one final file is OK.
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care.

# I've broken out the rule that makes _fastq.list files so I can request the list without
# making the actual merged file. The basic FASTQ merge step doesn't do any batching.
localrules: list_fastq_or_bam
rule list_fastq_or_bam:
    priority: 100
    output:
        fofn  = r"{cell}/{fullid}_{barcode}_{pf}_{extn}.list"
    input:
        infiles = lambda wc: [ ancient(f"{EXPDIR}/{f}")
                               for f in SC[wc.cell][wc.barcode][f"{wc.extn}_{wc.pf}"] ]
    wildcard_constraints:
        extn = r"fastq|fastq\.gz|bam",
    run:
        # Just write all the input filenames to the output file.
        with open(output.fofn, "w") as fh:
            try:
                if not input.infiles:
                    # Some Snakemake versions still add an empty list
                    raise AttributeError("input.infiles is empty")
                for fname in input.infiles: print(fname, file=fh)
            except AttributeError:
                # So there are no files. Make an empty list then.
                pass

rule concat_gzip_md5sum_bam:
    priority: 100
    output:
        bam    = "{cell}/{fullid}_{barcode}_{pf}.bam",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_{pf}.bam.md5",
    input:
        fofn    = "{cell}/{fullid}_{barcode}_{pf}_bam.list",
    threads: 4
    resources:
        mem_mb = 12000,
        n_cpus = 4,
    run:
        # Samtools 'cat' is a *lot* faster than merging the BAM files.
        # Also samtools can accept a file of filenames to merge so we don't need xargs
        if os.stat(str(input.fofn)).st_size:
            shell(r"{TOOLBOX} samtools cat -@ {threads} -b {input.fofn} -o {output.bam}")
        else:
            # We're merging nothing. But Snakemake must have her output file.
            shell("touch {output.bam}")

        # md5sum
        shell(r"( cd $(dirname {output.bam}) && md5sum $(basename {output.bam}) ) > {output.md5}")


rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_{pf}.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_{pf}.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_{pf}.fastq.count",
    input:
        fofn    = "{cell}/{fullid}_{barcode}_{pf}_fastq.list",
        fofn_gz = "{cell}/{fullid}_{barcode}_{pf}_fastq.gz.list",
    threads: 8
    resources:
        mem_mb = 24000,
        n_cpus = 8,
    run:
        # Rely on xargs to deal with way more files than could fit on the command line
        # and zip them all into one.
        shell(r"xargs -rd '\n' cat <{input.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        # Add already gzipped files. Normally there will only be one or the other, but we do support both
        # Note that we're just concatenating the zipped files, not decompressing and recompressing,
        # but we do want to verify integrity, and this version should do so in a cache-friendly way.
        shell(r"""xargs -n 1 -rd '\n' sh -c '{PIGZ} -t "$@" >&2 || exit 255 ; cat "$@"' - <{input.fofn_gz} >> {output.gz}""")

        # Base counter
        shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        shell(r"( cd $(dirname {output.gz}) && md5sum $(basename {output.gz}) ) > {output.md5}")

