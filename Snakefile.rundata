# vim: ft=python

from hesiod import ( load_final_summary, find_sequencing_summary, find_summary,
                     dump_yaml, load_yaml )

# Rules to filter, compress and combine the original files.
# These rules are designed to be included in Snakefile.main and will not run standalone.

# Note that these rules are bypassed if the rundata directory is missing, allowing us to repeat QC
# without errors relating to missing files.

# Compress the file discovered by the above function and rename it, matching the base of
# the FASTQ and BAM files. Note the original name is preserved in the GZIP header and can
# be revealed by 'gunzip -Nlv {output.gz}'.
rule gzip_sequencing_summary:
    output:
        gz  = "{cell}/{fullid}_sequencing_summary.txt.gz",
        md5 = "md5sums/{cell}/{fullid}_sequencing_summary.txt.gz.md5"
    input:  lambda wc: [find_sequencing_summary(EXPDIR, wc.cell)]
    threads: 2
    shell:
        r"""{PIGZ} -v -p{threads} -Nc {input} >{output.gz}
            ( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}
         """

localrules: convert_final_summary, copy_report
rule convert_final_summary:
    output:
        yaml = "{cell}/cell_final_summary.yaml",
    input: lambda wc: find_summary('final_summary.txt', EXPDIR, wc.cell)
    run:
        dump_yaml(load_final_summary(str(input)), str(output.yaml))

rule copy_report:
    output:
        pdf = "{cell}/{fullid}_report.pdf"
    input: lambda wc: find_summary('report.pdf', EXPDIR, wc.cell, allow_missing=True) or []
    run:
        if input:
            shell("cp -T {input} {output.pdf}")
        else:
            shell("touch {output}")

# Link (or copy) and checksum the fast5 file
# md5summer that keeps the file path out of the .md5 file
# Note that the fast5_out() utility function determines outputs for this rule.
# Note2 that barcode=. should work with this even though it's a bit sus.
rule copy_md5sum_fast5:
    output:
        f5  = "{cell}/fast5_{barcode}_{pfs}/{f5fn}.fast5",
        md5 = "md5sums/{cell}/fast5_{barcode}_{pfs}/{f5fn}.fast5.md5"
    input:
        f5 = EXPDIR + "/{cell}/fast5_{pfs}/{barcode}/{f5fn}.fast5"
    threads: 2
    shell:
       r"""if [ "$(stat -c %u/%D {input.f5})" = "$(id -u)/$(stat -c %D $(dirname {output.f5}))"  ] ; then
              ln -T {input.f5} {output.f5}
           else
              cp -T {input.f5} {output.f5}
           fi
           ( cd `dirname {output.f5}` && md5sum `basename {output.f5}` ) > {output.md5}
        """

# These two concatenate and zip and sums the fastq. The fastq are smaller so one file is OK
# The name for the file is as per doc/filename_convention.txt but this rule doesn't care
# TODO - should possibly convert this to zip the individual chunks, as per the no{calref} files?

# I've broken out the rule that makes _fastq.list files so I can request the list without
# making the actual merged file.
localrules: list_fastq
rule list_fastq:
    priority: 100
    output:
        fofn  = "{cell}/{fullid}_{barcode}_{pf}_fastq{gz,\.gz|}.list"
    input:
        fastq = lambda wc: [f"{EXPDIR}/{f}" for f in SC[wc.cell][wc.barcode][f"fastq{wc.gz}_{wc.pf}"]]
    run:
        # Just write all the input files to the output file.
        with open(output.fofn, "w") as fh:
            try:
                for fname in input.fastq: print(fname, file=fh)
            except AttributeError:
                # So there are no files. Make an empty list then.
                pass

rule concat_gzip_md5sum_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_{pf}.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_{pf}.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_{pf}.fastq.count"
    input:
        fofn    = "{cell}/{fullid}_{barcode}_{pf}_fastq.list",
        fofn_gz = "{cell}/{fullid}_{barcode}_{pf}_fastq.gz.list",
    threads: 8
    run:
        # Rely on xargs to deal with way more files than could fit on the command line
        # and zip them all into one.
        shell(r"xargs -rd '\n' cat <{input.fofn} | {PIGZ} -p{threads} -c > {output.gz}")

        # Add already gzipped files. Normally there will only be one or the other, but we do support both
        # Note that we're just concatenating the zipped files, not decompressing and recompressing,
        # but we do want to verify integrity, and this version should do so in a cache-friendly way.
        shell(r"""xargs -n 1 -rd '\n' sh -c '{PIGZ} -t "$@" >&2 || exit 255 ; cat "$@"' - <{input.fofn_gz} >> {output.gz}""")

        # Base counter
        shell(r"{PIGZ} -cd {output.gz} | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

# We've decided to do the calref mapping up front. Here's the rule to combine the filtered
# fastq.gz files, by simply concatenating them rather than unzip/rezip.
# Note this is specific to the pass reads. We don't partition the fail reads (should we??)
rule concat_md5sum_nocalref_fastq:
    priority: 100
    output:
        gz     = "{cell}/{fullid}_{barcode}_no{calref}.fastq.gz",
        md5    = "md5sums/{cell}/{fullid}_{barcode}_no{calref}.fastq.gz.md5",
        counts = "counts/{cell}/{fullid}_{barcode}_no{calref}.fastq.count",
        fofn   = temp("fastq_pass_tmp/{cell}/{fullid}_{barcode}_no{calref}fq.list"),
        fofn2  = temp("fastq_pass_tmp/{cell}/{fullid}_{barcode}_no{calref}fq_count.list"),
    input:
        fastq     = lambda wc: [f"fastq_pass_tmp/{f}.no{wc.calref}fq.gz"    for f in SC[wc.cell][wc.barcode]['fastq_pass']],
        fastq_gz  = lambda wc: [f"fastq_pass_tmp/{f}no{wc.calref}fq.gz"     for f in SC[wc.cell][wc.barcode]['fastq.gz_pass']],
        counts    = lambda wc: [f"fastq_pass_tmp/{f}.no{wc.calref}fq.count" for f in SC[wc.cell][wc.barcode]['fastq_pass']],
        counts_gz = lambda wc: [f"fastq_pass_tmp/{f}no{wc.calref}fq.count"  for f in SC[wc.cell][wc.barcode]['fastq.gz_pass']],
    run:
        def getinputs(*vnames):
            """Little helper to get combined input values"""
            for v in vnames:
                try: yield from getattr(input, v)
                except AttributeError: pass

        # Avoid blowing the command line limit by listing files in a file. This could be
        # marked temporary but it's small and handy for debugging.
        with open(output.fofn, "w") as fh:
            for fname in getinputs("fastq", "fastq_gz"):
                print(fname, file=fh)

        fofn2_linecount = 0
        with open(output.fofn2, "w") as fh:
            for fname in getinputs("counts", "counts_gz"):
                print(fname, file=fh)
                fofn2_linecount += 1

        # Simply concatenate the gzipped files. This works even if no files.
        shell(r"xargs -rd '\n' cat <{output.fofn} > {output.gz}")

        if fofn2_linecount:
            # Combine base counts
            shell(r"xargs -rd '\n' cat <{output.fofn2} | fq_base_combiner.awk -r fn=`basename {output.gz} .gz` > {output.counts}")
        else:
            # Synthesize an empty count.
            shell(r"true | fq_base_counter.awk -r fn=`basename {output.gz} .gz` > {output.counts}")

        # Checksum over the whole file
        shell(r"( cd `dirname {output.gz}` && md5sum `basename {output.gz}` ) > {output.md5}")

rule concat_md5sum_calref_bam:
    output:
        bam  = "{cell}/{fullid}_{barcode}_{calref}.bam",
        md5  = "md5sums/{cell}/{fullid}_{barcode}_{calref}.bam.md5",
        fofn = temp("{cell}/{fullid}_{barcode}_{calref}_bam.list")
    input:
        bam    = lambda wc: [f"fastq_pass_tmp/{f}.{wc.calref}.bam" for f in SC[wc.cell][wc.barcode]['fastq_pass']],
        bam_gz = lambda wc: [f"fastq_pass_tmp/{f}{wc.calref}.bam" for f in SC[wc.cell][wc.barcode]['fastq.gz_pass']],
    threads: 4
    run:
        def getinputs(*vnames):
            """Little helper to get combined input values"""
            for v in vnames:
                try: yield from getattr(input, v)
                except AttributeError: pass

        fofn_linecount = 0
        with open(output.fofn, "w") as fh:
            for fname in getinputs("bam", "bam_gz"):
                print(fname, file=fh)
                fofn_linecount += 1

        if fofn_linecount:
            # samtools merge (files in fastq_pass_tmp should be pre-sorted)
            shell("{TOOLBOX} samtools merge -@ {threads} -l9 -b {output.fofn} {output.bam}")
        else:
            # We're merging nothing. But Snakemake must have her output file.
            shell("touch {output.bam}")

        shell(r"( cd `dirname {output.bam}` && md5sum `basename {output.bam}` ) > {output.md5}")


# This rule produces the stuff in fastq_pass_tmp, and will normally be applied
# to every individual fastq_pass file prior to combining the results. It uses nested
# implicit FIFOs. I call it "bashception".
# Note this type of construct is vulnerable to the kill-before-flush bug but it seems with
# this arrangement we are OK. See:
#  https://www.pixelbeat.org/docs/coreutils-gotchas.html
# In normal operation the temp dir will be removed by the "onsuccess" handler below.
rule map_calref:
    output:
        fq     = temp("fastq_pass_tmp/{cell_pf_bc}/{f}.fastq.{gz}no{calref}fq.gz"),
        bam    = temp("fastq_pass_tmp/{cell_pf_bc}/{f}.fastq.{gz}{calref}.bam"),
        counts = temp("fastq_pass_tmp/{cell_pf_bc}/{f}.fastq.{gz}no{calref}fq.count"),
    input:
        fq     = lambda wc: f"{EXPDIR}/{wc.cell_pf_bc}/{wc.f}.fastq.{wc.gz}".rstrip('.'),
        ref    = lambda wc: ancient(calibration_refs[wc.calref]),
    params:
        rg     = r'@RG\tID:1\tSM:{cell_pf_bc}\tPL:promethion',
        mmopts = '-t 1 -a --MD --sam-hit-only -y --secondary=no -x map-ont '
    wildcard_constraints:
        f      = "[^/]+",
        gz     = "gz|",
    threads: 6
    shell:
       r"""{TOOLBOX} minimap2 {params.mmopts} -R {params.rg:q} {input.ref} {input.fq} | tee >( \
                lambda_splitter.awk \
                    -v paf=/dev/stdin \
                    -v nolambda=>({PIGZ} -c -p{threads} > {output.fq}) \
                    <({PIGZ} -fdc {input.fq}) ) | \
                {TOOLBOX} samtools sort - -@ {threads} -o {output.bam}
           {PIGZ} -dc -p{threads} {output.fq} | fq_base_counter.awk -r fn=`basename {output.fq} .gz` > {output.counts}
        """

# Remove the fastq_pass_tmp directory which should normally be empty of files if
# everything worked.
onsuccess:
    shell("find fastq_pass_tmp/ -type d -delete || true")

