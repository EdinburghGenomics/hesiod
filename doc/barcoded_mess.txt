After adding the ability to process barcoded runs, the report is a mess.

What to do? The tables are aggregated by project but have one row per barcode.

Ideally I'd have a view like we see with Illuminatus - ie. the fastqscreen output.
Potentially I could import the JavaScript and make the plot, but making this work with
PanDoc will make an unholy mess. Won't it?

And I should have a dropdown so you can select phylum/order/species - again that means
more JavaScript. Or else I could make it a selector like for % vs counts? But the %
mode makes no sense when everything is downsampled to 10k.

Yeah, we'll shelve this for now. Make the histogram as that's most useful. But my choice of
graphing library should be in accord with the idea above. Grrr.

OK, let's get the delivery sorted first and then come back to it.

---

Jan 2023

See https://github.com/EdinburghGenomics/hesiod/issues/1

Urmi wants to press on and fix this. My idea is that for any cell there can be a
sample_names.tsv which maps the barcodes to the actual sample names. Like:

barcodeNN  internal_name  external_name

I'll want a script that fetches the file from somewhere. Ideally this should be in LIMS
but for now it will have to copy from a shared location, and I'll need to apply some
checks to it. The coding will not affect the processing, only the report (and later the
delivery). We'll have two
reports - one with all codes and no names (as now), one with only the named codes.

How to do this? In most cases the sample names will be fixed by project, but sometimes
by pool, so for a run/cell like:

20220101_EGS2_12345XX/12345XXpool01/20220101_1142_1E_PAM30735_b8d4bc73

The possible files to look for would be, in this order:

12345XXpool01_sample_names.tsv
PAM30735_sample_names.tsv
12345_sample_names.tsv

We'll assume for now that there is a shared directory to be searched for these things.
Having found the file, we need to process it. We should be suspicious. If the file has
only 2 columns we can be happy without an external name. Other
problems should lead to a result like:

<< sample_names.yaml >>
error: 'Malformed sample table in PAM30735_sample_names.tsv'

OK. Let's do this. Done. Now for when there are no barcodes, I want a heuristic about which
to show on the regular report.
I think under 1000 reads out of 10M is a good cutoff, so 0.0001 (0.01%). This can be applied
at the report maker stage.

Tasks:

* Have the report maker able to make filtered or unfiltered reports
    --filter sample_names.yaml
    --filter yaml (implies per-cell sample_names.yaml)
    --filter all (or none)
    --filter cutoff
* Have a hyperlink between the two
* Apply the cutoff (as per --filter cutoff) if sample_names.yaml has no barcodes
  (I thought about doing this at the driver level, rather than trying to
  have make_report.py be clever, but I think it needs to be clever).

OK this is trickier than I thought, because the report could potentially have a mixture
of cells...
Do all cells actually have barcodes? Or none? Or some?
Do all cells have a valid (with barcodes) sample_names.yaml? Or none? Or some?

Need to think about all these cases. I think the driver is just going to run the report
maker regardless with --filter yaml and so we need to anticipate all the options.
OK I did this bit now what about the driver.

sample_names_fetch.py needs to be run once per cell, and it outputs {cell}/sample_names.yaml
It's forgiving of most errors, aside from malformed cell name, and permissions issues.
So we can run it as a Snakemake rule, and use the result as input to make_report.txt.

We need to make the rule a localrule, and we need to make it always re-run (to pick up
updates) and we need to have a global setting for SAMPLE_NAMES_PATH.

OK. Decision needed. For most other stuff, the one_cell rule folds all the info from the
various files into cell_info.yaml and then the make_report.py relies on this. Shall I do
the same for this new stuff of should I keep it as it is, where the cell_info.yaml has
no sample name stuff and the make_report script handles it all? What are the pros and cons:

pros: Means it works like everything else
cons: More bloat in Snakefile.main (in one_cell and get_cell_info())

erm? OK, idea is to take the code currently in rule one_cell and put it into a separate
script one_cell_info.py. This will take YAML on STDIN and print YAML on STDOUT. So it
should be easy to test. Then I can modify the rule to marshall the input, output and
parameters as YAML and pipe it to the script.

No this is silly. Just add to the code we have. It works. And I have a better way than
my try/catch to see erors from this part of the code:

Simply add this at the top:
logger.quiet.discard('all')

---

Again. I'm going round in circles here. I've made the make_report.py script respect barcodes
but I think I did this wrong. Rather than adding --cutoff and --filter to this script and
supporting various values for --filter I think I need to...

Move the basic gist of load_and_resolve_barcodes into get_cell_info(). This will only be
processing one cell at a time so no loop needed. This will require a _filter_yaml element
(the one_cell rule in turn requires sample_names.yaml) and will add a _filter element
which is a dict {barcode: name} as before and a '_filter_type' value which will be 'yaml'
or 'cutoff' or 'none' if _filter is empty (which implies bool(cutoff) is False).

OK this makes more logical sense, I think. Then the --filter for make_report.py will be:
'off' (ignore filter entirely) or 'all' (show everything and link)  or 'on' (use '_filter')
as found in the YAML.
