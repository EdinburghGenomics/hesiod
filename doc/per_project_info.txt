In each Hesiod report we have one or more projects represented, but in the initial version
we're not compiling any per-project info. I think we should. This particularly came up when
looking at how to add the blob stats, which should probably be grouped by project.

Urmi agrees, so after "About this run" we have:

Project 12345XY (or 12345YX?)

Cell Count
Files in Pass
Files in Fail

[ Then a table with rows for passed/failed/filtered ]
Total Reads
Total Bases
Max Length

Then the Blob Tables

BLAST hits by Phylum
BLAST hits by Order
BLAST hits by Species


# To make this work...

The Snakefile needs to work out which project is represented on a cell (simply by taking
the first 5 chars). At the moment there is potential for mis-typing the project initials
so I'll account for that in the report.

Then the list_blob_plots needs to make an overview by project - tables_by_project.yaml,
linking to 3x blobstats.{taxlevel}.{project}.csv per project, so a dict of lists of named
tuples.

{ project: [ { title: ...,
               csv: ... },
           ] },

Now the other info will be resolved by project internally to make_report.py

Ah - but I also have to consider that the tables could be separate for pass + nolambda and the
number of reads in the subsample is important. I should incorporate this into the table I think.

{ project: [ { title: "BLAST hits for (pass|nolambda) reads by phylum"
               counts: {dict_by_cell},
               csv: filename },
           ] },

I could have a rule to make blobstats.{project}.{pf}.{taxlevel}.csv or I could roll everything into
one rule that loops through. The former seems more snake-makey. In which case I need to calculate
a master dict of { project: cells }. OK, I've done this.

----

We've hit a fundamental problem. In a regular reference-free QC report I don't why these numbers
are calculated as they are but I know they relate to the depth of coverage in the individual BAM files.
Ideally, in a de-novo assembly, high-coverage contigs are well resolved and may represent many
reads, so counting the contigs belonging to each taxon is not too useful. But multiplying that number by
the coverage for a given BAM gets you back to an estimate of the amount of that taxon in that library.

So here where we're running blobtools on the actual raw reads and there is no BAM, we really just want to
count the number of reads assigned to each taxon. Possibly normalised by length. Possibly using the
complexity index as a cutoff?

I believe that just running the current script will churn out nonsense numbers because it will be multiplying
something by the complexity (non-dustiness) score - which is pretty arbitrary.
Also it may fail completely because I'm trying to combine reports from mutliple 'assemblies' (as blobtools sees
the samples), whereas in our regular QC reports we always tabulate all of the libraries which have been
aligned back to a single assembly, for which we have a single blobDB based on a single BLAST summary of a
single subsample.

So what to do? I think I'll press on and finish implementing what I started. The numbers will be junk, however
I think that fixing them will be a matter of replacing the parseBlobTable.R script with a more appropriate
calculation. Therefore the mechanisms that put the tables into the report will stand.

One thing I will get rid of, though - the 'counts' sections in the blobstats_by_project.yaml files.
I've just realised the blobplot.stats.txt files have this info in them, so there's no need to patch
it in via this clunky mechanism.

I'm going to do a checkpoint commit of the code then rip this out.
