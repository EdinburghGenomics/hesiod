Hesiod currently has a major inefficiency that md5summing of and minimap2 aligning of the
reads are done as all separate jobs, but these complete in just a couple of seconds and most of the
runtime is spent waiting for SLURM to churn through all the jobs.

I can't group them into a single job because that would take far too long. Really want to group
about 100 at a time, I think.

We should have an easy fix in terms of https://snakemake.readthedocs.io/en/stable/executing/grouping.html

But to get this working properly I may need to switch from cluster.yml to using profiles which I've
avoided doing. But it's probably worth it really. Hmmm.

Well I'll give it a bash anyway. First I need to remind myself if Snakemake can be forced to load a profile
from a fixed location and not the home dir? Yes, apparently you just give the absolute path to the profile.
Cool. I'll remind myself what I did with Verkko to make that happy. I wonder how easily I can implement
overriding resource settings on the fly with profiles? There must be a cunning way.

OK I think I have everything working in test/scratch, albeit with a patched version of Snakemake (for now).
I can define a group for every rule I want to batch up, and tell Smakemake to run these in batches of 100,
albeit the syntax is a little funky.

What I'll need is a Python script that emits config.yaml files because some things in the file need to
be fixed on the fly.

---

Related question, is can I sort out the annoyance that we always run a whole load of BLAST jobs even if
there is nothing to BLAST? Ideas would be:

1) Use checkpoint rules to dynamically determine the number of jobs (I mean, this should work fine)

2) Is there any way to try the rule locally then only submit to the cluster of this fails (hacky, even if it works)

No - I can use the "{attempt}" placeholder to set resources but there's no way to switch the 'localrules'
setting based upon this parameter. So yeah make it a checkpoint rule. And test it on a big run with barcodes cos
that is the main place I need it. (ie. the later Tim Aitman runs)

OK, so adding a checkpoint rule. What needs to happen to make this work?

rule merge_blast_reports requests BLOB_CHUNKS (eg. 20) inputs per barcode,
so with 12 barcodes we spawn 240 jobs even if not all barcodes are in use.

instead we should have the input to this be a function which refers to checkpoints.
split_fasta_in_chunks should be a checkpoint rule that produces a directory of outputs
and then we should see what is produced in that directory. Yup.
Beware of filesystem latency issues here - it's possible that the directory will be seen by the
master process, and thus the rule will complete, even while all the files are not there.
We will armour against this. - DONE

---

OK it's all coded up. Let's test it. I'll test by reprocessing the latest Tim Aitman run.
This had barcodes but only one actual sample per cell so it's perfect.

$ mkdir ~/test_promethion/runs/2022/20221103_EGS2_25070AT
$ ln -snf /lustre-gseg/promethion/prom_runs/2022/20221103_EGS2_25070AT/25* \
    ~/test_promethion/runs/2022/20221103_EGS2_25070AT

Cool
