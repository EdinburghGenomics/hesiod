Hesiod currently has a major inefficiency that md5summing of and minimap2 aligning of the
reads are done as all separate jobs, but these complete in just a couple of seconds and most of the
runtime is spent waiting for SLURM to churn through all the jobs.

I can't group them into a single job because that would take far too long. Really want to group
about 100 at a time, I think.

We should have an easy fix in terms of https://snakemake.readthedocs.io/en/stable/executing/grouping.html

But to get this working properly I may need to switch from cluster.yml to using profiles which I've
avoided doing. But it's probably worth it really. Hmmm.

Well I'll give it a bash anyway. First I need to remind myself if Snakemake can be forced to load a profile
from a fixed location and not the home dir? Yes, apparently you just give the absolute path to the profile.
Cool. I'll remind myself what I did with Verkko to make that happy. I wonder how easily I can implement
overriding resource settings on the fly with profiles? There must be a cunning way.

OK I think I have everything working in test/scratch, albeit with a patched version of Snakemake (for now).
I can define a group for every rule I want to batch up, and tell Smakemake to run these in batches of 100,
albeit the syntax is a little funky.

What I'll need is a Python script that emits config.yaml files because some things in the file need to
be fixed on the fly.

---

Related question, is can I sort out the annoyance that we always run a whole load of BLAST jobs even if
there is nothing to BLAST? Ideas would be:

1) Use checkpoint rules to dynamically determine the number of jobs (I mean, this should work fine)

2) Is there any way to try the rule locally then only submit to the cluster of this fails (hacky, even if it works)

No - I can use the "{attempt}" placeholder to set resources but there's no way to switch the 'localrules'
setting based upon this parameter. So yeah make it a checkpoint rule. And test it on a big run with barcodes cos
that is the main place I need it. (ie. the later Tim Aitman runs)

OK, so adding a checkpoint rule. What needs to happen to make this work?

rule merge_blast_reports requests BLOB_CHUNKS (eg. 20) inputs per barcode,
so with 12 barcodes we spawn 240 jobs even if not all barcodes are in use.

instead we should have the input to this be a function which refers to checkpoints.
split_fasta_in_chunks should be a checkpoint rule that produces a directory of outputs
and then we should see what is produced in that directory. Yup.
Beware of filesystem latency issues here - it's possible that the directory will be seen by the
master process, and thus the rule will complete, even while all the files are not there.
We will armour against this. - DONE

---

OK it's all coded up. Let's test it. I'll test by reprocessing the latest Tim Aitman run.
This had barcodes but only one actual sample per cell so it's perfect.

$ mkdir ~/test_promethion/runs/2022/20221103_EGS2_25070AT
$ ln -snf /lustre-gseg/promethion/prom_runs/2022/20221103_EGS2_25070AT/25* \
    ~/test_promethion/runs/2022/20221103_EGS2_25070AT

Cool

---

At this point I realise that I've only half implemented the BAM support and I need to merge
the BAM files that are now listed in sc_data.yaml into the output. Hopefully not a big
job though. Remember to allocate enough resource, and use 'samtools' not Picard for the
merge, but you can compare the result to what you got with Picard before.

I might have to merge in batches and then merge the batches, but we'll see. Actually the batching
of our .fast5 processing may resolve this issue for us...

anyway, back to the main event.

---

Hmmm. The job grouping is shonky. It's putting 10000 jobs together for no apparent reason. I'm just not sure
it's solid enough for us. Is there any other way to make this happen? Let me think.

I can define a rule that takes N inputs and makes N outputs. If there were exactly N outputs this would be
OK. What if there are a multiple of N outputs?

Say the rule processes 4 at once and I have 12 files to process.

rule silly:
    output: expand( "out_{{x}}.{n}" for n in range(4) )
    input:  expand( "in_{{x}}.{n}" for n in range(4) )
    shell:
        ...

This requires my outputs to have batch numbers in the names. I could make a really hacky input function to
have any inputs I like though. For calref .bam files this is fine because I merge them all anyway so we
lose the names, but for FAST5 it's bad.

For the FAST5 files I want to keep the names. Ah, but what if I batch into directories?
I was thinking that tracking directories rather than individual files would be bad, but
actually giving Snakemake less to think about is a win. So lettuce have a think about that.

rule less_silly:
    output:
        f5 = directory("{cell}/fast5_{barcode}_{pfs}/batch_{batch}")
        md5 = "{cell}/fast5_{barcode}_{pfs}/batch_{batch}/all.md5"
    input:
        function that gets a chunk of fast5 files based on batch

The function is not very tricky it just takes a slice of files from SC.

Now the copy_fast5 job would have to request all the chunks. Maybe it could request
all the 'all.md5' files and then produce an 'all_fast5.md5' file. Ideally I'd like these
split out - currently the copy_md5sum_fast5 rule makes all the outputs but I think
copy_fast5 could do some splitting. Maybe? Or else I can modify the disseminate script to
work with the one big file.

OK, I think this is reasonable. I'll implement it tomorrow. Amend the copy_fast5 rule
first, I think.

def i_copy_fast5(wildcards=None):
    """Available files within a single directory are divided into
       batches of 100
    """
    batch_size = 100

    # Dict mapping each directory to the number of files it contains
    dirs = { (cell, bc, pf): len(SC[cell][bc][pf])
                    for cell, barcodes in SC.items()
                    for bc in barcodes
                    for pf in ['fast5_pass', 'fast5_fail'] }

    # List of md5 files we thus need to generate.
    res = []
    for (cell, bc, pf), total in dirs.items():
        batches_for_dir = ((total - 1) // batch_size) + 1
        for b in range(batches_for_dir):
            md5 = f"{cell}/fast5_{bc}_{pf}/batch{batch_size}_{b:08d}/all.md5"
            res.append(md5)
    return res

# And we'll need a corresponding input function for copy_md5sum_fast5_batch

def i_copy_md5sum_fast5_batch(wildcards):
    all_in_dir = SC[wildcards.cell][wildcards.bc][wildcards.pf]

    batch_size = int(wildcards.bs)
    batch_num = int(wildcards.b)

    return all_in_dir[batch_num*batch_size:(batch_num+1)*batch_size]

OK that all looks reasonable. With some modification I think I have this working nicely.

Next is to do the same for concat_md5sum_nocalref_fastq and concat_md5sum_calref_bam. Both these
rules take multiple inputs from fastq_pass_tmp and they are pretty ugly as it is.
Instead they should take a list files as input and the lists should be partitioned by chunk.

File is still:

fastq_pass_tmp/25070AT0005/20221103_1710_1B_PAM82832_3ed26d0e/fastq_pass/unclassified/PAM82832_pass_unclassified_3ed26d0e_3f3157da_27.fastq.gznolambdafq.gz

no, actually:

fastq_pass_tmp/25070AT0005/20221103_1710_1B_PAM82832_3ed26d0e/fastq_unclassified_pass/PAM82832_pass_unclassified_3ed26d0e_3f3157da_27[.nolambdafq.fastq.gz]
ditto for .nolambdafq.fastq.count and .lambda.bam.

And the list is:

fastq_pass_tmp/25070AT0005/20221103_1710_1B_PAM82832_3ed26d0e/fastq_unclassified_pass/batch100_00000000.list

And the rule depends on all the list files (like the fast5 rule depends on all the md5sums chunks)

----

OK so I re-jigged the two rules concat_md5sum_nocalref_fastq and concat_md5sum_calref_bam in terms of the fofn input files
and the rules are easier to read (yay) but the input function is a bit long-winded (meh). Now I need to re-do the map_calref
function so instead of outputting the individual files it outputs "fastq_pass_tmp/{cell}/fastq_{barcode}_pass/batch{bsize}_{batch}.list"

rename map_calref to map_calref_batch

actually the dir should be "fastq_nolambda_tmp" not "fastq_pass_tmp".
